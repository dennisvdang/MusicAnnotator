{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e815a67-f356-49f8-a3a3-48cf03a25126",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14feef35-509a-48d7-bb9b-7fbe8ed7cbae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SI-PLCA Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f24358-71dc-4858-9c9e-da9ef22edc61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright (C) 2009-2010 Ron J. Weiss (ronw@nyu.edu)\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "\"\"\"plca: Probabilistic Latent Component Analysis\n",
    "\n",
    "This module implements a number of variations of the PLCA algorithms\n",
    "described in [2] and [3] with both Dirichlet and (approximate)\n",
    "Entropic priors over the parmaters.\n",
    "\n",
    "PLCA is a variant of non-negative matrix factorization which\n",
    "decomposes a (2D) probabilitity distribution (arbitrarily normalized\n",
    "non-negative matrix in the NMF case) V into the product of\n",
    "distributions over the columns W = {w_k}, rows H = {h_k}, and mixing\n",
    "weights Z = diag(z_k).  See [1-3] for more details.\n",
    "\n",
    "References\n",
    "----------\n",
    "[1] R. J. Weiss and J. P. Bello. \"Identifying Repeated Patterns in\n",
    "    Music Using Sparse Convolutive Non-Negative Matrix\n",
    "    Factorization\". In Proc. International Conference on Music\n",
    "    Information Retrieval (ISMIR), 2010.\n",
    "\n",
    "[2] P. Smaragdis and B. Raj. \"Shift-Invariant Probabilistic Latent\n",
    "    Component Analysis\". Technical Report TR2007-009, MERL, December\n",
    "    2007.\n",
    "\n",
    "[3] P. Smaragdis, B. Raj, and M. Shashanka. \"Sparse and\n",
    "    shift-invariant feature extraction from non-negative data\".  In\n",
    "    Proc. ICASSP, 2008.\n",
    "\n",
    "Copyright (C) 2009-2010 Ron J. Weiss <ronw@nyu.edu>\n",
    "\n",
    "LICENSE: This module is licensed under the GNU GPL. See COPYING for details.\n",
    "\"\"\"\n",
    "\n",
    "logger = logging.getLogger(\"plca\")\n",
    "EPS = np.finfo(float).eps\n",
    "# EPS = 1e-100\n",
    "\n",
    "\n",
    "def kldivergence(V, WZH):\n",
    "    # return np.sum(V * np.log(V / WZH) - V + WZH)\n",
    "    return np.sum(WZH - V * np.log(WZH))\n",
    "\n",
    "\n",
    "def normalize(A, axis=None):\n",
    "    Ashape = A.shape\n",
    "    try:\n",
    "        norm = A.sum(axis) + EPS\n",
    "    except TypeError:\n",
    "        norm = A.copy()\n",
    "        for ax in reversed(sorted(axis)):\n",
    "            norm = norm.sum(ax)\n",
    "        norm += EPS\n",
    "    if axis:\n",
    "        nshape = np.array(Ashape)\n",
    "        nshape[axis] = 1\n",
    "        norm.shape = nshape\n",
    "    return A / norm\n",
    "\n",
    "\n",
    "def shift(a, shift, axis=None, circular=True):\n",
    "    \"\"\"Shift array along a given axis.\n",
    "\n",
    "    If circular is False, zeros are inserted for elements rolled off\n",
    "    the end of the array.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    np.roll\n",
    "    \"\"\"\n",
    "    aroll = np.roll(a, shift, axis)\n",
    "    if not circular and shift != 0:\n",
    "        if axis is None:\n",
    "            arollflattened = aroll.flatten()\n",
    "            if shift > 0:\n",
    "                arollflattened[:shift] = 0\n",
    "            elif shift < 0:\n",
    "                arollflattened[shift:] = 0\n",
    "            aroll = np.reshape(arollflattened, aroll.shape)\n",
    "        else:\n",
    "            index = [slice(None)] * a.ndim\n",
    "            if shift > 0:\n",
    "                index[axis] = slice(0, shift)\n",
    "            elif shift < 0:\n",
    "                index[axis] = slice(shift, None)\n",
    "            aroll[index] = 0\n",
    "    return aroll\n",
    "\n",
    "\n",
    "class PLCA(object):\n",
    "    \"\"\"Probabilistic Latent Component Analysis\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    analyze\n",
    "        Performs PLCA decomposition using the EM algorithm from [2].\n",
    "    reconstruct(W, Z, H, norm=1.0)\n",
    "        Reconstructs input matrix from the PLCA parameters W, Z, and H.\n",
    "    plot(V, W, Z, H)\n",
    "        Makes a pretty plot of V and the decomposition.\n",
    "\n",
    "    initialize()\n",
    "        Randomly initializes the parameters.\n",
    "    do_estep(W, Z, H)\n",
    "        Performs the E-step of the EM parameter estimation algorithm.\n",
    "    do_mstep()\n",
    "        Performs the M-step of the EM parameter estimation algorithm.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    You probably don't want to initialize this class directly.  Most\n",
    "    interactions should be through the static methods analyze,\n",
    "    reconstruct, and plot.\n",
    "\n",
    "    Subclasses that want to use a similar interface (e.g. SIPLCA)\n",
    "    should also implement initialize, do_estep, and do_mstep.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Generate some random data:\n",
    "    >>> F = 20\n",
    "    >>> T = 100\n",
    "    >>> rank = 3\n",
    "    >>> means = [F/4.0, F/2.0, 3.0*F/4]\n",
    "    >>> f = np.arange(F)\n",
    "    >>> trueW = plca.normalize(np.array([np.exp(-(f - m)**2 / F)\n",
    "    ...                                  for m in means]).T, 0)\n",
    "    >>> trueZ = np.ones(rank) / rank\n",
    "    >>> trueH = plca.normalize(np.random.rand(rank, T), 1)\n",
    "    >>> V = plca.PLCA.reconstruct(trueW, trueZ, trueH)\n",
    "\n",
    "    Perform the decomposition:\n",
    "    >>> W, Z, H, norm, recon, logprob = plca.PLCA.analyze(V, rank=rank)\n",
    "    INFO:plca:Iteration 0: logprob = 8.784769\n",
    "    INFO:plca:Iteration 50: logprob = 8.450114\n",
    "    INFO:plca:Iteration 99: final logprob = 8.449504\n",
    "\n",
    "    Plot the parameters:\n",
    "    >>> plt.figure(1)\n",
    "    >>> plca.PLCA.plot(V, W, Z, H)\n",
    "    >>> plt.figure(2)\n",
    "    >>> plca.PLCA.plot(V, trueW, trueZ, trueH)\n",
    "\n",
    "    W, Z, H and trueW, trueZ, trueH should be the same modulo\n",
    "    permutations along the rank dimension.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SIPLCA : Shift-Invariant PLCA\n",
    "    SIPLCA2 : 2D Shift-Invariant PLCA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        V,\n",
    "        rank,\n",
    "        alphaW=0,\n",
    "        alphaZ=0,\n",
    "        alphaH=0,\n",
    "        betaW=0,\n",
    "        betaZ=0,\n",
    "        betaH=0,\n",
    "        nu=50.0,\n",
    "        minpruneiter=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : array, shape (`F`, `T`)\n",
    "            Matrix to analyze.\n",
    "        rank : int\n",
    "            Rank of the decomposition (i.e. number of columns of `W`\n",
    "            and rows of `H`).\n",
    "        alphaW, alphaZ, alphaH : float or appropriately shaped array\n",
    "            Dirichlet prior parameters for `W`, `Z`, and `H`.\n",
    "            Negative values lead to sparser distributions, positive\n",
    "            values makes the distributions more uniform.  Defaults to\n",
    "            0 (no prior).\n",
    "\n",
    "            **Note** that the prior is not parametrized in the\n",
    "            standard way where the uninformative prior has alpha=1.\n",
    "        betaW, betaZ, betaH : non-negative float\n",
    "            Entropic prior parameters for `W`, `Z`, and `H`.  Large\n",
    "            values lead to sparser distributions.  Defaults to 0 (no\n",
    "            prior).\n",
    "        nu : float\n",
    "            Approximation parameter for the Entropic prior.  It's\n",
    "            probably safe to leave the default.\n",
    "        \"\"\"\n",
    "        self.V = V.copy()\n",
    "        self.rank = rank\n",
    "\n",
    "        self.F, self.T = self.V.shape\n",
    "\n",
    "        # Allocate the sufficient statistics here, so they don't have to be\n",
    "        # reallocated at every iteration.  This becomes especially important\n",
    "        # for the more sophistacted models with many hidden variables.\n",
    "        self.VRW = np.empty((self.F, self.rank))\n",
    "        self.VRH = np.empty((self.T, self.rank))\n",
    "\n",
    "        self.alphaW = 1 + alphaW\n",
    "        self.alphaZ = 1 + alphaZ\n",
    "        self.alphaH = 1 + alphaH\n",
    "\n",
    "        if betaW < 0 or betaZ < 0 or betaH < 0:\n",
    "            raise ValueError(\n",
    "                \"Entropic prior parameters beta{W,Z,H} must be \" \"non-negative\"\n",
    "            )\n",
    "        self.betaW = betaW\n",
    "        self.betaZ = betaZ\n",
    "        self.betaH = betaH\n",
    "        self.nu = nu\n",
    "\n",
    "        self.minpruneiter = minpruneiter\n",
    "\n",
    "    @classmethod\n",
    "    def analyze(\n",
    "        cls,\n",
    "        V,\n",
    "        rank,\n",
    "        niter=100,\n",
    "        convergence_thresh=1e-9,\n",
    "        printiter=50,\n",
    "        plotiter=None,\n",
    "        plotfilename=None,\n",
    "        initW=None,\n",
    "        initZ=None,\n",
    "        initH=None,\n",
    "        updateW=True,\n",
    "        updateZ=True,\n",
    "        updateH=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Iteratively performs the PLCA decomposition using the EM algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : array, shape (`F`, `T`)\n",
    "            Matrix to analyze.\n",
    "        niter : int\n",
    "            Number of iterations to perform.  Defaults to 100.\n",
    "        convergence_thresh : float\n",
    "        updateW, updateZ, updateH : boolean\n",
    "            If False keeps the corresponding parameter fixed.\n",
    "            Defaults to True.\n",
    "        initW, initZ, initH : array\n",
    "            Initial settings for `W`, `Z`, and `H`.  Unused by default.\n",
    "        printiter : int\n",
    "            Prints current log probability once every `printiter`\n",
    "            iterations.  Defaults to 50.\n",
    "        plotiter : int or None\n",
    "            If not None, the current decomposition is plotted once\n",
    "            every `plotiter` iterations.  Defaults to None.\n",
    "        kwargs : dict\n",
    "            Arguments to pass into the class's constructor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        W : array, shape (`F`, `rank`)\n",
    "            Set of `rank` bases found in `V`, i.e. P(f | z).\n",
    "        Z : array, shape (`rank`)\n",
    "            Mixing weights over basis vector activations, i.e. P(z).\n",
    "        H : array, shape (`rank`, `T`)\n",
    "            Activations of each basis in time, i.e. P(t | z).\n",
    "        norm : float\n",
    "            Normalization constant to make `V` sum to 1.\n",
    "        recon : array\n",
    "            Reconstruction of `V` using `W`, `Z`, and `H`\n",
    "        logprob : float\n",
    "        \"\"\"\n",
    "        norm = V.sum()\n",
    "        V /= norm\n",
    "\n",
    "        params = cls(V, rank, **kwargs)\n",
    "        iW, iZ, iH = params.initialize()\n",
    "\n",
    "        W = iW if initW is None else initW.copy()\n",
    "        Z = iZ if initZ is None else initZ.copy()\n",
    "        H = iH if initH is None else initH.copy()\n",
    "\n",
    "        params.W = W\n",
    "        params.Z = Z\n",
    "        params.H = H\n",
    "\n",
    "        oldlogprob = -np.inf\n",
    "        for n in range(niter):\n",
    "            logprob, WZH = params.do_estep(W, Z, H)\n",
    "            if n % printiter == 0:\n",
    "                logger.info(\"Iteration %d: logprob = %f\", n, logprob)\n",
    "            if plotiter and n % plotiter == 0:\n",
    "                params.plot(V, W, Z, H, n)\n",
    "                if not plotfilename is None:\n",
    "                    plt.savefig(\"%s_%04d.png\" % (plotfilename, n))\n",
    "            if logprob < oldlogprob:\n",
    "                logger.debug(\n",
    "                    \"Warning: logprob decreased from %f to %f at \" \"iteration %d!\",\n",
    "                    oldlogprob,\n",
    "                    logprob,\n",
    "                    n,\n",
    "                )\n",
    "                # import pdb; pdb.set_trace()\n",
    "            elif n > 0 and logprob - oldlogprob < convergence_thresh:\n",
    "                logger.info(\"Converged at iteration %d\", n)\n",
    "                break\n",
    "            oldlogprob = logprob\n",
    "\n",
    "            nW, nZ, nH = params.do_mstep(n)\n",
    "\n",
    "            if updateW:\n",
    "                W = nW\n",
    "            if updateZ:\n",
    "                Z = nZ\n",
    "            if updateH:\n",
    "                H = nH\n",
    "\n",
    "            params.W = W\n",
    "            params.Z = Z\n",
    "            params.H = H\n",
    "\n",
    "        if plotiter:\n",
    "            params.plot(V, W, Z, H, n)\n",
    "            if not plotfilename is None:\n",
    "                plt.savefig(\"%s_%04d.png\" % (plotfilename, n))\n",
    "        logger.info(\"Iteration %d: final logprob = %f\", n, logprob)\n",
    "        recon = norm * WZH\n",
    "        return W, Z, H, norm, recon, logprob\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(W, Z, H, norm=1.0):\n",
    "        \"\"\"Computes the approximation to V using W, Z, and H\"\"\"\n",
    "        return norm * np.dot(W * Z, H)\n",
    "\n",
    "    @classmethod\n",
    "    def plot(cls, V, W, Z, H, curriter=-1):\n",
    "        WZH = cls.reconstruct(W, Z, H)\n",
    "        plottools.plotall([V, WZH], subplot=(3, 1), align=\"xy\", cmap=plt.cm.hot)\n",
    "        plottools.plotall(\n",
    "            9 * [None] + [W, Z, H],\n",
    "            subplot=(4, 3),\n",
    "            clf=False,\n",
    "            align=\"\",\n",
    "            cmap=plt.cm.hot,\n",
    "            colorbar=False,\n",
    "        )\n",
    "        plt.draw()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initializes the parameters\n",
    "\n",
    "        W and H are initialized randomly.  Z is initialized to have a\n",
    "        uniform distribution.\n",
    "        \"\"\"\n",
    "        W = normalize(np.random.rand(self.F, self.rank), 0)\n",
    "        Z = np.ones(self.rank) / self.rank\n",
    "        H = normalize(np.random.rand(self.rank, self.T), 1)\n",
    "        return W, Z, H\n",
    "\n",
    "    def compute_logprob(self, W, Z, H, recon):\n",
    "        logprob = np.sum(self.V * np.log(recon + EPS * recon))\n",
    "        # Add Dirichlet and Entropic priors.\n",
    "        logprob += (\n",
    "            np.sum((self.alphaW - 1) * np.log(W + EPS * W))\n",
    "            + np.sum((self.alphaZ - 1) * np.log(Z + EPS * Z))\n",
    "            + np.sum((self.alphaH - 1) * np.log(H + EPS * H))\n",
    "        )\n",
    "        # Add Entropic priors.\n",
    "        logprob += (\n",
    "            self.betaW * np.sum(W * np.log(W + EPS * W))\n",
    "            + self.betaZ * np.sum(Z * np.log(Z + EPS * Z))\n",
    "            + self.betaH * np.sum(H * np.log(H + EPS * H))\n",
    "        )\n",
    "        return logprob\n",
    "\n",
    "    def do_estep(self, W, Z, H):\n",
    "        \"\"\"Performs the E-step of the EM parameter estimation algorithm.\n",
    "\n",
    "        Computes the posterior distribution over the hidden variables.\n",
    "        \"\"\"\n",
    "        WZH = self.reconstruct(W, Z, H)\n",
    "        logprob = self.compute_logprob(W, Z, H, WZH)\n",
    "\n",
    "        VdivWZH = self.V / WZH\n",
    "        for z in range(self.rank):\n",
    "            tmp = np.outer(W[:, z] * Z[z], H[z, :]) * VdivWZH\n",
    "            self.VRW[:, z] = tmp.sum(1)\n",
    "            self.VRH[:, z] = tmp.sum(0)\n",
    "\n",
    "        return logprob, WZH\n",
    "\n",
    "    def do_mstep(self, curriter):\n",
    "        \"\"\"Performs the M-step of the EM parameter estimation algorithm.\n",
    "\n",
    "        Computes updated estimates of W, Z, and H using the posterior\n",
    "        distribution computer in the E-step.\n",
    "        \"\"\"\n",
    "        Zevidence = self._fix_negative_values(self.VRW.sum(0) + self.alphaZ - 1)\n",
    "        initialZ = normalize(Zevidence)\n",
    "        Z = self._apply_entropic_prior_and_normalize(\n",
    "            initialZ, Zevidence, self.betaZ, nu=self.nu\n",
    "        )\n",
    "\n",
    "        Wevidence = self._fix_negative_values(self.VRW + self.alphaW - 1)\n",
    "        initialW = normalize(Wevidence, axis=0)\n",
    "        W = self._apply_entropic_prior_and_normalize(\n",
    "            initialW, Wevidence, self.betaW, nu=self.nu, axis=0\n",
    "        )\n",
    "\n",
    "        Hevidence = self._fix_negative_values(self.VRH.T + self.alphaH - 1)\n",
    "        initialH = normalize(Hevidence, axis=1)\n",
    "        H = self._apply_entropic_prior_and_normalize(\n",
    "            initialH, Hevidence, self.betaH, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        return self._prune_undeeded_bases(W, Z, H, curriter)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fix_negative_values(x, fix=EPS):\n",
    "        x[x <= 0] = fix\n",
    "        return x\n",
    "\n",
    "    def _prune_undeeded_bases(self, W, Z, H, curriter):\n",
    "        \"\"\"Discards bases which do not contribute to the decomposition\"\"\"\n",
    "        threshold = 10 * EPS\n",
    "        zidx = np.argwhere(Z > threshold).flatten()\n",
    "        if len(zidx) < self.rank and curriter >= self.minpruneiter:\n",
    "            logger.info(\n",
    "                \"Rank decreased from %d to %d during iteration %d\",\n",
    "                self.rank,\n",
    "                len(zidx),\n",
    "                curriter,\n",
    "            )\n",
    "            self.rank = len(zidx)\n",
    "            Z = Z[zidx]\n",
    "            W = W[:, zidx]\n",
    "            H = H[zidx, :]\n",
    "            self.VRW = self.VRW[:, zidx]\n",
    "            self.VRH = self.VRH[:, zidx]\n",
    "        return W, Z, H\n",
    "\n",
    "    @staticmethod\n",
    "    def _apply_entropic_prior_and_normalize(\n",
    "        param, evidence, beta, nu=50, niter=30, convergence_thresh=1e-7, axis=None\n",
    "    ):\n",
    "        \"\"\"Uses the approximation to the entropic prior from Matt Hoffman.\"\"\"\n",
    "        for i in range(niter):\n",
    "            lastparam = param.copy()\n",
    "            alpha = normalize(param ** (nu / (nu - 1.0)), axis)\n",
    "            param = normalize(evidence + beta * nu * alpha, axis)\n",
    "            # param = normalize(evidence + beta * nu * param**(nu / (nu - 1.0)), 1)\n",
    "            if np.mean(np.abs(param - lastparam)) < convergence_thresh:\n",
    "                logger.log(\n",
    "                    logging.DEBUG - 1,\n",
    "                    \"M-step finished after iteration \" \"%d (beta=%f)\",\n",
    "                    i,\n",
    "                    beta,\n",
    "                )\n",
    "                break\n",
    "        return param\n",
    "\n",
    "\n",
    "class SIPLCA(PLCA):\n",
    "    \"\"\"Sparse Shift-Invariant Probabilistic Latent Component Analysis\n",
    "\n",
    "    Decompose V into \\sum_k W_k * z_k h_k^T where * denotes\n",
    "    convolution.  Each basis W_k is a matrix.  Therefore, unlike PLCA,\n",
    "    `W` has shape (`F`, `win`, `rank`). This is the model used in [1].\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    PLCA : Probabilistic Latent Component Analysis\n",
    "    SIPLCA2 : 2D SIPLCA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, V, rank, win=1, circular=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : array, shape (`F`, `T`)\n",
    "            Matrix to analyze.\n",
    "        rank : int\n",
    "            Rank of the decomposition (i.e. number of columns of `W`\n",
    "            and rows of `H`).\n",
    "        win : int\n",
    "            Length of each of the convolutive bases.  Defaults to 1,\n",
    "            i.e. the model is identical to PLCA.\n",
    "        circular : boolean\n",
    "            If True, data shifted past `T` will wrap around to\n",
    "            0. Defaults to False.\n",
    "        alphaW, alphaZ, alphaH : float or appropriately shaped array\n",
    "            Sparsity prior parameters for `W`, `Z`, and `H`.  Negative\n",
    "            values lead to sparser distributions, positive values\n",
    "            makes the distributions more uniform.  Defaults to 0 (no\n",
    "            prior).\n",
    "\n",
    "            **Note** that the prior is not parametrized in the\n",
    "            standard way where the uninformative prior has alpha=1.\n",
    "        \"\"\"\n",
    "        PLCA.__init__(self, V, rank, **kwargs)\n",
    "\n",
    "        self.win = win\n",
    "        self.circular = circular\n",
    "\n",
    "        self.VRW = np.empty((self.F, self.rank, self.win))\n",
    "        self.VRH = np.empty((self.T, self.rank))\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(W, Z, H, norm=1.0, circular=False):\n",
    "        if W.ndim == 2:\n",
    "            W = W[:, np.newaxis, :]\n",
    "        if H.ndim == 1:\n",
    "            H = H[np.newaxis, :]\n",
    "        F, rank, win = W.shape\n",
    "        rank, T = H.shape\n",
    "\n",
    "        WZH = np.zeros((F, T))\n",
    "        for tau in range(win):\n",
    "            WZH += np.dot(W[:, :, tau] * Z, shift(H, tau, 1, circular))\n",
    "        return norm * WZH\n",
    "\n",
    "    def plot(self, V, W, Z, H, curriter=-1):\n",
    "        rank = len(Z)\n",
    "        nrows = rank + 2\n",
    "        WZH = self.reconstruct(W, Z, H, circular=self.circular)\n",
    "        plottools.plotall(\n",
    "            [V, WZH]\n",
    "            + [\n",
    "                self.reconstruct(W[:, z, :], Z[z], H[z, :], circular=self.circular)\n",
    "                for z in range(len(Z))\n",
    "            ],\n",
    "            title=[\"V (Iteration %d)\" % curriter, \"Reconstruction\"]\n",
    "            + [\"Basis %d reconstruction\" % x for x in range(len(Z))],\n",
    "            colorbar=False,\n",
    "            grid=False,\n",
    "            cmap=plt.cm.hot,\n",
    "            subplot=(nrows, 2),\n",
    "            order=\"c\",\n",
    "            align=\"xy\",\n",
    "        )\n",
    "        plottools.plotall(\n",
    "            [None] + [Z],\n",
    "            subplot=(nrows, 2),\n",
    "            clf=False,\n",
    "            plotfun=lambda x: plt.bar(np.arange(len(x)) - 0.4, x),\n",
    "            xticks=[[], range(rank)],\n",
    "            grid=False,\n",
    "            colorbar=False,\n",
    "            title=\"Z\",\n",
    "        )\n",
    "\n",
    "        plots = [None] * (3 * nrows + 2)\n",
    "        titles = plots + [\"W%d\" % x for x in range(rank)]\n",
    "        wxticks = [[]] * (3 * nrows + rank + 1) + [range(0, W.shape[2], 10)]\n",
    "        plots.extend(W.transpose((1, 0, 2)))\n",
    "        plottools.plotall(\n",
    "            plots,\n",
    "            subplot=(nrows, 6),\n",
    "            clf=False,\n",
    "            order=\"c\",\n",
    "            align=\"xy\",\n",
    "            cmap=plt.cm.hot,\n",
    "            colorbar=False,\n",
    "            ylabel=r\"$\\parallel$\",\n",
    "            grid=False,\n",
    "            title=titles,\n",
    "            yticks=[[]],\n",
    "            xticks=wxticks,\n",
    "        )\n",
    "\n",
    "        plots = [None] * (2 * nrows + 2)\n",
    "        titles = plots + [\"H%d\" % x for x in range(rank)]\n",
    "        if np.squeeze(H).ndim < 4:\n",
    "            plotH = np.squeeze(H)\n",
    "        else:\n",
    "            plotH = H.sum(2)\n",
    "        if rank == 1:\n",
    "            plotH = [plotH]\n",
    "        plots.extend(plotH)\n",
    "        plottools.plotall(\n",
    "            plots,\n",
    "            subplot=(nrows, 3),\n",
    "            order=\"c\",\n",
    "            align=\"xy\",\n",
    "            grid=False,\n",
    "            clf=False,\n",
    "            title=titles,\n",
    "            yticks=[[]],\n",
    "            colorbar=False,\n",
    "            cmap=plt.cm.hot,\n",
    "            ylabel=r\"$*$\",\n",
    "            xticks=[[]] * (3 * nrows - 1) + [range(0, V.shape[1], 100)],\n",
    "        )\n",
    "        plt.draw()\n",
    "\n",
    "    def initialize(self):\n",
    "        W, Z, H = super(SIPLCA, self).initialize()\n",
    "        W = np.random.rand(self.F, self.rank, self.win)\n",
    "        W /= W.sum(2).sum(0)[np.newaxis, :, np.newaxis]\n",
    "        return W, Z, H\n",
    "\n",
    "    def do_estep(self, W, Z, H):\n",
    "        WZH = self.reconstruct(W, Z, H, circular=self.circular)\n",
    "        logprob = self.compute_logprob(W, Z, H, WZH)\n",
    "\n",
    "        WZ = W * Z[np.newaxis, :, np.newaxis]\n",
    "        VdivWZH = (self.V / (WZH + EPS))[:, :, np.newaxis]\n",
    "        self.VRW[:] = 0\n",
    "        self.VRH[:] = 0\n",
    "        for tau in range(self.win):\n",
    "            Ht = shift(H, tau, 1, self.circular)\n",
    "            tmp = WZ[:, :, tau][:, np.newaxis, :] * Ht.T[np.newaxis, :, :] * VdivWZH\n",
    "            self.VRW[:, :, tau] += tmp.sum(1)\n",
    "            self.VRH += shift(tmp.sum(0), -tau, 0, self.circular)\n",
    "\n",
    "        return logprob, WZH\n",
    "\n",
    "    def do_mstep(self, curriter):\n",
    "        Zevidence = self._fix_negative_values(self.VRW.sum(2).sum(0) + self.alphaZ - 1)\n",
    "        initialZ = normalize(Zevidence)\n",
    "        Z = self._apply_entropic_prior_and_normalize(\n",
    "            initialZ, Zevidence, self.betaZ, nu=self.nu\n",
    "        )\n",
    "\n",
    "        Wevidence = self._fix_negative_values(self.VRW + self.alphaW - 1)\n",
    "        initialW = normalize(Wevidence, axis=[0, 2])\n",
    "        W = self._apply_entropic_prior_and_normalize(\n",
    "            initialW, Wevidence, self.betaW, nu=self.nu, axis=[0, 2]\n",
    "        )\n",
    "\n",
    "        Hevidence = self._fix_negative_values(self.VRH.T + self.alphaH - 1)\n",
    "        initialH = normalize(Hevidence, axis=1)\n",
    "        H = self._apply_entropic_prior_and_normalize(\n",
    "            initialH, Hevidence, self.betaH, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        return self._prune_undeeded_bases(W, Z, H, curriter)\n",
    "\n",
    "\n",
    "class SIPLCA2(SIPLCA):\n",
    "    \"\"\"Sparse 2D Shift-Invariant Probabilistic Latent Component Analysis\n",
    "\n",
    "    Shift invariance is over both rows and columns of `V`.  Unlike\n",
    "    PLCA and SIPLCA, the activations for each basis `H_k` describes\n",
    "    when the k-th basis is active in time *and* at what vertical\n",
    "    (frequency) offset.  Therefore, unlike PLCA and SIPLCA, `H` has\n",
    "    shape (`rank`, `win[1]`, `T`).\n",
    "\n",
    "    Note that this is not the same as the 2D-SIPLCA decomposition\n",
    "    described in Smaragdis and Raj, 2007.  `W` has the same shape as\n",
    "    in SIPLCA, regardless of `win[1]`.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    PLCA : Probabilistic Latent Component Analysis\n",
    "    SIPLCA : Shift-Invariant PLCA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, V, rank, win=1, circular=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : array, shape (`F`, `T`)\n",
    "            Matrix to analyze.\n",
    "        rank : int\n",
    "            Rank of the decomposition (i.e. number of columns of `W`\n",
    "            and rows of `H`).\n",
    "        win : int or tuple of 2 ints\n",
    "            `win[0]` is the length of the convolutive bases.  `win[1]`\n",
    "            is maximum frequency shift.  Defaults to (1, 1).\n",
    "        circular : boolean or tuple of 2 booleans\n",
    "            If `circular[0]` (`circular[1]`) is True, data shifted\n",
    "            horizontally (vertically) past `T` (`F`) will wrap around\n",
    "            to 0.  Defaults to (False, False).\n",
    "        alphaW, alphaZ, alphaH : float or appropriately shaped array\n",
    "            Sparsity prior parameters for `W`, `Z`, and `H`.  Negative\n",
    "            values lead to sparser distributions, positive values\n",
    "            makes the distributions more uniform.  Defaults to 0 (no\n",
    "            prior).\n",
    "\n",
    "            **Note** that the prior is not parametrized in the\n",
    "            standard way where the uninformative prior has alpha=1.\n",
    "        \"\"\"\n",
    "        PLCA.__init__(self, V, rank, **kwargs)\n",
    "        self.rank = rank\n",
    "\n",
    "        try:\n",
    "            self.winF, self.winT = win\n",
    "        except:\n",
    "            self.winF = self.winT = win\n",
    "        # Needed for compatibility with SIPLCA.\n",
    "        self.win = self.winT\n",
    "\n",
    "        try:\n",
    "            self.circularF, self.circularT = circular\n",
    "        except:\n",
    "            self.circularF = self.circularT = circular\n",
    "        # Needed for plot.\n",
    "        self.circular = (self.circularF, self.circularT)\n",
    "\n",
    "        self.VRW = np.empty((self.F, self.rank, self.winT))\n",
    "        self.VRH = np.empty((self.T, self.rank, self.winF))\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(W, Z, H, norm=1.0, circular=False):\n",
    "        if W.ndim == 2:\n",
    "            W = W[:, np.newaxis, :]\n",
    "        if Z.ndim == 0:\n",
    "            Z = Z[np.newaxis]\n",
    "        if H.ndim == 2:\n",
    "            H = H[np.newaxis, :, :]\n",
    "        F, rank, winT = W.shape\n",
    "        rank, winF, T = H.shape\n",
    "\n",
    "        try:\n",
    "            circularF, circularT = circular\n",
    "        except:\n",
    "            circularF = circularT = circular\n",
    "\n",
    "        recon = 0\n",
    "        for z in range(rank):\n",
    "            recon += sp.signal.fftconvolve(W[:, z, :] * Z[z], H[z, :, :])\n",
    "\n",
    "        WZH = recon[:F, :T]\n",
    "        if circularF:\n",
    "            WZH[: winF - 1, :] += recon[F:, :T]\n",
    "        if circularT:\n",
    "            WZH[:, : winT - 1] += recon[:F, T:]\n",
    "        if circularF and circularT:\n",
    "            WZH[: winF - 1, : winT - 1] += recon[F:, T:]\n",
    "\n",
    "        return norm * WZH\n",
    "\n",
    "    def initialize(self):\n",
    "        W, Z, H = super(SIPLCA2, self).initialize()\n",
    "        W = np.random.rand(self.F, self.rank, self.winT)\n",
    "        W /= W.sum(2).sum(0)[np.newaxis, :, np.newaxis]\n",
    "\n",
    "        H = np.random.rand(self.rank, self.winF, self.T)\n",
    "        H /= H.sum(2).sum(1)[:, np.newaxis, np.newaxis]\n",
    "        return W, Z, H\n",
    "\n",
    "    def do_estep(self, W, Z, H):\n",
    "        WZH = self.reconstruct(W, Z, H, circular=[self.circularF, self.circularT])\n",
    "        logprob = self.compute_logprob(W, Z, H, WZH)\n",
    "\n",
    "        WZ = W * Z[np.newaxis, :, np.newaxis]\n",
    "        VdivWZH = (self.V / (WZH + EPS))[:, :, np.newaxis]\n",
    "        self.VRW[:] = 0\n",
    "        self.VRH[:] = 0\n",
    "        for r in range(self.winF):\n",
    "            WZshifted = shift(WZ, r, 0, self.circularF)\n",
    "            for tau in range(self.winT):\n",
    "                Hshifted = shift(H[:, r, :], tau, 1, self.circularT)\n",
    "                tmp = (\n",
    "                    WZshifted[:, :, tau][:, :, np.newaxis] * Hshifted[np.newaxis, :, :]\n",
    "                ).transpose((0, 2, 1)) * VdivWZH\n",
    "                self.VRW[:, :, tau] += shift(tmp.sum(1), -r, 0, self.circularF)\n",
    "                self.VRH[:, :, r] += shift(tmp.sum(0), -tau, 0, self.circularT)\n",
    "\n",
    "        return logprob, WZH\n",
    "\n",
    "    def do_mstep(self, curriter):\n",
    "        Zevidence = self._fix_negative_values(self.VRW.sum(2).sum(0) + self.alphaZ - 1)\n",
    "        initialZ = normalize(Zevidence)\n",
    "        Z = self._apply_entropic_prior_and_normalize(\n",
    "            initialZ, Zevidence, self.betaZ, nu=self.nu\n",
    "        )\n",
    "\n",
    "        Wevidence = self._fix_negative_values(self.VRW + self.alphaW - 1)\n",
    "        initialW = normalize(Wevidence, axis=[0, 2])\n",
    "        W = self._apply_entropic_prior_and_normalize(\n",
    "            initialW, Wevidence, self.betaW, nu=self.nu, axis=[0, 2]\n",
    "        )\n",
    "\n",
    "        Hevidence = self._fix_negative_values(\n",
    "            self.VRH.transpose((1, 2, 0)) + self.alphaH - 1\n",
    "        )\n",
    "        initialH = normalize(Hevidence, axis=[1, 2])\n",
    "        H = self._apply_entropic_prior_and_normalize(\n",
    "            initialH, Hevidence, self.betaH, nu=self.nu, axis=[1, 2]\n",
    "        )\n",
    "\n",
    "        return self._prune_undeeded_bases(W, Z, H, curriter)\n",
    "\n",
    "\n",
    "class FactoredSIPLCA2(SIPLCA2):\n",
    "    \"\"\"Sparse 2D Shift-Invariant PLCA with factored `W`\n",
    "\n",
    "    This class performs the same decomposition as SIPLCA2, except W is\n",
    "    factored into two independent terms:\n",
    "      W = P(f, \\tau | k) = P(f | \\tau, k) P(\\tau | k)\n",
    "    and H is also factored into two independent terms:\n",
    "      H = P(t, r | k) = P(t | k) P(r | t, k)\n",
    "\n",
    "    This enables priors to be enforced *independently* over the rows\n",
    "    and columns of W_k.  The `alphaW` and `betaW` arguments now\n",
    "    control sparsity in each column of W_k and `alphaT` and `betaT`\n",
    "    control sparsity in the rows.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SIPLCA2 : 2D Shift-Invariant PLCA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, V, rank, alphaT=0, betaT=0, alphaR=0, betaR=0, **kwargs):\n",
    "        SIPLCA2.__init__(self, V, rank, **kwargs)\n",
    "        self.alphaT = 1 + alphaT\n",
    "        self.betaT = betaT\n",
    "        self.alphaR = 1 + alphaR\n",
    "        self.betaR = betaR\n",
    "\n",
    "    def do_mstep(self, curriter):\n",
    "        Zevidence = self._fix_negative_values(self.VRW.sum(2).sum(0) + self.alphaZ - 1)\n",
    "        initialZ = normalize(Zevidence)\n",
    "        Z = self._apply_entropic_prior_and_normalize(\n",
    "            initialZ, Zevidence, self.betaZ, nu=self.nu\n",
    "        )\n",
    "\n",
    "        # Factored W = P(f, \\tau | k) = P(f | \\tau, k) P(\\tau | k)\n",
    "        # P(f | \\tau, k)\n",
    "        Pf_evidence = self._fix_negative_values(self.VRW + self.alphaW - 1)\n",
    "        initialPf = normalize(Pf_evidence, 0)\n",
    "        Pf = self._apply_entropic_prior_and_normalize(\n",
    "            initialPf, Pf_evidence, self.betaW, nu=self.nu, axis=0\n",
    "        )\n",
    "\n",
    "        # P(\\tau | k)\n",
    "        Ptau_evidence = self._fix_negative_values(self.VRW.sum(0) + self.alphaT - 1)\n",
    "        initialPtau = normalize(Ptau_evidence, 1)\n",
    "        Ptau = self._apply_entropic_prior_and_normalize(\n",
    "            initialPtau, Ptau_evidence, self.betaT, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        # W = P(f, \\tau | k)\n",
    "        W = Pf * Ptau[np.newaxis, :, :]\n",
    "\n",
    "        # Factored H = P(t, r | k) = P(t | k) P(r | t, k)\n",
    "        # P(t | k)\n",
    "        Pt_evidence = self._fix_negative_values(self.VRH.sum(2).T + self.alphaH - 1)\n",
    "        initialPt = normalize(Pt_evidence, 1)\n",
    "        Pt = self._apply_entropic_prior_and_normalize(\n",
    "            initialPt, Pt_evidence, self.betaH, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        # P(r | t, k)\n",
    "        Pr_evidence = self._fix_negative_values(\n",
    "            self.VRH.transpose((1, 2, 0)) + self.alphaR - 1\n",
    "        )\n",
    "        initialPr = normalize(Pr_evidence, 1)\n",
    "        Pr = self._apply_entropic_prior_and_normalize(\n",
    "            initialPr, Pr_evidence, self.betaR, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        # H = P(r, t | k)\n",
    "        H = Pt[:, np.newaxis, :] * Pr\n",
    "\n",
    "        # Hevidence = self._fix_negative_values(self.VRH.transpose((1,2,0))\n",
    "        #                             + self.alphaH - 1)\n",
    "        # initialH = normalize(Hevidence, axis=[1, 2])\n",
    "        # H = self._apply_entropic_prior_and_normalize(\n",
    "        #    initialH, Hevidence, self.betaH, nu=self.nu, axis=[1, 2])\n",
    "\n",
    "        return self._prune_undeeded_bases(W, Z, H, curriter)\n",
    "\n",
    "\n",
    "class DiscreteWSIPLCA2(FactoredSIPLCA2):\n",
    "    \"\"\"Sparse (Time) Warp and 2D Shift-Invariant PLCA\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    PLCA : Probabilistic Latent Component Analysis\n",
    "    SIPLCA2 : 2D SIPLCA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, V, rank, warpfactors=[1], **kwargs):\n",
    "        FactoredSIPLCA2.__init__(self, V, rank, **kwargs)\n",
    "\n",
    "        self.warpfactors = np.array(warpfactors, dtype=float)\n",
    "        self.nwarp = len(self.warpfactors)\n",
    "        self.VRH = np.empty((self.T, self.rank, self.winF, self.nwarp))\n",
    "\n",
    "        # Need to weigh each path by the number of repetitions of each\n",
    "        # tau.  Keep track of it here.\n",
    "        self.taus = []\n",
    "        self.tauproportions = []\n",
    "        for n, warp in enumerate(self.warpfactors):\n",
    "            currtaus = np.floor(warp * np.arange(self.win / warp))\n",
    "\n",
    "            currtauproportions = np.empty(len(currtaus))\n",
    "            for m, tau in enumerate(currtaus):\n",
    "                currtauproportions[m] = 1.0 / np.sum(currtaus == tau)\n",
    "\n",
    "            self.taus.append([int(x) for x in currtaus])\n",
    "            self.tauproportions.append(currtauproportions)\n",
    "\n",
    "        # print self.taus\n",
    "        # print self.tauproportions\n",
    "        # print [x.sum() for x in self.tauproportions]\n",
    "\n",
    "    def reconstruct(self, W, Z, H, norm=1.0, circular=False):\n",
    "        if W.ndim == 2:\n",
    "            W = W[:, np.newaxis, :]\n",
    "        if H.ndim == 3:\n",
    "            H = H[np.newaxis, :, :, :]\n",
    "        F, rank, winT = W.shape\n",
    "        rank, winF, nwarp, T = H.shape\n",
    "\n",
    "        try:\n",
    "            circularF, circularT = circular\n",
    "        except:\n",
    "            circularF = circularT = circular\n",
    "\n",
    "        recon = np.zeros((F, T))\n",
    "        for r in range(self.winF):\n",
    "            Wshifted = shift(W, r, 0, circularF)\n",
    "            for n, warp in enumerate(self.warpfactors):\n",
    "                for delay, tau in enumerate(self.taus[n]):\n",
    "                    recon += np.dot(\n",
    "                        Wshifted[:, :, tau] * Z,\n",
    "                        shift(H[:, r, n, :], delay, 1, circularT)\n",
    "                        * self.tauproportions[n][delay],\n",
    "                    )\n",
    "        return norm * recon\n",
    "\n",
    "    def initialize(self):\n",
    "        W, Z, H = super(DiscreteWSIPLCA2, self).initialize()\n",
    "        H = normalize(\n",
    "            np.random.rand(self.rank, self.winF, self.nwarp, self.T), axis=[1, 2, 3]\n",
    "        )\n",
    "        return W, Z, H\n",
    "\n",
    "    def do_estep(self, W, Z, H):\n",
    "        WZH = self.reconstruct(W, Z, H, circular=[self.circularF, self.circularT])\n",
    "        logprob = self.compute_logprob(W, Z, H, WZH)\n",
    "\n",
    "        WZ = W * Z[np.newaxis, :, np.newaxis]\n",
    "        VdivWZH = (self.V / (WZH + EPS))[:, :, np.newaxis]\n",
    "        self.VRW[:] = 0\n",
    "        self.VRH[:] = 0\n",
    "        for r in range(self.winF):\n",
    "            WZshifted = shift(WZ, r, 0, self.circularF)\n",
    "            for n, warp in enumerate(self.warpfactors):\n",
    "                for delay, tau in enumerate(self.taus[n]):\n",
    "                    Hshifted = (\n",
    "                        shift(H[:, r, n, :], delay, 1, self.circularT)\n",
    "                        * self.tauproportions[n][delay]\n",
    "                    )  # / warp) # FIXME\n",
    "                    tmp = (\n",
    "                        WZshifted[:, :, tau][:, :, np.newaxis]\n",
    "                        * Hshifted[np.newaxis, :, :]\n",
    "                    ).transpose((0, 2, 1)) * VdivWZH\n",
    "                    self.VRW[:, :, tau] += shift(tmp.sum(1), -r, 0, self.circularF)\n",
    "                    self.VRH[:, :, r, n] += shift(tmp.sum(0), -delay, 0, self.circularT)\n",
    "\n",
    "        return logprob, WZH\n",
    "\n",
    "    def do_mstep(self, curriter):\n",
    "        Zevidence = self._fix_negative_values(self.VRW.sum(2).sum(0) + self.alphaZ - 1)\n",
    "        initialZ = normalize(Zevidence)\n",
    "        Z = self._apply_entropic_prior_and_normalize(\n",
    "            initialZ, Zevidence, self.betaZ, nu=self.nu\n",
    "        )\n",
    "\n",
    "        # Factored W = P(f, \\tau | k) = P(f | \\tau, k) P(\\tau | k)\n",
    "        # P(f | \\tau, k)\n",
    "        Pf_evidence = self._fix_negative_values(self.VRW + self.alphaW - 1)\n",
    "        initialPf = normalize(Pf_evidence, 0)\n",
    "        Pf = self._apply_entropic_prior_and_normalize(\n",
    "            initialPf, Pf_evidence, self.betaW, nu=self.nu, axis=0\n",
    "        )\n",
    "\n",
    "        # P(\\tau | k)\n",
    "        Ptau_evidence = self._fix_negative_values(self.VRW.sum(0) + self.alphaT - 1)\n",
    "        initialPtau = normalize(Ptau_evidence, 1)\n",
    "        Ptau = self._apply_entropic_prior_and_normalize(\n",
    "            initialPtau, Ptau_evidence, self.betaT, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        # W = P(f, \\tau | k)\n",
    "        W = Pf * Ptau[np.newaxis, :, :]\n",
    "\n",
    "        # Factored H = P(t, r, w | k) = P(t | k) P(r, n | t, k)\n",
    "        # P(t | k)\n",
    "        Pt_evidence = self._fix_negative_values(\n",
    "            self.VRH.sum(3).sum(2).T + self.alphaH - 1\n",
    "        )\n",
    "        initialPt = normalize(Pt_evidence, 1)\n",
    "        Pt = self._apply_entropic_prior_and_normalize(\n",
    "            initialPt, Pt_evidence, self.betaH, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        # P(r, n | t, k)\n",
    "        Prn_evidence = self._fix_negative_values(\n",
    "            self.VRH.transpose((1, 2, 3, 0)) + self.alphaR - 1\n",
    "        )\n",
    "        initialPrn = normalize(Prn_evidence, [1, 2])\n",
    "        Prn = self._apply_entropic_prior_and_normalize(\n",
    "            initialPrn, Prn_evidence, self.betaR, nu=self.nu, axis=[1, 2]\n",
    "        )\n",
    "\n",
    "        # H = P(r, n, t | k)\n",
    "        H = Pt[:, np.newaxis, np.newaxis, :] * Prn\n",
    "\n",
    "        return self._prune_undeeded_bases(W, Z, H, curriter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8a4e0-3aa9-43a9-875c-a5b18170e5af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Segmenter Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9ba788-f70e-4c56-8238-38384fc1b85f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING segmenter 2024-02-13 21:44:38,484 2527244351.py:20  Unable to import mlab module.  Feature extraction and evaluation will not work.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import logging\n",
    "import optparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(levelname)s %(name)s %(asctime)s '\n",
    "                    '%(filename)s:%(lineno)d  %(message)s')\n",
    "logger = logging.getLogger('segmenter')\n",
    "\n",
    "try:\n",
    "    from mlabwrap import mlab\n",
    "    mlab.addpath('coversongs')\n",
    "except:\n",
    "    logger.warning('Unable to import mlab module.  Feature extraction '\n",
    "                   'and evaluation will not work.')\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(y, sr):\n",
    "    \"\"\"Computes beat-synchronous chroma features from the given wave file\n",
    "\n",
    "    Calls Dan Ellis' chrombeatftrs Matlab function.\n",
    "    \"\"\"\n",
    "    y_harm, y_perc = librosa.effects.hpss(y)\n",
    "    chromagram = librosa.feature.chroma_stft(y=y_harm, sr=sr, n_chroma=12, n_fft=4096)\n",
    "    chromagram_normalized = librosa.util.normalize(chromagram, norm=1, axis=0)\n",
    "    \n",
    "    logger.info(\"Extracting beat-synchronous chroma features from %s\", wavfilename)\n",
    "    x, fs = mlab.wavread(wavfilename, nout=2)\n",
    "    feats, beats = mlab.chrombeatftrs(\n",
    "        x.mean(1)[:, np.newaxis], fs, fctr, fsd, type, nout=2\n",
    "    )\n",
    "    songlen = x.shape[0] / fs\n",
    "    return feats, beats.flatten(), songlen\n",
    "\n",
    "\n",
    "def segment_song(\n",
    "    seq,\n",
    "    rank=4,\n",
    "    win=32,\n",
    "    seed=None,\n",
    "    nrep=1,\n",
    "    minsegments=3,\n",
    "    maxlowen=10,\n",
    "    maxretries=5,\n",
    "    uninformativeWinit=False,\n",
    "    uninformativeHinit=True,\n",
    "    normalize_frames=True,\n",
    "    viterbi_segmenter=False,\n",
    "    align_downbeats=False,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Segment the given feature sequence using SI-PLCA\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq : array, shape (F, T)\n",
    "        Feature sequence to segment.\n",
    "    rank : int\n",
    "        Number of patterns (unique segments) to search for.\n",
    "    win : int\n",
    "        Length of patterns in frames.\n",
    "    seed : int\n",
    "        Random number generator seed.  Defaults to None.\n",
    "    nrep : int\n",
    "        Number of times to repeat the analysis.  The repetition with\n",
    "        the lowest reconstrucion error is returned.  Defaults to 1.\n",
    "    minsegments : int\n",
    "        Minimum number of segments in the output.  The analysis is\n",
    "        repeated until the output contains at least `minsegments`\n",
    "        segments is or `maxretries` is reached.  Defaults to 3.\n",
    "    maxlowen : int\n",
    "        Maximum number of low energy frames in the SIPLCA\n",
    "        reconstruction.  The analysis is repeated if it contains too\n",
    "        many gaps.  Defaults to 10.\n",
    "    maxretries : int\n",
    "        Maximum number of retries to perform if `minsegments` or\n",
    "       `maxlowen` are not satisfied.  Defaults to 5.\n",
    "    uninformativeWinit : boolean\n",
    "        If True, `W` is initialized to have a flat distribution.\n",
    "        Defaults to False.\n",
    "    uninformativeHinit : boolean\n",
    "        If True, `H` is initialized to have a flat distribution.\n",
    "        Defaults to True.\n",
    "    normalize_frames : boolean\n",
    "        If True, normalizes each frame of `seq` so that the maximum\n",
    "        value is 1.  Defaults to True.\n",
    "    viterbi_segmenter : boolean\n",
    "        If True uses uses the Viterbi algorithm to convert SIPLCA\n",
    "        decomposition into segmentation, otherwises uses the process\n",
    "        described in [1].  Defaults to False.\n",
    "    align_downbeats : boolean\n",
    "        If True, postprocess the SIPLCA analysis to find the optimal\n",
    "        alignments of the components of W with V.  I.e. try to align\n",
    "        the first column of W to the downbeats in the song.  Defaults\n",
    "        to False.\n",
    "    kwargs : dict\n",
    "        Keyword arguments passed to plca.SIPLCA.analyze.  See\n",
    "        plca.SIPLCA for more details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : array, length `T`\n",
    "        Segment label for each frame of `seq`.\n",
    "    W : array, shape (`F`, `rank`, `win`)\n",
    "        Set of `F` x `win` shift-invariant basis functions found in `seq`.\n",
    "    Z : array, length `rank`\n",
    "        Set of mixing weights for each basis.\n",
    "    H : array, shape (`rank`, `T`)\n",
    "        Activations of each basis in time.\n",
    "    segfun : array, shape (`rank`, `T`)\n",
    "        Raw segmentation function used to generate segment labels from\n",
    "        SI-PLCA decomposition.  Corresponds to $\\ell_k(t)$ in [1].\n",
    "    norm : float\n",
    "        Normalization constant to make `seq` sum to 1.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The experimental results reported in [1] were found using the\n",
    "    default values for all keyword arguments while varying kwargs.\n",
    "\n",
    "    \"\"\"\n",
    "    seq = seq.copy()\n",
    "    if normalize_frames:\n",
    "        seq /= seq.max(0) + np.finfo(float).eps\n",
    "\n",
    "    logger.info(\"Using random seed %s.\", seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if \"alphaWcutoff\" in kwargs and \"alphaWslope\" in kwargs:\n",
    "        kwargs[\"alphaW\"] = create_sparse_W_prior(\n",
    "            (seq.shape[0], win), kwargs[\"alphaWcutoff\"], kwargs[\"alphaWslope\"]\n",
    "        )\n",
    "        del kwargs[\"alphaWcutoff\"]\n",
    "        del kwargs[\"alphaWslope\"]\n",
    "\n",
    "    F, T = seq.shape\n",
    "    if uninformativeWinit:\n",
    "        kwargs[\"initW\"] = np.ones((F, rank, win)) / (F * win)\n",
    "    if uninformativeHinit:\n",
    "        kwargs[\"initH\"] = np.ones((rank, T)) / T\n",
    "\n",
    "    outputs = []\n",
    "    for n in xrange(nrep):\n",
    "        outputs.append(plca.SIPLCA.analyze(seq, rank=rank, win=win, **kwargs))\n",
    "    div = [x[-1] for x in outputs]\n",
    "    W, Z, H, norm, recon, div = outputs[np.argmin(div)]\n",
    "\n",
    "    # Need to rerun segmentation if there are too few segments or\n",
    "    # if there are too many gaps in recon (i.e. H)\n",
    "    lowen = seq.shape[0] * np.finfo(float).eps\n",
    "    nlowen_seq = np.sum(seq.sum(0) <= lowen)\n",
    "    if nlowen_seq > maxlowen:\n",
    "        maxlowen = nlowen_seq\n",
    "    nlowen_recon = np.sum(recon.sum(0) <= lowen)\n",
    "    nretries = maxretries\n",
    "    while (len(Z) < minsegments or nlowen_recon > maxlowen) and nretries > 0:\n",
    "        nretries -= 1\n",
    "        logger.info(\n",
    "            \"Redoing SIPLCA analysis (len(Z) = %d, number of \"\n",
    "            \"low energy frames = %d).\",\n",
    "            len(Z),\n",
    "            nlowen_recon,\n",
    "        )\n",
    "        outputs = []\n",
    "        for n in xrange(nrep):\n",
    "            outputs.append(plca.SIPLCA.analyze(seq, rank=rank, win=win, **kwargs))\n",
    "        div = [x[-1] for x in outputs]\n",
    "        W, Z, H, norm, recon, div = outputs[np.argmin(div)]\n",
    "        nlowen_recon = np.sum(recon.sum(0) <= lowen)\n",
    "\n",
    "    if align_downbeats:\n",
    "        alignedW = plca.normalize(find_downbeat(seq, W) + 0.1 * np.finfo(float).eps, 1)\n",
    "        rank = len(Z)\n",
    "        if uninformativeHinit:\n",
    "            kwargs[\"initH\"] = np.ones((rank, T)) / T\n",
    "        if \"alphaZ\" in kwargs:\n",
    "            kwargs[\"alphaZ\"] = 0\n",
    "        W, Z, H, norm, recon, div = plca.SIPLCA.analyze(\n",
    "            seq, rank=rank, win=win, initW=alignedW, **kwargs\n",
    "        )\n",
    "\n",
    "    if viterbi_segmenter:\n",
    "        segmentation_function = nmf_analysis_to_segmentation_using_viterbi_path\n",
    "    else:\n",
    "        segmentation_function = nmf_analysis_to_segmentation\n",
    "    labels, segfun = segmentation_function(seq, win, W, Z, H, **kwargs)\n",
    "\n",
    "    return labels, W, Z, H, segfun, norm\n",
    "\n",
    "\n",
    "def create_sparse_W_prior(shape, cutoff, slope):\n",
    "    \"\"\"Constructs sparsity parameters for W (alphaW) to learn pattern length\n",
    "\n",
    "    Follows equation (6) in the ISMIR paper referenced in this\n",
    "    module's docstring.\n",
    "    \"\"\"\n",
    "\n",
    "    # W.shape is (ndim, nseg, nwin)\n",
    "    prior = np.zeros(shape[-1])\n",
    "    prior[cutoff:] = prior[0] + slope * np.arange(shape[-1] - cutoff)\n",
    "\n",
    "    alphaW = np.zeros((shape[0], 1, shape[-1]))\n",
    "    alphaW[:, :] = prior\n",
    "    return alphaW\n",
    "\n",
    "\n",
    "def nmf_analysis_to_segmentation(\n",
    "    seq,\n",
    "    win,\n",
    "    W,\n",
    "    Z,\n",
    "    H,\n",
    "    min_segment_length=32,\n",
    "    use_Z_for_segmentation=True,\n",
    "    **ignored_kwargs\n",
    "):\n",
    "    if not use_Z_for_segmentation:\n",
    "        Z = np.ones(Z.shape)\n",
    "\n",
    "    segfun = []\n",
    "    for n, (w, z, h) in enumerate(zip(np.transpose(W, (1, 0, 2)), Z, H)):\n",
    "        reconz = plca.SIPLCA.reconstruct(w, z, h)\n",
    "        score = np.sum(reconz, 0)\n",
    "\n",
    "        # Smooth it out\n",
    "        score = np.convolve(score, np.ones(min_segment_length), \"same\")\n",
    "        # kernel_size = min_segment_length\n",
    "        # if kernel_size % 2 == 0:\n",
    "        #     kernel_size += 1\n",
    "        # score = sp.signal.medfilt(score, kernel_size)\n",
    "        segfun.append(score)\n",
    "\n",
    "    # Combine correlated segment labels\n",
    "    C = np.reshape(\n",
    "        [\n",
    "            np.correlate(x, y, mode=\"full\")[: 2 * win].max()\n",
    "            for x in segfun\n",
    "            for y in segfun\n",
    "        ],\n",
    "        (len(segfun), len(segfun)),\n",
    "    )\n",
    "\n",
    "    segfun = np.array(segfun)\n",
    "    segfun /= segfun.max()\n",
    "\n",
    "    labels = np.argmax(np.asarray(segfun), 0)\n",
    "    remove_short_segments(labels, min_segment_length)\n",
    "\n",
    "    return labels, segfun\n",
    "\n",
    "\n",
    "def nmf_analysis_to_segmentation_using_viterbi_path(\n",
    "    seq,\n",
    "    win,\n",
    "    W,\n",
    "    Z,\n",
    "    H,\n",
    "    selfloopprob=0.9,\n",
    "    use_Z_for_segmentation=True,\n",
    "    min_segment_length=32,\n",
    "    **ignored_kwargs\n",
    "):\n",
    "    if not use_Z_for_segmentation:\n",
    "        Z = np.ones(Z.shape)\n",
    "\n",
    "    rank = len(Z)\n",
    "    T = H.shape[1]\n",
    "    likelihood = np.empty((rank, T))\n",
    "    for z in xrange(rank):\n",
    "        likelihood[z] = plca.SIPLCA.reconstruct(W[:, z], Z[z], H[z]).sum(0)\n",
    "\n",
    "    transmat = np.zeros((rank, rank))\n",
    "    for z in xrange(rank):\n",
    "        transmat[z, :] = (1 - selfloopprob) / (rank - 1 + np.finfo(float).eps)\n",
    "        transmat[z, z] = selfloopprob\n",
    "\n",
    "    # Find Viterbi path.\n",
    "    loglikelihood = np.log(likelihood)\n",
    "    logtransmat = np.log(transmat)\n",
    "    lattice = np.zeros(loglikelihood.shape)\n",
    "    traceback = np.zeros(loglikelihood.shape, dtype=np.int)\n",
    "    lattice[0] = loglikelihood[0]\n",
    "    for n in xrange(1, T):\n",
    "        pr = logtransmat.T + lattice[:, n - 1]\n",
    "        lattice[:, n] = np.max(pr, axis=1) + loglikelihood[:, n]\n",
    "        traceback[:, n] = np.argmax(pr, axis=1)\n",
    "\n",
    "    # Do traceback to find most likely path.\n",
    "    reverse_state_sequence = []\n",
    "    s = lattice[:, -1].argmax()\n",
    "    for frame in reversed(traceback.T):\n",
    "        reverse_state_sequence.append(s)\n",
    "        s = frame[s]\n",
    "    labels = np.array(list(reversed(reverse_state_sequence)))\n",
    "\n",
    "    remove_short_segments(labels, min_segment_length)\n",
    "\n",
    "    return labels, likelihood\n",
    "\n",
    "\n",
    "def remove_short_segments(labels, min_segment_length):\n",
    "    \"\"\"Remove segments shorter than min_segment_length.\"\"\"\n",
    "    segment_borders = np.nonzero(np.diff(labels))[0]\n",
    "    short_segments_idx = np.nonzero(np.diff(segment_borders) < min_segment_length)[0]\n",
    "    logger.info(\n",
    "        \"Removing %d segments shorter than %d frames\",\n",
    "        len(short_segments_idx),\n",
    "        min_segment_length,\n",
    "    )\n",
    "    # Remove all adjacent short_segments.\n",
    "    segment_borders[short_segments_idx]\n",
    "\n",
    "    for idx in short_segments_idx:\n",
    "        start = segment_borders[idx]\n",
    "        try:\n",
    "            end = segment_borders[idx + 1] + 1\n",
    "        except IndexError:\n",
    "            end = len(labels)\n",
    "\n",
    "        try:\n",
    "            label = labels[start - 1]\n",
    "        except IndexError:\n",
    "            label = labels[end]\n",
    "\n",
    "        labels[start:end] = label\n",
    "\n",
    "\n",
    "def evaluate_segmentation(labels, gtlabels, Z=[0]):\n",
    "    \"\"\"Calls Matlab to evaluate the given segmentation labels\n",
    "\n",
    "    labels and gtlabels are arrays containing a numerical label for\n",
    "    each frame of the sound (as returned by segment_song).\n",
    "\n",
    "    Returns a dictionary containing name-value pairs of the form\n",
    "    'metric name': value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Matlab is really picky about the shape of these vectors.  Make\n",
    "    # sure labels is a row vector.\n",
    "    nlabels = max(labels.shape)\n",
    "    if labels.ndim == 1:\n",
    "        labels = labels[np.newaxis, :]\n",
    "    elif labels.shape[0] == nlabels:\n",
    "        labels = labels.T\n",
    "\n",
    "    perf = {}\n",
    "    perf[\"pfm\"], perf[\"ppr\"], perf[\"prr\"] = mlab.eval_segmentation_pairwise(\n",
    "        labels, gtlabels, nout=3\n",
    "    )\n",
    "    perf[\"So\"], perf[\"Su\"] = mlab.eval_segmentation_entropy(labels, gtlabels, nout=2)\n",
    "\n",
    "    perf[\"nlabels\"] = len(np.unique(labels))\n",
    "    perf[\"effrank\"] = len(Z)\n",
    "    perf[\"nsegments\"] = np.sum(np.diff(labels) != 0) + 1\n",
    "\n",
    "    for k, v in perf.iteritems():\n",
    "        perf[k] = float(v)\n",
    "\n",
    "    return perf\n",
    "\n",
    "\n",
    "def compute_effective_pattern_length(w):\n",
    "    wsum = w.sum(0)\n",
    "    # Find all taus in w that contain significant probability mass.\n",
    "    (nonzero_idx,) = np.nonzero(wsum > wsum.min())\n",
    "    winlen = nonzero_idx[-1] - nonzero_idx[0] + 1\n",
    "    return winlen\n",
    "\n",
    "\n",
    "def convert_labels_to_segments(labels, frametimes, songlen=None):\n",
    "    \"\"\"Covert frame-wise segmentation labels to a list of segments in HTK\n",
    "    format\"\"\"\n",
    "\n",
    "    # Nonzero points in diff(labels) correspond to the final frame of\n",
    "    # a segment (so just index into labels to find the segment label)\n",
    "    boundaryidx = np.concatenate(\n",
    "        ([0], np.nonzero(np.diff(labels))[0], [len(labels) - 1])\n",
    "    )\n",
    "    boundarytimes = frametimes[boundaryidx]\n",
    "\n",
    "    segstarttimes = boundarytimes[:-1]\n",
    "    segendtimes = boundarytimes[1:]\n",
    "    seglabels = labels[boundaryidx[1:]]\n",
    "\n",
    "    labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    segments = [\n",
    "        \"%.4f\\t%.4f\\t%s\" % (start, end, labels[label])\n",
    "        for start, end, label in zip(segstarttimes, segendtimes, seglabels)\n",
    "    ]\n",
    "\n",
    "    # Add silence before first beat and after last beat.\n",
    "    silencelabel = labels[seglabels.max() + 1]\n",
    "    segments = [\"0.0\\t%.4f\\t%s\" % (segstarttimes[0], silencelabel)] + segments\n",
    "    if songlen:\n",
    "        segments += [\"%.4f\\t%.4f\\t%s\" % (segendtimes[-1], songlen, silencelabel)]\n",
    "\n",
    "    return \"\\n\".join(segments + [\"\"])\n",
    "\n",
    "\n",
    "def _compute_summary_correlation(A, B):\n",
    "    return sum(np.correlate(A[x], B[x], \"full\") for x in xrange(A.shape[0]))\n",
    "\n",
    "\n",
    "def find_downbeat(V, W):\n",
    "    newW = W.copy()\n",
    "    for k in xrange(W.shape[1]):\n",
    "        Wlen = compute_effective_pattern_length(W[:, k, :])\n",
    "        Wk = W[:, k, :Wlen]\n",
    "        corr = np.array(\n",
    "            [\n",
    "                _compute_summary_correlation(plca.shift(Wk, r, 1), V)\n",
    "                for r in xrange(Wlen)\n",
    "            ]\n",
    "        )\n",
    "        bestshift = corr[:, Wlen:-Wlen].sum(1).argmin()\n",
    "        print(k, Wlen, bestshift)\n",
    "        newW[:, k, :Wlen] = plca.shift(Wk, bestshift, 1)\n",
    "    return newW\n",
    "\n",
    "\n",
    "def find_downbeat_slow(V, W, Z, H, **kwargs):\n",
    "    bopt = np.zeros(len(Z), dtype=int)\n",
    "    nW = np.zeros(W.shape)\n",
    "    nH = np.zeros(H.shape)\n",
    "    for k in np.argsort(Z)[::-1]:\n",
    "        Wlen = compute_effective_pattern_length(W[:, k, :])\n",
    "        params = []\n",
    "        logprobs = []\n",
    "        for b in xrange(Wlen):\n",
    "            initW = W\n",
    "            initW[:, k, :Wlen] = plca.shift(W[:, k, :Wlen], b, axis=1)\n",
    "            W, Z, H, norm, recon, logprob = plca.FactoredSIPLCA2.analyze(\n",
    "                V,\n",
    "                rank=len(Z),\n",
    "                win=[H.shape[1], W.shape[-1]],\n",
    "                niter=50,\n",
    "                circular=[True, False],\n",
    "                initW=initW,\n",
    "                initH=np.ones(H.shape),\n",
    "                initZ=Z,\n",
    "                **kwargs\n",
    "            )\n",
    "            params.append((W, Z, H))\n",
    "            logprobs.append(logprob)\n",
    "            print(b, logprobs[-1])\n",
    "        bopt[k] = np.argmax(logprobs)\n",
    "        W[:, k] = params[bopt[k]][0][:, k]\n",
    "        nH[k] = params[bopt[k]][2][k]\n",
    "    return bopt, W, Z, nH\n",
    "\n",
    "\n",
    "def shift_key_to_zero(W, Z, H):\n",
    "    newW = np.zeros(W.shape)\n",
    "    newH = np.zeros(H.shape)\n",
    "    for k in xrange(len(Z)):\n",
    "        key_profile = H[k].sum(1)\n",
    "        main_key = np.argmax(key_profile)\n",
    "        newW[:, k] = plca.shift(W[:, k], main_key, axis=0, circular=True)\n",
    "        newH[k] = plca.shift(H[k], -main_key, axis=0, circular=True)\n",
    "    return newW, Z, newH\n",
    "\n",
    "\n",
    "def segment_wavfile(wavfile, **kwargs):\n",
    "    \"\"\"Convenience function to compute segmentation of the given wavfile\n",
    "\n",
    "    Keyword arguments are passed into segment_song.\n",
    "\n",
    "    Returns a string containing list of segments in HTK label format.\n",
    "    \"\"\"\n",
    "    features, beattimes, songlen = extract_features(wavfile)\n",
    "    labels, W, Z, H, segfun, norm = segment_song(features, **kwargs)\n",
    "    segments = convert_labels_to_segments(labels, beattimes, songlen)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cda1a-d7ca-494d-8aa1-12d6f6d8c11c",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b01fa3-75cd-4763-b52b-4be91f67cd4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_path = \"../data/audio_files/processed/2.mp3\"\n",
    "df = pd.read_csv(\"../data/dataframes/clean_labeled.csv\")\n",
    "data = df.loc[df[\"SongID\"] == 2]\n",
    "sp_tempo = data[\"sp_tempo\"].values[0]\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "y_harm, y_perc = librosa.effects.hpss(y)\n",
    "chromagram = librosa.feature.chroma_stft(y=y_harm, sr=sr, n_chroma=12, n_fft=4096)\n",
    "chromagram_normalized = librosa.util.normalize(chromagram, norm=1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b0e239-7dc5-452b-9fde-5a5dc3cfb464",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m niter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of iterations for EM algorithm\u001b[39;00m\n\u001b[0;32m      5\u001b[0m siplca \u001b[38;5;241m=\u001b[39m SIPLCA(chromagram_normalized, rank\u001b[38;5;241m=\u001b[39mrank, win\u001b[38;5;241m=\u001b[39mwin)\n\u001b[1;32m----> 6\u001b[0m W, Z, H, norm, recon, logprob \u001b[38;5;241m=\u001b[39m \u001b[43mSIPLCA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchromagram_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mniter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 301\u001b[0m, in \u001b[0;36mPLCA.analyze\u001b[1;34m(cls, V, rank, niter, convergence_thresh, printiter, plotiter, plotfilename, initW, initZ, initH, updateW, updateZ, updateH, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m oldlogprob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(niter):\n\u001b[1;32m--> 301\u001b[0m     logprob, WZH \u001b[38;5;241m=\u001b[39m \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_estep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m printiter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    303\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: logprob = \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, n, logprob)\n",
      "Cell \u001b[1;32mIn[5], line 615\u001b[0m, in \u001b[0;36mSIPLCA.do_estep\u001b[1;34m(self, W, Z, H)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_estep\u001b[39m(\u001b[38;5;28mself\u001b[39m, W, Z, H):\n\u001b[1;32m--> 615\u001b[0m     WZH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircular\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m     logprob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_logprob(W, Z, H, WZH)\n\u001b[0;32m    618\u001b[0m     WZ \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m*\u001b[39m Z[np\u001b[38;5;241m.\u001b[39mnewaxis, :, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "Cell \u001b[1;32mIn[5], line 531\u001b[0m, in \u001b[0;36mSIPLCA.reconstruct\u001b[1;34m(W, Z, H, norm, circular)\u001b[0m\n\u001b[0;32m    529\u001b[0m WZH \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((F, T))\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tau \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(win):\n\u001b[1;32m--> 531\u001b[0m     WZH \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(W[:, :, tau] \u001b[38;5;241m*\u001b[39m Z, \u001b[43mshift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircular\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m norm \u001b[38;5;241m*\u001b[39m WZH\n",
      "Cell \u001b[1;32mIn[5], line 99\u001b[0m, in \u001b[0;36mshift\u001b[1;34m(a, shift, axis, circular)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m shift \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     98\u001b[0m             index[axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(shift, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 99\u001b[0m         aroll[index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m aroll\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Parameters for SI-PLCA\n",
    "rank = 5  # Number of components\n",
    "win = 3  # Window size for shift invariance\n",
    "niter = 100  # Number of iterations for EM algorithm\n",
    "siplca = SIPLCA(chromagram_normalized, rank=rank, win=win)\n",
    "W, Z, H, norm, recon, logprob = SIPLCA.analyze(\n",
    "    chromagram_normalized, rank=rank, niter=niter, win=win\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec50c8d-6a7a-45e5-956f-fc7ec30e2441",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

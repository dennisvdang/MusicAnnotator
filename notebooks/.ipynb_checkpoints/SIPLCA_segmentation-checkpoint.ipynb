{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d057bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b313b931",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9f9192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"plca\")\n",
    "EPS = np.finfo(float).eps\n",
    "\n",
    "def segment_audiofile(audiofile, **kwargs):\n",
    "    \"\"\"Convenience function to compute segmentation of the given audio file\"\"\"\n",
    "    features, beattimes, songlen = extract_features(audiofile)\n",
    "    labels, W, Z, H, segfun, norm = segment_song(features, **kwargs)\n",
    "    segments = convert_labels_to_segments(labels, beattimes, songlen)\n",
    "    return segments\n",
    "\n",
    "def extract_features(wavfile):\n",
    "    \"\"\"Computes beat-synchronous chroma features from the given wave file using librosa\"\"\"\n",
    "    y, sr = librosa.load(wavfile)\n",
    "    y_harm, y_perc = librosa.effects.hpss(y)\n",
    "    chromagram = librosa.feature.chroma_stft(y=y_harm, sr=sr, n_chroma=12, n_fft=4096)\n",
    "    chromagram_normalized = librosa.util.normalize(chromagram, norm=1, axis=0)\n",
    "    \n",
    "    logger.info(\"Extracting beat-synchronous chroma features from %s\", wavfile)\n",
    "    tempo, beats = librosa.beat.beat_track(y=y_perc, sr=sr)\n",
    "    songlen = librosa.get_duration(y=y, sr=sr)\n",
    "    return chromagram_normalized, beats, songlen\n",
    "\n",
    "\n",
    "def segment_song(seq, rank=4, win=32, seed=None, nrep=1, minsegments=3, maxlowen=10, maxretries=5, uninformativeWinit=False, uninformativeHinit=True, normalize_frames=True, viterbi_segmenter=False, align_downbeats=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Segment a song using the SIPLCA algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    seq (numpy.ndarray): Input sequence.\n",
    "    rank (int): Rank of the decomposition.\n",
    "    win (int): Window size.\n",
    "    seed (int): Random seed.\n",
    "    nrep (int): Number of repetitions.\n",
    "    minsegments (int): Minimum number of segments required.\n",
    "    maxlowen (int): Maximum number of low energy frames allowed.\n",
    "    maxretries (int): Maximum number of retries.\n",
    "    uninformativeWinit (bool): Whether to use uninformative initialization for W.\n",
    "    uninformativeHinit (bool): Whether to use uninformative initialization for H.\n",
    "    normalize_frames (bool): Whether to normalize the frames.\n",
    "    viterbi_segmenter (bool): Whether to use the Viterbi segmenter.\n",
    "    align_downbeats (bool): Whether to align downbeats.\n",
    "    **kwargs: Additional keyword arguments.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the labels, W, Z, H, segmentation function, and norm.\n",
    "    \"\"\"\n",
    "    seq = seq.copy()\n",
    "    if normalize_frames:\n",
    "        seq /= seq.max(0) + np.finfo(float).eps\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if \"alphaWcutoff\" in kwargs and \"alphaWslope\" in kwargs:\n",
    "        kwargs[\"alphaW\"] = create_sparse_W_prior((seq.shape[0], win), kwargs[\"alphaWcutoff\"], kwargs[\"alphaWslope\"])\n",
    "        del kwargs[\"alphaWcutoff\"]\n",
    "        del kwargs[\"alphaWslope\"]\n",
    "\n",
    "    F, T = seq.shape\n",
    "    if uninformativeWinit:\n",
    "        kwargs[\"initW\"] = np.ones((F, rank, win)) / (F * win)\n",
    "    if uninformativeHinit:\n",
    "        kwargs[\"initH\"] = np.ones((rank, T)) / T\n",
    "\n",
    "    outputs = [SIPLCA.analyze(seq, rank=rank, win=win, **kwargs) for _ in range(nrep)]\n",
    "    div = [x[-1] for x in outputs]\n",
    "    W, Z, H, norm, recon, div = outputs[np.argmin(div)]\n",
    "\n",
    "    lowen = seq.shape[0] * np.finfo(float).eps\n",
    "    nlowen_seq = np.sum(seq.sum(0) <= lowen)\n",
    "    maxlowen = max(nlowen_seq, maxlowen)\n",
    "    nlowen_recon = np.sum(recon.sum(0) <= lowen)\n",
    "    nretries = maxretries\n",
    "    while (len(Z) < minsegments or nlowen_recon > maxlowen) and nretries > 0:\n",
    "        nretries -= 1\n",
    "        outputs = [SIPLCA.analyze(seq, rank=rank, win=win, **kwargs) for _ in range(nrep)]\n",
    "        div = [x[-1] for x in outputs]\n",
    "        W, Z, H, norm, recon, div = outputs[np.argmin(div)]\n",
    "        nlowen_recon = np.sum(recon.sum(0) <= lowen)\n",
    "\n",
    "    if align_downbeats:\n",
    "        alignedW = normalize(find_downbeat(seq, W) + 0.1 * np.finfo(float).eps, 1)\n",
    "        rank = len(Z)\n",
    "        if uninformativeHinit:\n",
    "            kwargs[\"initH\"] = np.ones((rank, T)) / T\n",
    "        if \"alphaZ\" in kwargs:\n",
    "            kwargs[\"alphaZ\"] = 0\n",
    "        W, Z, H, norm, recon, div = SIPLCA.analyze(seq, rank=rank, win=win, initW=alignedW, **kwargs)\n",
    "\n",
    "    segmentation_function = nmf_analysis_to_segmentation_using_viterbi_path if viterbi_segmenter else nmf_analysis_to_segmentation\n",
    "    labels, segfun = segmentation_function(seq, win, W, Z, H, **kwargs)\n",
    "\n",
    "    return labels, W, Z, H, segfun, norm\n",
    "\n",
    "\n",
    "def create_sparse_W_prior(shape, cutoff, slope):\n",
    "    \"\"\"Constructs sparsity parameters for W (alphaW) to learn pattern lengths.\"\"\"\n",
    "    prior = np.zeros(shape[-1])\n",
    "    prior[cutoff:] = prior[0] + slope * np.arange(shape[-1] - cutoff)\n",
    "\n",
    "    alphaW = np.zeros((shape[0], 1, shape[-1]))\n",
    "    alphaW[:, :] = prior\n",
    "    return alphaW\n",
    "\n",
    "\n",
    "def normalize(A, axis=None):\n",
    "    EPS = 1e-8\n",
    "    Ashape = A.shape\n",
    "    \n",
    "    try:\n",
    "        norm = A.sum(axis) + EPS\n",
    "    except TypeError:\n",
    "        norm = A.copy()\n",
    "        for ax in reversed(sorted(axis)):\n",
    "            norm = norm.sum(ax)\n",
    "        norm += EPS\n",
    "    \n",
    "    if axis:\n",
    "        nshape = np.array(Ashape)\n",
    "        nshape[axis] = 1\n",
    "        norm.shape = nshape\n",
    "    \n",
    "    return A / norm\n",
    "\n",
    "\n",
    "def shift(a, shift, axis=None, circular=True):\n",
    "    \"\"\"\n",
    "    Shifts the array `a` along the given `axis`.\n",
    "    \"\"\"\n",
    "    aroll = np.roll(a, shift, axis)\n",
    "    \n",
    "    if not circular and shift != 0:\n",
    "        if axis is None:\n",
    "            aroll_flattened = aroll.flatten()\n",
    "            \n",
    "            if shift > 0:\n",
    "                aroll_flattened[:shift] = 0\n",
    "            elif shift < 0:\n",
    "                aroll_flattened[shift:] = 0\n",
    "            \n",
    "            aroll = np.reshape(aroll_flattened, aroll.shape)\n",
    "        else:\n",
    "            index = [slice(None)] * a.ndim\n",
    "            \n",
    "            if shift > 0:\n",
    "                index[axis] = slice(0, shift)\n",
    "            elif shift < 0:\n",
    "                index[axis] = slice(shift, None)\n",
    "            \n",
    "            aroll[tuple(index)] = 0\n",
    "    \n",
    "    return aroll\n",
    "\n",
    "\n",
    "def find_downbeat(V, W):\n",
    "    \"\"\"\n",
    "    Finds the downbeat in the given audio signal `V` using the pattern matrix `W`.\n",
    "    \"\"\"\n",
    "    newW = W.copy()\n",
    "    \n",
    "    for k in range(W.shape[1]):\n",
    "        Wlen = compute_effective_pattern_length(W[:, k, :])\n",
    "        Wk = W[:, k, :Wlen]\n",
    "        \n",
    "        onset_env = librosa.onset.onset_strength(V)\n",
    "        _, beat_frames = librosa.beat.beat_track(onset_envelope=onset_env)\n",
    "        beat_frames_sync = librosa.util.sync(V, beat_frames)\n",
    "        \n",
    "        bestshift = beat_frames_sync[Wlen:-Wlen].sum() // len(beat_frames_sync[Wlen:-Wlen])\n",
    "        \n",
    "        print(k, Wlen, bestshift)\n",
    "        \n",
    "        newW[:, k, :Wlen] = shift(Wk, bestshift, 1)\n",
    "    \n",
    "    return newW\n",
    "\n",
    "\n",
    "def nmf_analysis_to_segmentation(seq, win, W, Z, H, min_segment_length=32, use_Z_for_segmentation=True, **ignored_kwargs):\n",
    "    \"\"\"\n",
    "    Perform segmentation based on NMF analysis.\n",
    "    \"\"\"\n",
    "    if not use_Z_for_segmentation:\n",
    "        Z = np.ones(Z.shape)\n",
    "\n",
    "    segfun = []\n",
    "    for n, (w, z, h) in enumerate(zip(np.transpose(W, (1, 0, 2)), Z, H)):\n",
    "        reconz = PLCA.reconstruct(w, z, h)\n",
    "        score = np.sum(reconz, 0)\n",
    "\n",
    "        # Smooth it out\n",
    "        score = np.convolve(score, np.ones(min_segment_length), \"same\")\n",
    "        segfun.append(score)\n",
    "\n",
    "    # Combine correlated segment labels\n",
    "    C = np.reshape(\n",
    "        [\n",
    "            np.correlate(x, y, mode=\"full\")[: 2 * win].max()\n",
    "            for x in segfun\n",
    "            for y in segfun\n",
    "        ],\n",
    "        (len(segfun), len(segfun)),\n",
    "    )\n",
    "\n",
    "    segfun = np.array(segfun)\n",
    "    segfun /= segfun.max()\n",
    "\n",
    "    labels = np.argmax(np.asarray(segfun), 0)\n",
    "    remove_short_segments(labels, min_segment_length)\n",
    "\n",
    "    return labels, segfun\n",
    "\n",
    "\n",
    "def nmf_analysis_to_segmentation_using_viterbi_path(seq, win, W, Z, H, selfloopprob=0.9, use_Z_for_segmentation=True, min_segment_length=32, **ignored_kwargs):\n",
    "    \"\"\"\n",
    "    Perform segmentation using Viterbi path based on NMF analysis.\n",
    "\n",
    "    Parameters:\n",
    "    seq (np.ndarray): Input sequence.\n",
    "    win (int): Window size.\n",
    "    W (np.ndarray): NMF basis matrix.\n",
    "    Z (np.ndarray): NMF activation matrix.\n",
    "    H (np.ndarray): NMF coefficient matrix.\n",
    "    selfloopprob (float, optional): Self-loop probability.\n",
    "    use_Z_for_segmentation (bool, optional): Flag indicating whether to use Z for segmentation.\n",
    "    min_segment_length (int, optional): Minimum segment length.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Segmentation labels.\n",
    "    np.ndarray: Likelihood.\n",
    "\n",
    "    \"\"\"\n",
    "    if not use_Z_for_segmentation:\n",
    "        Z = np.ones(Z.shape)\n",
    "\n",
    "    rank = len(Z)\n",
    "    T = H.shape[1]\n",
    "    likelihood = np.empty((rank, T))\n",
    "    for z in range(rank):\n",
    "        likelihood[z] = PLCA.reconstruct(W[:, z], Z[z], H[z]).sum(0)\n",
    "\n",
    "    transmat = np.zeros((rank, rank))\n",
    "    for z in range(rank):\n",
    "        transmat[z, :] = (1 - selfloopprob) / (rank - 1 + np.finfo(float).eps)\n",
    "        transmat[z, z] = selfloopprob\n",
    "\n",
    "    # Find Viterbi path.\n",
    "    loglikelihood = np.log(likelihood)\n",
    "    logtransmat = np.log(transmat)\n",
    "    lattice = np.zeros(loglikelihood.shape)\n",
    "    traceback = np.zeros(loglikelihood.shape, dtype=np.int)\n",
    "    lattice[0] = loglikelihood[0]\n",
    "    for n in range(1, T):\n",
    "        pr = logtransmat.T + lattice[:, n - 1]\n",
    "        lattice[:, n] = np.max(pr, axis=1) + loglikelihood[:, n]\n",
    "        traceback[:, n] = np.argmax(pr, axis=1)\n",
    "\n",
    "    # Do traceback to find most likely path.\n",
    "    reverse_state_sequence = []\n",
    "    s = lattice[:, -1].argmax()\n",
    "    for frame in reversed(traceback.T):\n",
    "        reverse_state_sequence.append(s)\n",
    "        s = frame[s]\n",
    "    labels = np.array(list(reversed(reverse_state_sequence)))\n",
    "\n",
    "    remove_short_segments(labels, min_segment_length)\n",
    "\n",
    "    return labels, likelihood\n",
    "\n",
    "\n",
    "def remove_short_segments(labels, min_segment_length):\n",
    "    \"\"\"Remove segments shorter than min_segment_length.\"\"\"\n",
    "    segment_borders = np.nonzero(np.diff(labels))[0]\n",
    "    short_segments_idx = np.nonzero(np.diff(segment_borders) < min_segment_length)[0]\n",
    "    logger.info(\n",
    "        \"Removing %d segments shorter than %d frames\",\n",
    "        len(short_segments_idx),\n",
    "        min_segment_length,\n",
    "    )\n",
    "    \n",
    "    # Remove all adjacent short_segments.\n",
    "    segment_borders = np.delete(segment_borders, short_segments_idx)\n",
    "\n",
    "    for idx in short_segments_idx:\n",
    "        start = segment_borders[idx]\n",
    "        try:\n",
    "            end = segment_borders[idx + 1] + 1\n",
    "        except IndexError:\n",
    "            end = len(labels)\n",
    "\n",
    "        try:\n",
    "            label = labels[start - 1]\n",
    "        except IndexError:\n",
    "            label = labels[end]\n",
    "\n",
    "        labels[start:end] = label\n",
    "        \n",
    "        \n",
    "def compute_effective_pattern_length(w):\n",
    "    \"\"\"Compute the effective pattern length based on the sum of probabilities in w.\"\"\"\n",
    "    wsum = w.sum(0)\n",
    "    nonzero_idx = np.nonzero(wsum > wsum.min())[0]\n",
    "    winlen = nonzero_idx[-1] - nonzero_idx[0] + 1\n",
    "    return winlen\n",
    "\n",
    "\n",
    "def convert_labels_to_segments(labels, frametimes, songlen=None):\n",
    "    \"\"\"Convert frame-wise segmentation labels to a list of segments in HTK format.\"\"\"\n",
    "    boundaryidx = np.concatenate(([0], np.nonzero(np.diff(labels))[0], [len(labels) - 1]))\n",
    "    boundarytimes = frametimes[boundaryidx]\n",
    "\n",
    "    segstarttimes = boundarytimes[:-1]\n",
    "    segendtimes = boundarytimes[1:]\n",
    "    seglabels = labels[boundaryidx[1:]]\n",
    "\n",
    "    labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    segments = [\n",
    "        \"%.4f\\t%.4f\\t%s\" % (start, end, labels[label])\n",
    "        for start, end, label in zip(segstarttimes, segendtimes, seglabels)\n",
    "    ]\n",
    "\n",
    "    silencelabel = labels[seglabels.max() + 1]\n",
    "    segments = [\"0.0\\t%.4f\\t%s\" % (segstarttimes[0], silencelabel)] + segments\n",
    "    if songlen:\n",
    "        segments += [\"%.4f\\t%.4f\\t%s\" % (segendtimes[-1], songlen, silencelabel)]\n",
    "\n",
    "    return \"\\n\".join(segments + [\"\"])\n",
    "\n",
    "\n",
    "class PLCA(object):\n",
    "    \"\"\"Probabilistic Latent Component Analysis\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    analyze\n",
    "        Performs PLCA decomposition using the EM algorithm from [2].\n",
    "    reconstruct(W, Z, H, norm=1.0)\n",
    "        Reconstructs input matrix from the PLCA parameters W, Z, and H.\n",
    "    plot(V, W, Z, H)\n",
    "        Makes a pretty plot of V and the decomposition.\n",
    "\n",
    "    initialize()\n",
    "        Randomly initializes the parameters.\n",
    "    do_estep(W, Z, H)\n",
    "        Performs the E-step of the EM parameter estimation algorithm.\n",
    "    do_mstep()\n",
    "        Performs the M-step of the EM parameter estimation algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        V,\n",
    "        rank,\n",
    "        alphaW=0,\n",
    "        alphaZ=0,\n",
    "        alphaH=0,\n",
    "        betaW=0,\n",
    "        betaZ=0,\n",
    "        betaH=0,\n",
    "        nu=50.0,\n",
    "        minpruneiter=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        self.V = V.copy()\n",
    "        self.rank = rank\n",
    "\n",
    "        self.F, self.T = self.V.shape\n",
    "\n",
    "        # Allocate the sufficient statistics here, so they don't have to be\n",
    "        # reallocated at every iteration.  This becomes especially important\n",
    "        # for the more sophistacted models with many hidden variables.\n",
    "        self.VRW = np.empty((self.F, self.rank))\n",
    "        self.VRH = np.empty((self.T, self.rank))\n",
    "\n",
    "        self.alphaW = 1 + alphaW\n",
    "        self.alphaZ = 1 + alphaZ\n",
    "        self.alphaH = 1 + alphaH\n",
    "\n",
    "        if betaW < 0 or betaZ < 0 or betaH < 0:\n",
    "            raise ValueError(\n",
    "                \"Entropic prior parameters beta{W,Z,H} must be \" \"non-negative\"\n",
    "            )\n",
    "        self.betaW = betaW\n",
    "        self.betaZ = betaZ\n",
    "        self.betaH = betaH\n",
    "        self.nu = nu\n",
    "\n",
    "        self.minpruneiter = minpruneiter\n",
    "\n",
    "    @classmethod\n",
    "    def analyze(\n",
    "        cls,\n",
    "        V,\n",
    "        rank,\n",
    "        niter=100,\n",
    "        convergence_thresh=1e-9,\n",
    "        printiter=50,\n",
    "        plotiter=None,\n",
    "        plotfilename=None,\n",
    "        initW=None,\n",
    "        initZ=None,\n",
    "        initH=None,\n",
    "        updateW=True,\n",
    "        updateZ=True,\n",
    "        updateH=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Iteratively performs the PLCA decomposition using the EM algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : array, shape (`F`, `T`)\n",
    "            Matrix to analyze.\n",
    "        niter : int\n",
    "            Number of iterations to perform.  Defaults to 100.\n",
    "        convergence_thresh : float\n",
    "        updateW, updateZ, updateH : boolean\n",
    "            If False keeps the corresponding parameter fixed.\n",
    "            Defaults to True.\n",
    "        initW, initZ, initH : array\n",
    "            Initial settings for `W`, `Z`, and `H`.  Unused by default.\n",
    "        printiter : int\n",
    "            Prints current log probability once every `printiter`\n",
    "            iterations.  Defaults to 50.\n",
    "        plotiter : int or None\n",
    "            If not None, the current decomposition is plotted once\n",
    "            every `plotiter` iterations.  Defaults to None.\n",
    "        kwargs : dict\n",
    "            Arguments to pass into the class's constructor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        W : array, shape (`F`, `rank`)\n",
    "            Set of `rank` bases found in `V`, i.e. P(f | z).\n",
    "        Z : array, shape (`rank`)\n",
    "            Mixing weights over basis vector activations, i.e. P(z).\n",
    "        H : array, shape (`rank`, `T`)\n",
    "            Activations of each basis in time, i.e. P(t | z).\n",
    "        norm : float\n",
    "            Normalization constant to make `V` sum to 1.\n",
    "        recon : array\n",
    "            Reconstruction of `V` using `W`, `Z`, and `H`\n",
    "        logprob : float\n",
    "        \"\"\"\n",
    "        norm = V.sum()\n",
    "        V /= norm\n",
    "\n",
    "        params = cls(V, rank, **kwargs)\n",
    "        iW, iZ, iH = params.initialize()\n",
    "\n",
    "        W = iW if initW is None else initW.copy()\n",
    "        Z = iZ if initZ is None else initZ.copy()\n",
    "        H = iH if initH is None else initH.copy()\n",
    "\n",
    "        params.W = W\n",
    "        params.Z = Z\n",
    "        params.H = H\n",
    "\n",
    "        oldlogprob = -np.inf\n",
    "        for n in range(niter):\n",
    "            logprob, WZH = params.do_estep(W, Z, H)\n",
    "            if n % printiter == 0:\n",
    "                logger.info(\"Iteration %d: logprob = %f\", n, logprob)\n",
    "            if logprob < oldlogprob:\n",
    "                logger.debug(\n",
    "                    \"Warning: logprob decreased from %f to %f at \" \"iteration %d!\",\n",
    "                    oldlogprob,\n",
    "                    logprob,\n",
    "                    n,\n",
    "                )\n",
    "                # import pdb; pdb.set_trace()\n",
    "            elif n > 0 and logprob - oldlogprob < convergence_thresh:\n",
    "                logger.info(\"Converged at iteration %d\", n)\n",
    "                break\n",
    "            oldlogprob = logprob\n",
    "\n",
    "            nW, nZ, nH = params.do_mstep(n)\n",
    "\n",
    "            if updateW:\n",
    "                W = nW\n",
    "            if updateZ:\n",
    "                Z = nZ\n",
    "            if updateH:\n",
    "                H = nH\n",
    "\n",
    "            params.W = W\n",
    "            params.Z = Z\n",
    "            params.H = H\n",
    "\n",
    "        logger.info(\"Iteration %d: final logprob = %f\", n, logprob)\n",
    "        recon = norm * WZH\n",
    "        return W, Z, H, norm, recon, logprob\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(W, Z, H, norm=1.0):\n",
    "        \"\"\"Computes the approximation to V using W, Z, and H\"\"\"\n",
    "        print(\"Shape of W:\", W.shape)\n",
    "        print(\"Shape of Z:\", Z.shape)\n",
    "        print(\"Shape of H:\", H.shape)\n",
    "        return norm * np.dot(W.T * Z, H)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initializes the parameters\n",
    "\n",
    "        W and H are initialized randomly.  Z is initialized to have a\n",
    "        uniform distribution.\n",
    "        \"\"\"\n",
    "        W = normalize(np.random.rand(self.F, self.rank), 0)\n",
    "        Z = np.ones(self.rank) / self.rank\n",
    "        H = normalize(np.random.rand(self.rank, self.T), 1)\n",
    "        return W, Z, H\n",
    "\n",
    "    def compute_logprob(self, W, Z, H, recon):\n",
    "        logprob = np.sum(self.V * np.log(recon + EPS * recon))\n",
    "        # Add Dirichlet and Entropic priors.\n",
    "        logprob += (\n",
    "            np.sum((self.alphaW - 1) * np.log(W + EPS * W))\n",
    "            + np.sum((self.alphaZ - 1) * np.log(Z + EPS * Z))\n",
    "            + np.sum((self.alphaH - 1) * np.log(H + EPS * H))\n",
    "        )\n",
    "        # Add Entropic priors.\n",
    "        logprob += (\n",
    "            self.betaW * np.sum(W * np.log(W + EPS * W))\n",
    "            + self.betaZ * np.sum(Z * np.log(Z + EPS * Z))\n",
    "            + self.betaH * np.sum(H * np.log(H + EPS * H))\n",
    "        )\n",
    "        return logprob\n",
    "\n",
    "    def do_estep(self, W, Z, H):\n",
    "        \"\"\"Performs the E-step of the EM parameter estimation algorithm.\n",
    "\n",
    "        Computes the posterior distribution over the hidden variables.\n",
    "        \"\"\"\n",
    "        WZH = self.reconstruct(W, Z, H)\n",
    "        logprob = self.compute_logprob(W, Z, H, WZH)\n",
    "\n",
    "        VdivWZH = self.V / WZH\n",
    "        for z in range(self.rank):\n",
    "            tmp = (W[:, z] * Z[z])[:, np.newaxis] @ H[z, :] * VdivWZH\n",
    "            self.VRW[:, z] = np.sum(tmp, axis=1)\n",
    "            self.VRH[:, z] = np.sum(tmp, axis=0)\n",
    "\n",
    "        return logprob, WZH\n",
    "\n",
    "    def do_mstep(self, curriter):\n",
    "        \"\"\"Performs the M-step of the EM parameter estimation algorithm.\n",
    "\n",
    "        Computes updated estimates of W, Z, and H using the posterior\n",
    "        distribution computer in the E-step.\n",
    "        \"\"\"\n",
    "        Zevidence = self._fix_negative_values(self.VRW.sum(0) + self.alphaZ - 1)\n",
    "        initialZ = normalize(Zevidence)\n",
    "        Z = self._apply_entropic_prior_and_normalize(\n",
    "            initialZ, Zevidence, self.betaZ, nu=self.nu\n",
    "        )\n",
    "\n",
    "        Wevidence = self._fix_negative_values(self.VRW + self.alphaW - 1)\n",
    "        initialW = normalize(Wevidence, axis=0)\n",
    "        W = self._apply_entropic_prior_and_normalize(\n",
    "            initialW, Wevidence, self.betaW, nu=self.nu, axis=0\n",
    "        )\n",
    "\n",
    "        Hevidence = self._fix_negative_values(self.VRH.T + self.alphaH - 1)\n",
    "        initialH = normalize(Hevidence, axis=1)\n",
    "        H = self._apply_entropic_prior_and_normalize(\n",
    "            initialH, Hevidence, self.betaH, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        return self._prune_undeeded_bases(W, Z, H, curriter)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fix_negative_values(x, fix=EPS):\n",
    "        x[x <= 0] = fix\n",
    "        return x\n",
    "\n",
    "    def _prune_undeeded_bases(self, W, Z, H, curriter):\n",
    "        \"\"\"Discards bases which do not contribute to the decomposition\"\"\"\n",
    "        threshold = 10 * EPS\n",
    "        zidx = np.argwhere(Z > threshold).flatten()\n",
    "        if len(zidx) < self.rank and curriter >= self.minpruneiter:\n",
    "            logger.info(\n",
    "                \"Rank decreased from %d to %d during iteration %d\",\n",
    "                self.rank,\n",
    "                len(zidx),\n",
    "                curriter,\n",
    "            )\n",
    "            self.rank = len(zidx)\n",
    "            Z = Z[zidx]\n",
    "            W = W[:, zidx]\n",
    "            H = H[zidx, :]\n",
    "            self.VRW = self.VRW[:, zidx]\n",
    "            self.VRH = self.VRH[:, zidx]\n",
    "        return W, Z, H\n",
    "\n",
    "    @staticmethod\n",
    "    def _apply_entropic_prior_and_normalize(\n",
    "        param, evidence, beta, nu=50, niter=30, convergence_thresh=1e-7, axis=None\n",
    "    ):\n",
    "        \"\"\"Uses the approximation to the entropic prior from Matt Hoffman.\"\"\"\n",
    "        for i in range(niter):\n",
    "            lastparam = param.copy()\n",
    "            alpha = normalize(param ** (nu / (nu - 1.0)), axis)\n",
    "            param = normalize(evidence + beta * nu * alpha, axis)\n",
    "            # param = normalize(evidence + beta * nu * param**(nu / (nu - 1.0)), 1)\n",
    "            if np.mean(np.abs(param - lastparam)) < convergence_thresh:\n",
    "                logger.log(\n",
    "                    logging.DEBUG - 1,\n",
    "                    \"M-step finished after iteration \" \"%d (beta=%f)\",\n",
    "                    i,\n",
    "                    beta,\n",
    "                )\n",
    "                break\n",
    "        return param\n",
    "    \n",
    "    \n",
    "class SIPLCA(PLCA):\n",
    "    \"\"\"Sparse Shift-Invariant Probabilistic Latent Component Analysis\n",
    "\n",
    "    Decompose V into \\sum_k W_k * z_k h_k^T where * denotes\n",
    "    convolution.  Each basis W_k is a matrix.  Therefore, unlike PLCA,\n",
    "    `W` has shape (`F`, `win`, `rank`). This is the model used in [1].\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    PLCA : Probabilistic Latent Component Analysis\n",
    "    SIPLCA2 : 2D SIPLCA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, V, rank, win=1, circular=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : array, shape (`F`, `T`)\n",
    "            Matrix to analyze.\n",
    "        rank : int\n",
    "            Rank of the decomposition (i.e. number of columns of `W`\n",
    "            and rows of `H`).\n",
    "        win : int\n",
    "            Length of each of the convolutive bases.  Defaults to 1,\n",
    "            i.e. the model is identical to PLCA.\n",
    "        circular : boolean\n",
    "            If True, data shifted past `T` will wrap around to\n",
    "            0. Defaults to False.\n",
    "        alphaW, alphaZ, alphaH : float or appropriately shaped array\n",
    "            Sparsity prior parameters for `W`, `Z`, and `H`.  Negative\n",
    "            values lead to sparser distributions, positive values\n",
    "            makes the distributions more uniform.  Defaults to 0 (no\n",
    "            prior).\n",
    "\n",
    "            **Note** that the prior is not parametrized in the\n",
    "            standard way where the uninformative prior has alpha=1.\n",
    "        \"\"\"\n",
    "        PLCA.__init__(self, V, rank, **kwargs)\n",
    "\n",
    "        self.win = win\n",
    "        self.circular = circular\n",
    "\n",
    "        self.VRW = np.empty((self.F, self.rank, self.win))\n",
    "        self.VRH = np.empty((self.T, self.rank))\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(W, Z, H, norm=1.0, circular=False):\n",
    "        if W.ndim == 2:\n",
    "            W = W[:, np.newaxis, :]\n",
    "        if H.ndim == 1:\n",
    "            H = H[np.newaxis, :]\n",
    "        F, rank, win = W.shape\n",
    "        rank, T = H.shape\n",
    "\n",
    "        WZH = np.zeros((F, T))\n",
    "        for tau in range(win):\n",
    "            WZH += np.dot(W[:, :, tau] * Z, shift(H, tau, 1, circular))\n",
    "        return norm * WZH\n",
    "\n",
    "    def initialize(self):\n",
    "        W, Z, H = super(SIPLCA, self).initialize()\n",
    "        W = np.random.rand(self.F, self.rank, self.win)\n",
    "        W /= W.sum(2).sum(0)[np.newaxis, :, np.newaxis]\n",
    "        return W, Z, H\n",
    "\n",
    "    def do_estep(self, W, Z, H):\n",
    "        WZH = self.reconstruct(W, Z, H, circular=self.circular)\n",
    "        logprob = self.compute_logprob(W, Z, H, WZH)\n",
    "\n",
    "        WZ = W * Z[np.newaxis, :, np.newaxis]\n",
    "        VdivWZH = (self.V / (WZH + EPS))[:, :, np.newaxis]\n",
    "        self.VRW[:] = 0\n",
    "        self.VRH[:] = 0\n",
    "        for tau in range(self.win):\n",
    "            Ht = shift(H, tau, 1, self.circular)\n",
    "            tmp = WZ[:, :, tau][:, np.newaxis, :] * Ht.T[np.newaxis, :, :] * VdivWZH\n",
    "            self.VRW[:, :, tau] += tmp.sum(1)\n",
    "            self.VRH += shift(tmp.sum(0), -tau, 0, self.circular)\n",
    "\n",
    "        return logprob, WZH\n",
    "\n",
    "    def do_mstep(self, curriter):\n",
    "        Zevidence = self._fix_negative_values(self.VRW.sum(2).sum(0) + self.alphaZ - 1)\n",
    "        initialZ = normalize(Zevidence)\n",
    "        Z = self._apply_entropic_prior_and_normalize(\n",
    "            initialZ, Zevidence, self.betaZ, nu=self.nu\n",
    "        )\n",
    "\n",
    "        Wevidence = self._fix_negative_values(self.VRW + self.alphaW - 1)\n",
    "        initialW = normalize(Wevidence, axis=[0, 2])\n",
    "        W = self._apply_entropic_prior_and_normalize(\n",
    "            initialW, Wevidence, self.betaW, nu=self.nu, axis=[0, 2]\n",
    "        )\n",
    "\n",
    "        Hevidence = self._fix_negative_values(self.VRH.T + self.alphaH - 1)\n",
    "        initialH = normalize(Hevidence, axis=1)\n",
    "        H = self._apply_entropic_prior_and_normalize(\n",
    "            initialH, Hevidence, self.betaH, nu=self.nu, axis=1\n",
    "        )\n",
    "\n",
    "        return self._prune_undeeded_bases(W, Z, H, curriter)\n",
    "\n",
    "    \n",
    "def make_key_invariant_chromagram(y, sr):\n",
    "    # Compute the chromagram\n",
    "    chromagram = librosa.feature.chroma_cqt(y=y, sr=sr, bins_per_octave=24)\n",
    "    \n",
    "    # Calculate the chroma values to determine the key\n",
    "    chroma_vals = [np.sum(chromagram[i]) for i in range(12)]\n",
    "    pitches = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    keyfreqs = {pitches[i]: chroma_vals[i] for i in range(12)}\n",
    "    \n",
    "    # Define major and minor profiles for key correlation\n",
    "    maj_profile = [6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88]\n",
    "    min_profile = [6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17]\n",
    "    \n",
    "    # Compute correlations for all keys\n",
    "    maj_key_corrs = [np.corrcoef(maj_profile, [keyfreqs.get(pitches[(i + m) % 12]) for m in range(12)])[1,0] for i in range(12)]\n",
    "    min_key_corrs = [np.corrcoef(min_profile, [keyfreqs.get(pitches[(i + m) % 12]) for m in range(12)])[1,0] for i in range(12)]\n",
    "    keys = [p + ' major' for p in pitches] + [p + ' minor' for p in pitches]\n",
    "    key_corrs = dict(zip(keys, maj_key_corrs + min_key_corrs))\n",
    "    \n",
    "    # Identify the key with the highest correlation\n",
    "    key = max(key_corrs, key=key_corrs.get)\n",
    "    \n",
    "    # Determine tonic index and make chromagram key-invariant\n",
    "    tonic = key.split()[0]\n",
    "    tonic_index = pitches.index(tonic)\n",
    "    key_invariant_chromagram = np.roll(chromagram, -tonic_index, axis=0)\n",
    "    \n",
    "    return key_invariant_chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9062937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W: (12, 60)\n",
      "Shape of Z: ()\n",
      "Shape of H: (11905,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (60,12) and (11905,) not aligned: 12 (dim 1) != 11905 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m niter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m# number of iterations to perform\u001b[39;00m\n\u001b[0;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m123\u001b[39m)  \u001b[38;5;66;03m# Make this reproduceable\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43msegment_audiofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mniter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplotiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n",
      "Cell \u001b[1;32mIn[37], line 7\u001b[0m, in \u001b[0;36msegment_audiofile\u001b[1;34m(audiofile, **kwargs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to compute segmentation of the given audio file\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m features, beattimes, songlen \u001b[38;5;241m=\u001b[39m extract_features(audiofile)\n\u001b[1;32m----> 7\u001b[0m labels, W, Z, H, segfun, norm \u001b[38;5;241m=\u001b[39m \u001b[43msegment_song\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m segments \u001b[38;5;241m=\u001b[39m convert_labels_to_segments(labels, beattimes, songlen)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m segments\n",
      "Cell \u001b[1;32mIn[37], line 90\u001b[0m, in \u001b[0;36msegment_song\u001b[1;34m(seq, rank, win, seed, nrep, minsegments, maxlowen, maxretries, uninformativeWinit, uninformativeHinit, normalize_frames, viterbi_segmenter, align_downbeats, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     W, Z, H, norm, recon, div \u001b[38;5;241m=\u001b[39m SIPLCA\u001b[38;5;241m.\u001b[39manalyze(seq, rank\u001b[38;5;241m=\u001b[39mrank, win\u001b[38;5;241m=\u001b[39mwin, initW\u001b[38;5;241m=\u001b[39malignedW, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m segmentation_function \u001b[38;5;241m=\u001b[39m nmf_analysis_to_segmentation_using_viterbi_path \u001b[38;5;28;01mif\u001b[39;00m viterbi_segmenter \u001b[38;5;28;01melse\u001b[39;00m nmf_analysis_to_segmentation\n\u001b[1;32m---> 90\u001b[0m labels, segfun \u001b[38;5;241m=\u001b[39m \u001b[43msegmentation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels, W, Z, H, segfun, norm\n",
      "Cell \u001b[1;32mIn[37], line 186\u001b[0m, in \u001b[0;36mnmf_analysis_to_segmentation\u001b[1;34m(seq, win, W, Z, H, min_segment_length, use_Z_for_segmentation, **ignored_kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m segfun \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, (w, z, h) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39mtranspose(W, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)), Z, H)):\n\u001b[1;32m--> 186\u001b[0m     reconz \u001b[38;5;241m=\u001b[39m \u001b[43mPLCA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(reconz, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# Smooth it out\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 499\u001b[0m, in \u001b[0;36mPLCA.reconstruct\u001b[1;34m(W, Z, H, norm)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of Z:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Z\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of H:\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m norm \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (60,12) and (11905,) not aligned: 12 (dim 1) != 11905 (dim 0)"
     ]
    }
   ],
   "source": [
    "audio_path = '../data/audio_files/processed/2.mp3'\n",
    "rank = 4  \n",
    "win = 60  # win controls the length of each chroma pattern\n",
    "niter = 200  # number of iterations to perform\n",
    "np.random.seed(123)  # Make this reproduceable\n",
    "\n",
    "labels = segment_audiofile(audio_path, win=win, rank=rank, niter=niter, plotiter=10)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a554e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '../data/audio_files/processed/2.mp3'\n",
    "df = pd.read_csv('../data/dataframes/clean_labeled.csv')\n",
    "data = df.loc[df['SongID'] == 2]\n",
    "sp_tempo = data['sp_tempo'].values[0]\n",
    "\n",
    "# Preprocess the audio\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "y_harm, y_perc = librosa.effects.hpss(y)\n",
    "key_invariant_chromagram = make_key_invariant_chromagram(y=y_harm, sr=sr)\n",
    "seq = np.abs(key_invariant_chromagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293670b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b024bf7-9c0e-4df8-8774-3fb52ad361ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import gzip\n",
    "\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8e4a60d-7c0d-462b-80de-2c3f29414a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_key_invariant(chromagram):\n",
    "    \"\"\"Returns a key-invariant chromagram.\"\"\"\n",
    "    maj_profile = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])\n",
    "    min_profile = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])\n",
    "    avg_chroma = np.mean(chromagram, axis=1)\n",
    "    maj_corrs = np.correlate(avg_chroma, maj_profile, mode='same')\n",
    "    min_corrs = np.correlate(avg_chroma, min_profile, mode='same')\n",
    "    key_shift = np.argmax(np.concatenate((maj_corrs, min_corrs))) % 12\n",
    "    return np.roll(chromagram, -key_shift, axis=0)\n",
    "\n",
    "def segment_by_beats(feature_array, beat_frames):\n",
    "    \"\"\"Segments a 2D array of audio features by beat frames.\"\"\"\n",
    "    beat_frames = np.append(beat_frames, feature_array.shape[1])\n",
    "    return [feature_array[:, beat_frames[i]:beat_frames[i + 1]] for i in range(len(beat_frames) - 1)]\n",
    "\n",
    "def map_labels_to_beats(df_labels, beat_times):\n",
    "    \"\"\"Map chorus labels to beat-synced data.\"\"\"\n",
    "    labels = np.zeros(len(beat_times) - 1)\n",
    "    for _, row in df_labels.iterrows():\n",
    "        indices = np.where((beat_times[:-1] >= row['start_time']) & (beat_times[:-1] < row['end_time']))[0]\n",
    "        labels[indices] = 1 if row['label'] == 'chorus' else 0\n",
    "    return labels\n",
    "\n",
    "def save_compressed_pickle(file_path, data):\n",
    "    \"\"\"Save data to a compressed pickle file.\"\"\"\n",
    "    with gzip.open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b62ff02-f3b5-4e81-8bb5-5a87e02f3906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...: 100%|██████████| 332/332 [1:06:43<00:00, 12.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ensure required directories exist\n",
    "os.makedirs(\"../data/pkl/segments\", exist_ok=True)\n",
    "os.makedirs(\"../data/pkl/labels\", exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "df = pd.read_csv('../data/dataframes/clean_labeled.csv')\n",
    "TARGET_SR = 12000 # Target sample rate chosen to be 1/4 of the original 48kHz.\n",
    "HOP_LENGTH = 128  # Hop length for short-time Fourier transform. Hop length of 128 at 12kHz gives a similar frame rate to a hop length of 512 at 48kHz.\n",
    "\n",
    "# Process each song in the dataset\n",
    "for song_id in tqdm(df['SongID'].unique(), desc=\"Processing...\"):\n",
    "    # Load the audio file\n",
    "    audio_path = f'../data/audio_files/processed/{song_id}.mp3'\n",
    "    y, _ = librosa.load(audio_path, sr=TARGET_SR)\n",
    "    \n",
    "    # Harmonic-percussive source separation\n",
    "    y_harm, y_perc = librosa.effects.hpss(y)\n",
    "    \n",
    "    # Compute onset envelope from the percussive component\n",
    "    onset_env = librosa.onset.onset_strength(y=y_perc, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "\n",
    "    # Beat tracking\n",
    "    _, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "    beat_times = librosa.frames_to_time(beats, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "    \n",
    "    # Map labels to beats\n",
    "    df_labels = df[df['SongID'] == song_id][['start_time', 'end_time', 'label']]\n",
    "    labels = map_labels_to_beats(df_labels, beat_times)\n",
    "    \n",
    "    # Compute RMS energy from spectrogram to give a more accurate representation of energy over time because its frames can be windowed\n",
    "    S = np.abs(librosa.stft(y, hop_length=HOP_LENGTH))\n",
    "    rms = librosa.feature.rms(S=S)\n",
    "    \n",
    "    # Compute Mel Spectrogram and decompose into 4 components (4 chosen from EDA)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=TARGET_SR, n_mels=128, hop_length=HOP_LENGTH)\n",
    "    mel_acts = librosa.decompose.decompose(mel, n_components=4, sort=True)[1]\n",
    "    \n",
    "    # Compute chromagram, make it key invariant, and decompose \n",
    "    chromagram = librosa.feature.chroma_cqt(y=y_harm, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "    chroma_ki = make_key_invariant(chromagram)\n",
    "    chroma_acts = librosa.decompose.decompose(chroma_ki, n_components=3, sort=True)[1]\n",
    "    \n",
    "    # Compute tempogram, ensure non-negative, and decompose \n",
    "    tempogram = np.clip(librosa.feature.tempogram(onset_envelope=onset_env, sr=TARGET_SR, hop_length=HOP_LENGTH), 0, None)\n",
    "    tempogram_acts = librosa.decompose.decompose(tempogram, n_components=3, sort=True)[1]\n",
    "    \n",
    "    # Compute MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=TARGET_SR, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "    \n",
    "    # Standardize features, stack, and segment by beats\n",
    "    features = [rms, mel_acts, chroma_acts, tempogram_acts, mfccs]\n",
    "    total_inv_dim = sum(1.0 / dim for dim in dims.values()) # Calculate the total sum of inverse dimensions to normalize weights\n",
    "    weights = {feature: (1.0 / dims[feature]) / total_inv_dim for feature in dims} # Normalize weights so each feature weighs the same despite dimensionality\n",
    "    # Apply StandardScaler and weights to each feature\n",
    "    standardized_weighted_features = [StandardScaler().fit_transform(feature.T).T * weights[feature_name]\n",
    "                                      for feature, feature_name in zip(features, dims)]\n",
    "    concat_features = np.vstack(standardized_weighted_features)\n",
    "    segments = segment_by_beats(concat_features, beats)\n",
    "\n",
    "    # Save results with compression\n",
    "    save_compressed_pickle(f\"../data/pkl/segments/{song_id}_beats.pkl.gz\", segments)\n",
    "    save_compressed_pickle(f\"../data/pkl/labels/{song_id}_labels.pkl.gz\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35801f49-b806-48f5-a930-82a28e0046e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Start here after processing data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "12048a72-9d59-4888-930d-dc48bc31cee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f6423455-eaf5-4d31-9672-9405d278ab70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with gzip.open(os.path.join(save_path, 'beat_data.pkl.gz'), 'rb') as f:\n",
    "    X_dict, y_dict = pickle.load(f)\n",
    "\n",
    "song_data = list(X_dict.values())\n",
    "song_labels = list(y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd3bb8-9567-4e5d-8e94-280064c73373",
   "metadata": {},
   "source": [
    "## Data Structure Overview\n",
    "\n",
    "- `song_data`: A list where each element corresponds to an individual song's feature data.\n",
    "- `song_data[0]`: The feature data for the first song in the dataset.\n",
    "- `song_data[i]`: The feature data for the i-th song in the dataset.\n",
    "- `song_data[i][j]`: The j-th beat segment's feature data within the i-th song.\n",
    "- Each `song_data[i][j]` is structured as a 2D array:\n",
    "  - The first dimension has a fixed size of 24, representing the number of features.\n",
    "  - The second dimension has a variable size y, representing the number of frames in the beat segment.\n",
    "- The feature count is consistent across the dataset, with each beat segment containing 24 features.\n",
    "- `song_labels`: A list containing the corresponding labels for each song's beat segments.\n",
    "- `song_labels[i]`: An array of labels for each beat segment within the i-th song.\n",
    "\n",
    "---\n",
    "\n",
    "## Padding Process for Beat Segments and Labels\n",
    "\n",
    "### Step 1: Find max beat and max frames in a beat\n",
    "- Find maximum number of beats in any song (`max_beats`).\n",
    "- Find maximum number of frames in any beat segment across all songs (`max_frames`).\n",
    "\n",
    "### Step 2: Pad Data using -1\n",
    "- Pad songs to have max beats, pad beats to have max frames, ensuring that newly added beats have max frames.\n",
    "- Pad using -1\n",
    "\n",
    "### Step 3: Pad Labels using -1\n",
    "- Pad the array of labels for each song to match `max_beats`. Use `-1` as the padding value to denote labels for dummy beats.\n",
    "\n",
    "### Implementation Notes\n",
    "\n",
    "- This process assumes that your dataset's current structure allows iteration over songs and their corresponding beat segments and labels.\n",
    "- Padding with `-1` is a common approach when the padded values need to be easily distinguished from valid data. Ensure that your model or subsequent processing steps can handle this special value appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "32593b6f-998a-4796-8f04-6745946006e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 398. MiB for an array with shape (803, 24, 2705) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[269], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     empty_beats \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((max_beats \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(padded_beats), \u001b[38;5;241m24\u001b[39m, max_frames), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Combine the existing padded beats with the empty beats\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     padded_song \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_beats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempty_beats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     padded_song \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(padded_beats)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 398. MiB for an array with shape (803, 24, 2705) and data type float64"
     ]
    }
   ],
   "source": [
    "# Find max beat and frames across entire dataset\n",
    "max_beats = 0 \n",
    "max_frames = 0  \n",
    "for song in song_data:\n",
    "    max_beats = max(max_beats, len(song))\n",
    "    for beat_segment in song:\n",
    "        max_frames = max(max_frames, len(beat_segment[0])) \n",
    "\n",
    "# Initialize a new list for the padded song data\n",
    "padded_song_data = []\n",
    "\n",
    "# Iterate over each song in song_data\n",
    "for song in song_data:\n",
    "    # Pad each beat segment within the song to have max_frames\n",
    "    # Using -1 instead of 0\n",
    "    padded_beats = [np.pad(beat, ((0, 0), (0, max_frames - beat.shape[1])), 'constant', constant_values=-1) for beat in song]\n",
    "    \n",
    "    # If the song has fewer beats than max_beats, we create empty beats with -1\n",
    "    if len(padded_beats) < max_beats:\n",
    "        # Empty beats are 2D arrays filled with -1 with dimensions (24, max_frames)\n",
    "        empty_beats = np.full((max_beats - len(padded_beats), 24, max_frames), -1)\n",
    "        # Combine the existing padded beats with the empty beats\n",
    "        padded_song = np.vstack((padded_beats, empty_beats))\n",
    "    else:\n",
    "        padded_song = np.array(padded_beats)\n",
    "    \n",
    "    # Add the padded song to the new list\n",
    "    padded_song_data.append(padded_song)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ddaf7493-675e-4dc0-81e7-fe5cc2e519b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data back:\n",
    "with gzip.open(os.path.join(save_path, 'beat_data.pkl.gz'), 'rb') as f:\n",
    "    X_dict, y_dict = pickle.load(f)\n",
    "\n",
    "song_data = list(X_dict.values())\n",
    "song_labels = list(y_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "599e5589-1779-4417-8def-83522c16b772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (543, 24) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[249], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     X_dict[song_id] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mpad(beat, (\u001b[38;5;241m0\u001b[39m, max_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(beat)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, constant_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m beat \u001b[38;5;129;01min\u001b[39;00m beats]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Pad the list of beat segments to have max_beats\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     X_dict[song_id] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43msong_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_beats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43msong_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 3: Pad Labels\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m song_id, labels \u001b[38;5;129;01min\u001b[39;00m y_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Pad the array of labels for each song to match max_beats\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\numpy\\lib\\arraypad.py:737\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_pad_dispatcher, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad\u001b[39m(array, pad_width, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    531\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m    Pad an array.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m           [100, 100, 100, 100, 100, 100, 100]])\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 737\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m     pad_width \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pad_width)\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pad_width\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (543, 24) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Step 1: Determine Maximum Number of Beats and Frames\n",
    "max_beats = max(len(beats) for beats in X_dict.values())\n",
    "max_frames = max(len(beat) for beats in X_dict.values() for beat in beats)\n",
    "\n",
    "# Step 2: Pad Beat Segments (Data)\n",
    "for song_id, beats in X_dict.items():\n",
    "    # Pad each beat segment to have max_frames\n",
    "    X_dict[song_id] = [np.pad(beat, (0, max_frames - len(beat)), 'constant', constant_values=-1) for beat in beats]\n",
    "    # Pad the list of beat segments to have max_beats\n",
    "    X_dict[song_id] = np.pad(X_dict[song_id], (0, max_beats - len(X_dict[song_id])), 'constant', constant_values=(-1, -1))\n",
    "\n",
    "# Step 3: Pad Labels\n",
    "for song_id, labels in y_dict.items():\n",
    "    # Pad the array of labels for each song to match max_beats\n",
    "    y_dict[song_id] = np.pad(labels, (0, max_beats - len(labels)), 'constant', constant_values=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2a4d70ce-51b7-42fb-ae4f-afb5be3ef67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (543, 24) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[243], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m         padded_beat_segment \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(beat_segment, ((\u001b[38;5;241m0\u001b[39m, max_frames \u001b[38;5;241m-\u001b[39m beat_segment\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, constant_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m         padded_beat_segments\u001b[38;5;241m.\u001b[39mappend(padded_beat_segment)\n\u001b[1;32m---> 15\u001b[0m     padded_beat_segments \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_beat_segments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_beats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeat_segments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     X_dict[song_id] \u001b[38;5;241m=\u001b[39m padded_beat_segments\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Pad labels\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\numpy\\lib\\arraypad.py:737\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_pad_dispatcher, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad\u001b[39m(array, pad_width, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    531\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m    Pad an array.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m           [100, 100, 100, 100, 100, 100, 100]])\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 737\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m     pad_width \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pad_width)\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pad_width\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (543, 24) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Determine maximum number of beats and frames\n",
    "max_beats = 0\n",
    "max_frames = 0\n",
    "for song_id, beat_segments in X_dict.items():\n",
    "    max_beats = max(max_beats, len(beat_segments))\n",
    "    for beat_segment in beat_segments:\n",
    "        max_frames = max(max_frames, beat_segment.shape[0])\n",
    "\n",
    "# Pad beat segments\n",
    "for song_id, beat_segments in X_dict.items():\n",
    "    padded_beat_segments = []\n",
    "    for beat_segment in beat_segments:\n",
    "        padded_beat_segment = np.pad(beat_segment, ((0, max_frames - beat_segment.shape[0]), (0, 0)), mode='constant', constant_values=-1)\n",
    "        padded_beat_segments.append(padded_beat_segment)\n",
    "    padded_beat_segments = np.pad(padded_beat_segments, ((0, max_beats - len(beat_segments)), (0, 0), (0, 0)), mode='constant', constant_values=-1)\n",
    "    X_dict[song_id] = padded_beat_segments\n",
    "\n",
    "# Pad labels\n",
    "for song_id, labels in y_dict.items():\n",
    "    padded_labels = np.pad(labels, (0, max_beats - len(labels)), mode='constant', constant_values=-1)\n",
    "    y_dict[song_id] = padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d402d436-0fe9-4796-90cd-f40b0743b59a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(beat_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0d98e9d1-d1f9-4c53-ad4b-adb86dbbb398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 232, list, 386, numpy.ndarray, (24, 41))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(X_train), len(X_train), type(X_train[0]), len(X_train[0]), type(X_train[0][0]),(X_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "769b06b2-3b24-4496-a80d-a7c7ebc9a11c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 232, numpy.ndarray, 385, numpy.float64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train), len(y_train), type(y_train[0]), len(y_train[0]), type(y_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "70c15d9e-5f3b-498a-b811-ba1a300918e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert dictionaries to lists for further processing\n",
    "X = list(beat_segments.values())\n",
    "y = list(beat_labels.values())\n",
    "\n",
    "# Split the data in 70/15/15 train/val/test splits\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7dd0a-c1b9-495d-b73b-f6fc4c5d3adf",
   "metadata": {},
   "source": [
    "X_train is a list of songs. \n",
    "Each song in X_train is a list of beats. \n",
    "Each beat is a 2d array of (24 features, num_frames)\n",
    "\n",
    "y_train is a list of song labels\n",
    "each song in y_train is an array of beat labels (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ef7ffa17-d2dd-4206-bab8-48c6f0b2eed4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (386, 24) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate over songs in X_train\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m song, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Flatten the song to a 1D array of beats\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     flat_song \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43msong\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Calculate the number of batches for the song\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(flat_song) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (386, 24) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Initialize empty lists for batched data\n",
    "batched_X_train = []\n",
    "batched_y_train = []\n",
    "\n",
    "# Iterate over songs in X_train\n",
    "for song, label in zip(X_train, y_train):\n",
    "    # Flatten the song to a 1D array of beats\n",
    "    flat_song = np.array(song).flatten()\n",
    "    \n",
    "    # Calculate the number of batches for the song\n",
    "    num_batches = len(flat_song) // batch_size\n",
    "    \n",
    "    # Split the flattened song into batches\n",
    "    batches = np.array_split(flat_song, num_batches)\n",
    "    \n",
    "    # Reshape the batches to have the same shape as the original beats\n",
    "    reshaped_batches = [batch.reshape((24, -1)) for batch in batches]\n",
    "    \n",
    "    # Append the batches to the batched_X_train list\n",
    "    batched_X_train.extend(reshaped_batches)\n",
    "    \n",
    "    # Repeat the song label for each batch\n",
    "    batched_y_train.extend([label] * num_batches)\n",
    "\n",
    "# Convert the batched data to numpy arrays\n",
    "batched_X_train = np.array(batched_X_train)\n",
    "batched_y_train = np.array(batched_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d89c40cd-910a-4d19-a4e1-cd33e37dec6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to numpy arrays for batching\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Calculate the number of batches for train, validation, and test sets\n",
    "num_train_batches = len(X_train) // batch_size\n",
    "num_val_batches = len(X_val) // batch_size\n",
    "num_test_batches = len(X_test) // batch_size\n",
    "\n",
    "# Split train, validation, and test sets into batches\n",
    "train_batches = np.array_split(X_train, num_train_batches)\n",
    "train_labels_batches = np.array_split(y_train, num_train_batches)\n",
    "val_batches = np.array_split(X_val, num_val_batches)\n",
    "val_labels_batches = np.array_split(y_val, num_val_batches)\n",
    "test_batches = np.array_split(X_test, num_test_batches)\n",
    "test_labels_batches = np.array_split(y_test, num_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "83a2e0d1-393e-4324-9bba-ab5590083c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train), batch_size):\n",
    "    X_train_batches = X_train[i:i + batch_size]\n",
    "    label_batch = labels[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf565fae-a72d-4180-a5fb-2c493c1862c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In progress.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "40130424-6aa4-4a0c-9a54-7e7538c6bcee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_batches\n\u001b[0;32m     85\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m---> 86\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_batches_pad_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m create_batches_pad_encode(X_val, y_val, batch_size, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m create_batches_pad_encode(X_test, y_test, batch_size, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[161], line 78\u001b[0m, in \u001b[0;36mcreate_batches_pad_encode\u001b[1;34m(data, labels, batch_size, padding_value)\u001b[0m\n\u001b[0;32m     76\u001b[0m label_batch \u001b[38;5;241m=\u001b[39m labels[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Pad sequences and labels\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m padded_data \u001b[38;5;241m=\u001b[39m \u001b[43mpad_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m padded_labels \u001b[38;5;241m=\u001b[39m pad_labels(label_batch, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mpadding_value)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Positionally encode sequences\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[161], line 35\u001b[0m, in \u001b[0;36mpad_data\u001b[1;34m(data_batches, padding, value)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pad beats within songs in data_batches to have the same number of frames,\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mand ensure all songs have beats padded to the same length.\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# First, find the maximum number of frames across all beats in the batch\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m max_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([beat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_batches \u001b[38;5;28;01mfor\u001b[39;00m song \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;28;01mfor\u001b[39;00m beat \u001b[38;5;129;01min\u001b[39;00m song])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Next, find the maximum number of beats in a song across the entire batch\u001b[39;00m\n\u001b[0;32m     38\u001b[0m max_beats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(song) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_batches \u001b[38;5;28;01mfor\u001b[39;00m song \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "Cell \u001b[1;32mIn[161], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pad beats within songs in data_batches to have the same number of frames,\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mand ensure all songs have beats padded to the same length.\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# First, find the maximum number of frames across all beats in the batch\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m max_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[43mbeat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_batches \u001b[38;5;28;01mfor\u001b[39;00m song \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;28;01mfor\u001b[39;00m beat \u001b[38;5;129;01min\u001b[39;00m song])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Next, find the maximum number of beats in a song across the entire batch\u001b[39;00m\n\u001b[0;32m     38\u001b[0m max_beats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(song) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_batches \u001b[38;5;28;01mfor\u001b[39;00m song \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "def get_positional_encoding(position, d_model):\n",
    "    \"\"\"Compute the sinusoidal encoding for a given position and model size.\"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    angle_rads = position[:, np.newaxis] * angle_rates\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def add_positional_encoding(padded_sequences):\n",
    "    \"\"\"Add the positional encoding to each sequence.\"\"\"\n",
    "    encoded_sequences = []\n",
    "    for sequence in padded_sequences:\n",
    "        encoded_seq = []\n",
    "        for sub_seq in sequence:\n",
    "            # Get the sequence length and feature dimension\n",
    "            sequence_length = sub_seq.shape[0]\n",
    "            feature_dim = sub_seq.shape[1]\n",
    "\n",
    "            # Compute and add the positional encoding\n",
    "            pos_encoding = get_positional_encoding(np.arange(sequence_length), feature_dim)\n",
    "            encoded_sub_seq = sub_seq + pos_encoding\n",
    "\n",
    "            encoded_seq.append(encoded_sub_seq)\n",
    "        encoded_sequences.append(encoded_seq)\n",
    "\n",
    "    return encoded_sequences\n",
    "\n",
    "def pad_data(data_batches, padding='post', value=-1):\n",
    "    \"\"\"Pad beats within songs in data_batches to have the same number of frames,\n",
    "    and ensure all songs have beats padded to the same length.\"\"\"\n",
    "    \n",
    "    # First, find the maximum number of frames across all beats in the batch\n",
    "    max_frames = max([beat.shape[1] for batch in data_batches for song in batch for beat in song])\n",
    "    \n",
    "    # Next, find the maximum number of beats in a song across the entire batch\n",
    "    max_beats = max([len(song) for batch in data_batches for song in batch])\n",
    "    \n",
    "    padded_batches = []\n",
    "    for batch in data_batches:\n",
    "        padded_batch = []\n",
    "        for song in batch:\n",
    "            padded_song = []\n",
    "            for beat in song:\n",
    "                # Pad each beat to have the same number of frames\n",
    "                padded_beat = np.pad(beat, ((0, 0), (0, max_frames - beat.shape[1]), (0, 0)), mode='constant', constant_values=value)\n",
    "                padded_song.append(padded_beat)\n",
    "            \n",
    "            # Now pad the song to have the same number of beats\n",
    "            while len(padded_song) < max_beats:\n",
    "                # Create a dummy beat with the correct number of frames and features, but no actual data\n",
    "                dummy_beat = np.full((1, max_frames, beat.shape[2]), value)\n",
    "                padded_song.append(dummy_beat)\n",
    "            \n",
    "            padded_batch.append(np.array(padded_song))\n",
    "        padded_batches.append(padded_batch)\n",
    "    \n",
    "    return padded_batches\n",
    "\n",
    "def pad_labels(labels, padding='post', value=-1):\n",
    "    \"\"\"Pad label sequences to the same length.\"\"\"\n",
    "    max_length = max([len(batch) for batch in labels])  # Assuming labels are simpler and directly reflect the batch size\n",
    "    padded = []\n",
    "    for batch in labels:\n",
    "        padded_batch = np.full((max_length,), value, dtype=np.int32)  # Create a padded batch with the default value\n",
    "        padded_batch[:len(batch)] = batch  # Fill in the actual labels\n",
    "        padded.append(padded_batch)\n",
    "    return padded\n",
    "\n",
    "def create_batches_pad_encode(data, labels, batch_size, padding_value=-1):\n",
    "    \"\"\"Create and pad batches of sequences and labels.\"\"\"\n",
    "    padded_batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        data_batch = data[i:i + batch_size]\n",
    "        label_batch = labels[i:i + batch_size]\n",
    "        # Pad sequences and labels\n",
    "        padded_data = pad_data(data_batch, padding='post', value=padding_value)\n",
    "        padded_labels = pad_labels(label_batch, padding='post', value=padding_value)\n",
    "        # Positionally encode sequences\n",
    "        padded_data = add_positional_encoding(padded_data)\n",
    "        padded_batches.append((padded_data, padded_labels))\n",
    "    return padded_batches\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = create_batches_pad_encode(X_train, y_train, batch_size, padding_value=-1)\n",
    "val_dataset = create_batches_pad_encode(X_val, y_val, batch_size, padding_value=-1)\n",
    "test_dataset = create_batches_pad_encode(X_test, y_test, batch_size, padding_value=-1)\n",
    "\n",
    "# [batch_size, num_sequences, max_sequence_length, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "585fa575-df73-4185-bb68-d4409282f339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Type=<class 'list'>, Length=232\n",
      "Dataset[0]: Type=<class 'list'>, Length=386\n",
      "Dataset[0][0]: Type=<class 'numpy.ndarray'>, Length=24\n",
      "Dataset[0][0][0]: Type=<class 'numpy.ndarray'>, Length=41\n",
      "Dataset[0][0][0][0]: Type=<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset_structure(dataset):\n",
    "    # Define a generic function to print the properties of an object\n",
    "    def print_properties(obj, name):\n",
    "        print(f\"{name}: Type={type(obj)}\", end=\"\")\n",
    "        if hasattr(obj, '__len__'):\n",
    "            print(f\", Length={len(obj)}\", end=\"\")\n",
    "        if isinstance(obj, tf.Tensor):\n",
    "            print(f\", Shape={obj.shape}\", end=\"\")\n",
    "        print()  # Newline\n",
    "    \n",
    "    print_properties(dataset, \"Dataset\")\n",
    "    print_properties(dataset[0], \"Dataset[0]\")\n",
    "    print_properties(dataset[0][0], \"Dataset[0][0]\")\n",
    "    print_properties(dataset[0][0][0], \"Dataset[0][0][0]\")\n",
    "    print_properties(dataset[0][0][0][0], \"Dataset[0][0][0][0]\")\n",
    "inspect_dataset_structure(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc8f44-4e08-4d12-b531-c84cee19a977",
   "metadata": {},
   "source": [
    "datasets are a list of tuples, where each tuple contains a batch of padded and positionally encoded sequences (songs) (`padded_sequences`) and their corresponding padded labels (`padded_labels`):\n",
    "\n",
    "Each sequence batch contains 16 song, each song is has the following shape and structure:\n",
    "\n",
    "Shape: [batch_size, num_beats, max_beat_length, feature_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c6b87b2c-578b-45ad-b4b4-329ca4ad80a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Type=<class 'list'>, Length=232\n",
      "Dataset[0] (data, labels): Type=<class 'list'>, Length=386\n",
      "Dataset[0][0]: Type=<class 'numpy.ndarray'>, Length=24\n",
      "Dataset[0][0][0]: Type=<class 'numpy.ndarray'>, Length=41\n",
      "Dataset[0][0][0][0]: Type=<class 'numpy.float64'>\n",
      "Dataset[0][1] (labels): Type=<class 'numpy.ndarray'>, Length=24\n",
      "Dataset[0][1][0]: Type=<class 'numpy.ndarray'>, Length=45\n",
      "Beat 1: Type=<class 'numpy.float64'>\n",
      "Song label length 45: Type=<class 'int'>\n",
      "Beat 2: Type=<class 'numpy.float64'>\n",
      "Song label length 45: Type=<class 'int'>\n",
      "Beat 3: Type=<class 'numpy.float64'>\n",
      "Song label length 45: Type=<class 'int'>\n",
      "Beat 4: Type=<class 'numpy.float64'>\n",
      "Song label length 45: Type=<class 'int'>\n",
      "Beat 5: Type=<class 'numpy.float64'>\n",
      "Song label length 45: Type=<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset_structure(dataset):\n",
    "    # Define a generic function to print the properties of an object\n",
    "    def print_properties(obj, name):\n",
    "        print(f\"{name}: Type={type(obj)}\", end=\"\")\n",
    "        if hasattr(obj, '__len__'):\n",
    "            print(f\", Length={len(obj)}\", end=\"\")\n",
    "        if isinstance(obj, tf.Tensor):\n",
    "            print(f\", Shape={obj.shape}\", end=\"\")\n",
    "        print()  # Newline\n",
    "    \n",
    "    print_properties(dataset, \"Dataset\")\n",
    "    print_properties(dataset[0], \"Dataset[0]\")\n",
    "    print_properties(dataset[0][0], \"Dataset[0][0]\")\n",
    "    print_properties(dataset[0][0][0], \"Dataset[0][0][0]\")\n",
    "    print_properties(dataset[0][0][0][0], \"Dataset[0][0][0][0]\")\n",
    "\n",
    "    # Now, let's inspect how the tensor shape changes for the first 5 beats\n",
    "    for i in range(5):  # Assuming there are at least 5 beats in the first song\n",
    "        beat = dataset[0][0][0][i]\n",
    "        song_label_len = len(dataset[0][1][i])\n",
    "        print_properties(beat, f\"Beat {i+1}\")\n",
    "inspect_dataset_structure(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35860c21-cbe2-401b-849d-580bcc22284b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"Custom binary cross-entropy loss to handle -1 labels, which are used for padding and should be ignored during loss calculation.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "    loss = bce * mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    \"\"\"Custom accuracy metric to handle -1 labels, which are used for padding and should be ignored during accuracy calculation.\"\"\"\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "    correct_predictions = tf.equal(tf.cast(tf.round(y_pred), tf.float32), y_true)\n",
    "    masked_correct_predictions = tf.cast(correct_predictions, tf.float32) * mask\n",
    "    accuracy = tf.reduce_sum(masked_correct_predictions) / tf.reduce_sum(mask)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def compile_model(max_frames, max_freq_bins, n_features, custom_binary_crossentropy, custom_accuracy, max_beats):\n",
    "    \"\"\"\n",
    "    Define and compile a CNN model with considerations for environmental sound classification.\n",
    "    \"\"\"\n",
    "    frame_input = layers.Input(shape=(max_frames, max_freq_bins, n_features))\n",
    "    \n",
    "    # Assuming Mx3 filter means spanning the entire frequency bins with a width of 3.\n",
    "    conv1 = layers.Conv2D(filters=180, kernel_size=(max_freq_bins, 3), activation='relu', padding='same')(frame_input)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(4, 4), padding='same')(conv1)\n",
    "    dropout1 = layers.Dropout(0.5)(pool1)  # Applying 50% dropout after pooling\n",
    "    \n",
    "    conv2 = layers.Conv2D(filters=180, kernel_size=(1, 3), activation='relu', padding='same')(dropout1) # Changed to 1x3 to not reduce the frequency dimension further\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(4, 4), padding='same')(conv2)\n",
    "    dropout2 = layers.Dropout(0.5)(pool2)  # Applying 50% dropout after pooling\n",
    "    \n",
    "    conv3 = layers.Conv2D(filters=180, kernel_size=(1, 3), activation='relu', padding='same')(dropout2) # Changed to 1x3 for consistency\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(4, 4), padding='same')(conv3)\n",
    "    dropout3 = layers.Dropout(0.5)(pool3)  # Applying 50% dropout after pooling\n",
    "    \n",
    "    frame_features = layers.Flatten()(dropout3)\n",
    "    frame_feature_model = models.Model(inputs=frame_input, outputs=frame_features)\n",
    "\n",
    "    measure_input = layers.Input(shape=(max_beats, max_frames, max_freq_bins, n_features))\n",
    "    time_distributed = layers.TimeDistributed(frame_feature_model)(measure_input)\n",
    "    masking_layer = layers.Masking(mask_value=-1)(time_distributed)\n",
    "    lstm_out = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(masking_layer)\n",
    "    output = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(lstm_out)\n",
    "    model = models.Model(inputs=measure_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss=custom_binary_crossentropy, metrics=[custom_accuracy])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596ea8c-e986-4d12-9f10-006064e55385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation with stratified splitting, batching, and positional encoding\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"Generates sinusoidal positional encodings.\"\"\"\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model // 2)[np.newaxis, :]\n",
    "    angles = pos / np.power(10000, (2 * i) / d_model)\n",
    "    sin_enc = np.sin(angles)\n",
    "    cos_enc = np.cos(angles)\n",
    "    encoding = np.concatenate([sin_enc, cos_enc[:, :d_model // 2]], axis=-1)\n",
    "    return encoding\n",
    "\n",
    "def apply_positional_encoding(features, pos_encoding):\n",
    "    \"\"\"Applies positional encoding to the input features.\"\"\"\n",
    "    return features + pos_encoding[:features.shape[1], :]\n",
    "\n",
    "\n",
    "def run_cross_validation(n_splits=5, batch_size=32, feat_dim=128):\n",
    "    \"\"\"Runs cross-validation with stratified splitting, batching, and positional encoding.\"\"\"\n",
    "    song_ids = df['SongID'].unique()\n",
    "    all_labels = np.concatenate([padded_beat_labels[id] for id in song_ids])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(song_ids, all_labels), start=1):\n",
    "        print(f\"Fold {fold}/{n_splits}\")\n",
    "\n",
    "        train_ids, val_ids = song_ids[train_idx], song_ids[val_idx]\n",
    "        train_batches = [(get_batch_segments(ids, padded_beat_segments, padded_beat_labels, feat_dim), \n",
    "                          get_batch_labels(ids, padded_beat_labels)) for ids in create_batches(train_ids, batch_size)]\n",
    "        val_batches = [(get_batch_segments(ids, padded_beat_segments, padded_beat_labels, feat_dim), \n",
    "                        get_batch_labels(ids, padded_beat_labels)) for ids in create_batches(val_ids, batch_size)]\n",
    "\n",
    "        # Train and evaluate model\n",
    "        model = train_model(train_batches)\n",
    "        val_score = evaluate_model(model, val_batches)\n",
    "        fold_scores.append(val_score)\n",
    "\n",
    "    avg_score = np.mean(fold_scores)\n",
    "    print(f\"Average Cross-Validation Score: {avg_score}\")\n",
    "\n",
    "def train_model(train_batches):\n",
    "    # Define the directories for checkpoints and models\n",
    "    checkpoint_dir = os.path.join('..', 'checkpoints', 'CRNN')\n",
    "    model_dir = os.path.join('..', 'models', 'CRNN')\n",
    "\n",
    "    # Ensure the checkpoint and model directories exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Define the checkpoint path for the best model\n",
    "    best_model_filepath = os.path.join(model_dir, 'best_model.h5')\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=best_model_filepath,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_custom_accuracy',  # Use 'val_custom_accuracy' for validation custom accuracy\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',  \n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model with the simplified callbacks list\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "def evaluate_model(model, val_batches):\n",
    "    # Implement model evaluation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21052a43-7191-4e43-aba1-7f65f54ec21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db4149-9754-4c22-8716-9e1d44c6f349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f39cba-b94e-4b70-af11-5f6818ed106b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

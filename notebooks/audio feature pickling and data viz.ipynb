{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d23d75-8e2a-4aaa-bcba-192bcd97e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_beats(beats, onset_env, tempo, sr, hop_length, duration):\n",
    "    \"\"\"\n",
    "    Adjusts beat times to the nearest detected onsets and creates beat and measure grids for the audio.\n",
    "\n",
    "    This function assumes the beat times are evenly spaced within each measure and are in 4/4 time signature. It also backtracks the beats using the onset envelope to align them to the nearest detected onset.\n",
    "\n",
    "    Parameters:\n",
    "    beats : np.ndarray\n",
    "        An array of beat times in frame units generated from librosa.beat.beat_track.\n",
    "    onset_env : np.ndarray\n",
    "        Onset envelope of the audio signal, used for backtracking beats.\n",
    "    tempo : float\n",
    "        Estimated tempo of the audio in beats per minute.\n",
    "    sr : int\n",
    "        Sampling rate of the audio signal.\n",
    "    hop_length : int\n",
    "        Hop length used in the onset detection and beat tracking.\n",
    "    duration : float\n",
    "        Duration of the audio signal in seconds.\n",
    "\n",
    "    Returns:\n",
    "    beat_grid : np.ndarray\n",
    "        Array of quantized beat times.\n",
    "    measure_grid : np.ndarray\n",
    "        Array of quantized measure start times.\n",
    "    beats_per_measure : int\n",
    "        Number of beats per measure, which is set to 4 for a 4/4 time signature.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the `beats` array is empty or not one-dimensional.\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if beats.ndim != 1:\n",
    "        raise ValueError(\"The 'beats' array must be one-dimensional.\")\n",
    "    if beats.size == 0:\n",
    "        raise ValueError(\"The 'beats' array must not be empty.\")\n",
    "    \n",
    "    # Hardcoded assumption of 4/4 time signature\n",
    "    beats_per_measure = 4\n",
    "\n",
    "    # Track beats to align them to the nearest detected onset\n",
    "    beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Calculate the beat interval (seconds per beat)\n",
    "    beat_interval = 60.0 / tempo\n",
    "\n",
    "    # Backtrack from the first beat to align with time 0 if necessary\n",
    "    first_beat_time = 0\n",
    "\n",
    "    # Create beat grid from the first beat time to the end of the song\n",
    "    beat_grid = np.arange(first_beat_time, duration, beat_interval)\n",
    "\n",
    "    # Ensure beat grid does not go past the duration of the song\n",
    "    beat_grid = beat_grid[beat_grid <= duration]\n",
    "\n",
    "    # Create measure grid\n",
    "    measure_indices = np.arange(0, len(beat_grid), beats_per_measure)\n",
    "    measure_grid = beat_grid[measure_indices]\n",
    "\n",
    "    # Ensure measure grid does not go past the duration of the song\n",
    "    measure_grid = measure_grid[measure_grid <= duration]\n",
    "\n",
    "    return beat_grid, measure_grid\n",
    "\n",
    "\n",
    "def apply_measure_grid(ax, measure_grid):\n",
    "    \"\"\"\n",
    "    This function takes an axis object and applies measure grid lines,\n",
    "    sets x-ticks to measure start times for every fourth measure starting from measure 0,\n",
    "    labels them with measure numbers, and applies sub-ticks for intermediate measure times.\n",
    "    \n",
    "    Parameters:\n",
    "    ax (matplotlib.axes.Axes): The axis object to modify.\n",
    "    measure_grid (list or array): The list or array of measure start times in seconds.\n",
    "    \"\"\"\n",
    "    measure_numbers = np.arange(len(measure_grid))\n",
    "    # Adjust the list to start from measure 0 and get every fourth measure\n",
    "    major_measure_indices = [i for i, measure_num in enumerate(measure_numbers) if (measure_num) % 4 == 0]\n",
    "    major_measures = [measure_grid[i] for i in major_measure_indices]\n",
    "    major_labels = [measure_numbers[i] for i in major_measure_indices]\n",
    "    \n",
    "    # Set major x-axis ticks and labels (for measure 0 and every fourth measure after)\n",
    "    ax.set_xticks(major_measures, minor=False)\n",
    "    ax.set_xticklabels(major_labels, minor=False)\n",
    "\n",
    "    # Set minor x-axis ticks (for intermediate measures)\n",
    "    minor_measures = [measure for i, measure in enumerate(measure_grid) if i not in major_measure_indices]\n",
    "    ax.set_xticks(minor_measures, minor=True)\n",
    "    \n",
    "    # Overlay the major measure grid lines on the plot (for measure 0 and every fourth measure after)\n",
    "    for measure_time in major_measures:\n",
    "        ax.axvline(x=measure_time, color='green', linestyle='--', linewidth=2)  # Adjusted linewidth for major ticks\n",
    "    \n",
    "    # Overlay the minor measure grid lines on the plot (for intermediate measures)\n",
    "    for measure_time in minor_measures:\n",
    "        ax.axvline(x=measure_time, color='grey', linestyle=':', linewidth=1, alpha=0.8)  # Adjusted linewidth for minor ticks\n",
    "    \n",
    "    ax.set_xlabel('Measure Number')\n",
    "\n",
    "\n",
    "# Function to detect key from a chromagram using Krumhansl-Schmuckler key-finding algorithm profiles\n",
    "def detect_key_from_chromagram(chromagram, sr):\n",
    "    pitches = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "    # Calculate the sum of each pitch class across all time frames\n",
    "    chroma_vals = np.sum(chromagram, axis=1)\n",
    "\n",
    "    # Krumhansl-Schmuckler key-finding algorithm profiles\n",
    "    maj_profile = [6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88]\n",
    "    min_profile = [6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17]\n",
    "\n",
    "    # Correlation for major and minor keys\n",
    "    maj_key_corrs = [np.corrcoef(maj_profile, np.roll(chroma_vals, i))[1, 0] for i in range(12)]\n",
    "    min_key_corrs = [np.corrcoef(min_profile, np.roll(chroma_vals, i))[1, 0] for i in range(12)]\n",
    "\n",
    "    # Combine correlations and keys\n",
    "    key_corrs = maj_key_corrs + min_key_corrs\n",
    "    keys = [p + ' major' for p in pitches] + [p + ' minor' for p in pitches]\n",
    "\n",
    "    # Determine the best key\n",
    "    best_idx = np.argmax(key_corrs)\n",
    "    best_key = keys[best_idx]\n",
    "    best_corr = key_corrs[best_idx]\n",
    "\n",
    "    return best_key, best_corr\n",
    "\n",
    "\n",
    "# Function to convert standard key into Camelot key notation\n",
    "def get_camelot(key):\n",
    "    # Mapping from musical key to Camelot code\n",
    "    camelot_major = {\n",
    "        'B': '1B', 'F#': '2B', 'C#': '3B', 'G#': '4B', 'D#': '5B',\n",
    "        'A#': '6B', 'F': '7B', 'C': '8B', 'G': '9B', 'D': '10B', 'A': '11B', 'E': '12B'\n",
    "    }\n",
    "\n",
    "    camelot_minor = {\n",
    "        'G#': '1A', 'D#': '2A', 'A#': '3A', 'F': '4A', 'C': '5A',\n",
    "        'G': '6A', 'D': '7A', 'A': '8A', 'E': '9A', 'B': '10A', 'F#': '11A', 'C#': '12A'\n",
    "    }\n",
    "\n",
    "    # Split the detected key into pitch and mode\n",
    "    pitch, mode = key.split(' ')\n",
    "\n",
    "    # Return the corresponding Camelot code\n",
    "    if mode == 'major':\n",
    "        return camelot_major[pitch]\n",
    "    elif mode == 'minor':\n",
    "        return camelot_minor[pitch]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode in key: should be 'major' or 'minor'.\")\n",
    "\n",
    "\n",
    "def get_studio_bpm(beat_frames: np.ndarray, sr: int = 22050, hop_length: int = 512,\n",
    "                   variance_threshold: float = 0.01, window_length: int = 4) -> Tuple[Optional[float], Optional[float], Optional[float], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Analyze the provided beat frame indices to determine the studio BPM and the start frame of stable intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    - beat_frames (np.ndarray): Array of beat frame indices.\n",
    "    - sr (int): The sample rate of the audio. Default is 22050 Hz.\n",
    "    - hop_length (int): The number of samples per frame. Default is 512.\n",
    "    - variance_threshold (float): The threshold for the variance to consider a window of beats as stable.\n",
    "    - window_length (int): The number of beats to consider within each sliding window when calculating variance.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Optional[float], Optional[float], Optional[float], np.ndarray, np.ndarray]:\n",
    "        - The mean studio BPM (float or None if not determined).\n",
    "        - The median studio BPM (float or None if determined).\n",
    "        - The BPM that occurs most frequently near a whole number (float or None if not determined).\n",
    "        - The frame indices of the first beat of each stable interval.\n",
    "        - An array of beat interval durations that are considered stable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the time in seconds for each beat frame index\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Calculate beat intervals\n",
    "    beat_intervals = np.diff(beat_times)\n",
    "    total_intervals = len(beat_intervals)\n",
    "\n",
    "    # Store stable intervals (low-variance windows)\n",
    "    stable_intervals = []\n",
    "    stable_beat_indices = []\n",
    "\n",
    "    # Calculate variance in a sliding window\n",
    "    for i in range(total_intervals - window_length + 1):\n",
    "        window = beat_intervals[i:i + window_length]\n",
    "        if np.var(window) < variance_threshold:\n",
    "            # Extend the list with intervals from the current stable window\n",
    "            stable_intervals.extend(window)\n",
    "            # Record the frame index of the first beat in the stable window\n",
    "            stable_beat_indices.append(beat_frames[i])\n",
    "\n",
    "    # Initialize the BPM that occurs most frequently near a whole number to None\n",
    "    mode_studio_bpm = None\n",
    "\n",
    "    # If we found any stable intervals, calculate the BPMs\n",
    "    if stable_intervals:\n",
    "        # Calculate BPMs for each stable interval\n",
    "        stable_bpms = 60.0 / np.array(stable_intervals)\n",
    "        mean_studio_bpm = np.mean(stable_bpms)\n",
    "        median_studio_bpm = np.median(stable_bpms)\n",
    "\n",
    "        # Round BPMs to the nearest whole numbers and find the mode\n",
    "        rounded_bpms = np.round(stable_bpms)\n",
    "        mode_bpm, count = stats.mode(rounded_bpms)\n",
    "        if count > 0:\n",
    "            mode_studio_bpm = mode_bpm[0]\n",
    "    else:\n",
    "        # No stable intervals found; return None for mean and median BPM\n",
    "        mean_studio_bpm = None\n",
    "        median_studio_bpm = None\n",
    "\n",
    "    # Convert the stable beat indices to frame indices\n",
    "    stable_frames = beat_frames[stable_beat_indices]\n",
    "\n",
    "    # Return the mean and median studio BPM if calculated, the mode BPM, the frame indices of the stable beats, and the stable interval durations\n",
    "    return mean_studio_bpm, median_studio_bpm, mode_studio_bpm, stable_frames, stable_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091b2e2-07f3-4094-ab21-cb1906927142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# Define your directory and constants \n",
    "merged_df = pd.read_csv(r'..\\data\\dataframes\\sp_merged2.csv')\n",
    "mp3_directory = r\"..\\data\\audio_files\\processed\"\n",
    "export_directory = r\"..\\data\\pkl\"\n",
    "hop_length = 512\n",
    "sr = 22050\n",
    "data_list = []\n",
    "\n",
    "# Process each song\n",
    "for index, row in tqdm(merged_df.iterrows(), desc=\"Processing audio profiles\", total=merged_df.shape[0]):\n",
    "    audio_file_path = row['FilePath']\n",
    "    if os.path.exists(audio_file_path):\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_file_path, sr=sr)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        y_harm, y_perc = librosa.effects.hpss(y)\n",
    "\n",
    "        # Chroma profile\n",
    "        chroma_cq = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n",
    "        key, key_corr = detect_key_from_chromagram(chroma_cq, sr)\n",
    "        camelot = get_camelot(key)\n",
    "\n",
    "        # Tempo/rhythm profile\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "        studio_bpm, stable_intervals = get_studio_bpm(beats)\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "        tempogram_ratio = librosa.feature.tempogram_ratio(tg=tempogram, sr=sr)\n",
    "        \n",
    "        # Quantize beats and create measure grid\n",
    "        beat_grid, measure_grid = quantize_beats(beats, onset_env, studio_bpm, sr, hop_length, duration)\n",
    "        measure_numbers = np.arange(len(measure_grid))\n",
    "        measure_dict = {measure_number: measure_time for measure_number, measure_time in zip(measure_numbers, measure_grid)}\n",
    "        \n",
    "        # Data dictionary to hold features\n",
    "        data_dict = {\n",
    "            'SongID': row['SongID'], \n",
    "            'duration': duration, \n",
    "            'tempo': tempo, \n",
    "            'studio_bpm': studio_bpm,\n",
    "            'key': key,\n",
    "            'key_corr': key_corr,\n",
    "            'camelot_key': camelot\n",
    "        }\n",
    "        \n",
    "        # Append the data dictionary to the data list\n",
    "        data_list.append(data_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a dataframe\n",
    "new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Append this new dataframe to the original dataframe (if that's what you need)\n",
    "merged_df = merged_df.merge(new_data_df, on='SongID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b960d9-d63c-466a-8d86-e109984161cf",
   "metadata": {},
   "source": [
    "## Saving the audio features as pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e88e60-7a8f-4118-89f1-b58a59d3ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Define your directory and constants \n",
    "merged_df = pd.read_csv(r'..\\data\\dataframes\\sp_merged2.csv')\n",
    "\n",
    "mp3_directory = r\"..\\data\\audio_files\\processed\"\n",
    "export_directory = r\"..\\data\\pkl\"\n",
    "hop_length = 512\n",
    "sr = 22050\n",
    "\n",
    "\n",
    "# Process each song\n",
    "for index, row in tqdm(merged_df.iterrows(), desc=\"Processing audio profiles\", total=merged_df.shape[0]):\n",
    "    audio_file_path = row['FilePath']\n",
    "    if os.path.exists(audio_file_path):\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_file_path, sr=sr)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        y_harm, y_perc = librosa.effects.hpss(y)\n",
    "\n",
    "        # Chroma profile\n",
    "        chroma_cq = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n",
    "        key, key_corr = detect_key_from_chromagram(chroma_cq, sr)\n",
    "        camelot = get_camelot(key)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr, chroma=chroma_cq)\n",
    "\n",
    "        # Spectrogram\n",
    "        D = np.abs(librosa.stft(y))**2\n",
    "        S_mel = librosa.feature.melspectrogram(S=D, sr=sr)\n",
    "        S_mel_db = librosa.power_to_db(S_mel, ref=np.max)\n",
    "        # Centroid\n",
    "        centroid_mel = librosa.feature.spectral_centroid(S=S_mel, sr=sr, hop_length=hop_length)\n",
    "        # MFCC\n",
    "        mfccs = librosa.feature.mfcc(S=S_mel_db)\n",
    "        \n",
    "        # Tempo/rhythm profile\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "        studio_bpm, stable_intervals = get_studio_bpm(beats)\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "        tempogram_ratio = librosa.feature.tempogram_ratio(tg=tempogram, sr=sr)\n",
    "        \n",
    "        # Quantize beats and create measure grid\n",
    "        beat_grid, measure_grid = quantize_beats(beats, onset_env, studio_bpm, sr, hop_length, duration)\n",
    "        measure_numbers = np.arange(len(measure_grid))\n",
    "        measure_dict = {measure_number: measure_time for measure_number, measure_time in zip(measure_numbers, measure_grid)}\n",
    "\n",
    "        # Data dictionary to hold features\n",
    "        data = {\n",
    "            'SongID': row['SongID'], \n",
    "            'duration': duration, \n",
    "            'tempo': tempo, \n",
    "            'studio_bpm': studio_bpm,\n",
    "            'key': key,\n",
    "            'key_corr': key_corr,\n",
    "            'camelot_key': camelot,\n",
    "            'stable_intervals': stable_intervals.tolist(),\n",
    "            'y': y.tolist(),\n",
    "            'chroma_cq': chroma_cq.tolist(),\n",
    "            'tonnetz': tonnetz.tolist(),\n",
    "            'S_mel_db': S_mel_db.tolist(),\n",
    "            'centroid_mel': centroid_mel.tolist(),\n",
    "            'mfccs': mfccs.tolist(),\n",
    "            'tempogram': tempogram.tolist(),\n",
    "            'tempogram_ratio': tempogram_ratio.tolist(),\n",
    "            'MeasureDict': measure_dict\n",
    "        }\n",
    "\n",
    "        # Save to pickle file\n",
    "        pickle_file_path = os.path.join(export_directory, f\"{row['SongID']}.pkl\")  # Ensure proper path joining\n",
    "        pd.to_pickle(data, pickle_file_path)\n",
    "    else:\n",
    "        print(f\"File not found: {audio_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d082cd-cce0-43e8-bfce-4608f072f2d3",
   "metadata": {},
   "source": [
    "## Making visual plots for all songs using pkl audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93529837-2836-4b06-b0c3-28a7b6f2967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "merged_df = pd.read_csv(r'../data/dataframes/sp_merged2.csv')\n",
    "merged_df = merged_df['SongID']\n",
    "export_directory = r\"../figures/audio_plots\"\n",
    "pkl_directory = r\"../data/pkl\"\n",
    "sr = 22050  \n",
    "hop_length = 512\n",
    "\n",
    "def apply_measure_grid(ax, measure_grid, measure_numbers):\n",
    "    # Filter the major (every fourth) measures\n",
    "    major_indices = measure_numbers % 4 == 0\n",
    "\n",
    "    # Set major x-axis ticks and labels (for measure 0 and every fourth measure after)\n",
    "    ax.set_xticks(measure_grid[major_indices])\n",
    "    ax.set_xticklabels(measure_numbers[major_indices])\n",
    "\n",
    "    # Set minor x-axis ticks (for intermediate measures)\n",
    "    ax.set_xticks(measure_grid[~major_indices], minor=True)\n",
    "\n",
    "    # Overlay the major measure grid lines on the plot (for measure 0 and every fourth measure after)\n",
    "    ax.vlines(measure_grid[major_indices], ax.get_ylim()[0], ax.get_ylim()[1], color='green', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Overlay the minor measure grid lines on the plot (for intermediate measures)\n",
    "    ax.vlines(measure_grid[~major_indices], ax.get_ylim()[0], ax.get_ylim()[1], color='grey', linestyle=':', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Measure Number')\n",
    "\n",
    "    \n",
    "def load_pickle_data(pkl_path):\n",
    "    with open(pkl_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "        \n",
    "for song_id in tqdm(merged_df, desc=\"Processing audio profiles\"):\n",
    "    export_fig_path = os.path.join(export_directory, f\"{song_id}.png\")\n",
    "    pkl_path = os.path.join(pkl_directory, f\"{song_id}.pkl\")\n",
    "\n",
    "    if not os.path.exists(export_fig_path):\n",
    "        if os.path.exists(pkl_path):\n",
    "            try:\n",
    "                data = load_pickle_data(pkl_path)\n",
    "\n",
    "                # Extracting the individual components from the data dictionary\n",
    "                y_harm = np.asarray(data['y_harm'])\n",
    "                y_perc = np.asarray(data['y_perc'])\n",
    "                S_mel_db = np.asarray(data['S_mel_db'])\n",
    "                tempogram = np.asarray(data['tempogram'])\n",
    "                tempogram_ratio = np.asarray(data['tempogram_ratio'])\n",
    "                chroma_cq = np.asarray(data['chroma_cq'])\n",
    "                tonnetz = np.asarray(data['tonnetz'])\n",
    "                duration = data['duration']\n",
    "                measure_grid = np.array(list(data['MeasureDict'].values()))\n",
    "                measure_numbers = np.array(list(data['MeasureDict'].keys()))\n",
    "        \n",
    "                # Create subplots\n",
    "                fig, axs = plt.subplots(6, 1, figsize=(20, 30), dpi=125)\n",
    "\n",
    "                # Harmonic/Percussive Waveform plot\n",
    "                axs[0].plot(np.linspace(0, duration, len(y_harm)), y_harm, alpha=0.5, label='Harmonic', color='b')\n",
    "                axs[0].plot(np.linspace(0, duration, len(y_perc)), y_perc, alpha=0.5, label='Percussive', color='r')\n",
    "                apply_measure_grid(axs[0], measure_grid, measure_numbers)\n",
    "                axs[0].set_title('Harmonic and Percussive Waveform')\n",
    "                axs[0].set_xlim([0, duration])\n",
    "        \n",
    "                # Mel Spectrogram plot\n",
    "                librosa.display.specshow(S_mel_db, sr=sr, x_axis='time', y_axis='mel', ax=axs[1], fmax=8000)\n",
    "                apply_measure_grid(axs[1], measure_grid, measure_numbers)\n",
    "                axs[1].set_title('Mel Spectrogram')\n",
    "                # Set the y-axis limits\n",
    "                axs[1].set_ylim(0, 8000)  # Assuming the fmax is 8000 Hz as specified in the specshow call\n",
    "        \n",
    "                # Tempogram plot\n",
    "                librosa.display.specshow(tempogram, sr=sr, hop_length=512, x_axis='time', y_axis='tempo', cmap='magma', ax=axs[2])\n",
    "                apply_measure_grid(axs[2], measure_grid, measure_numbers)\n",
    "                axs[2].set_title('Tempogram')\n",
    "        \n",
    "                # Tempogram ratio\n",
    "                # Define note labels for tempogram ratio\n",
    "                note_labels = [\n",
    "                    'Sixteenth note',\n",
    "                    'Dotted sixteenth',\n",
    "                    'Eighth triplet',\n",
    "                    'Eighth note',\n",
    "                    'Dotted eighth',\n",
    "                    'Quarter triplet',\n",
    "                    'Quarter note',\n",
    "                    'Dotted quarter',\n",
    "                    'Half triplet',\n",
    "                    'Half note',\n",
    "                    'Dotted half note',\n",
    "                    'Whole triplet',\n",
    "                    'Whole note'\n",
    "                ]\n",
    "                \n",
    "                librosa.display.specshow(tempogram_ratio, x_axis='time', ax=axs[3], sr=sr)\n",
    "                axs[3].set_xlim([0, duration])\n",
    "                apply_measure_grid(axs[3], measure_grid, measure_numbers)\n",
    "                axs[3].set_yticks(range(len(note_labels)))\n",
    "                axs[3].set_yticklabels(note_labels)\n",
    "                axs[3].set_title('Tempogram Ratio')\n",
    "        \n",
    "                # Chroma CQT plot\n",
    "                librosa.display.specshow(chroma_cq, y_axis='chroma', x_axis='time', ax=axs[4])\n",
    "                apply_measure_grid(axs[4], measure_grid, measure_numbers)\n",
    "                axs[4].set_title('Chroma CQT')\n",
    "        \n",
    "                # Tonnetz plot\n",
    "                librosa.display.specshow(tonnetz, sr=sr, hop_length=hop_length, y_axis='tonnetz', x_axis='time', ax=axs[5])\n",
    "                apply_measure_grid(axs[5], measure_grid, measure_numbers)\n",
    "                axs[5].set_title('Tonnetz')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(export_fig_path)\n",
    "                plt.close(fig)\n",
    "                del y_harm, y_perc, S_mel_db, tempogram, tempogram_ratio, chroma_cq, tonnetz, duration, measure_grid, measure_numbers\n",
    "                gc.collect()\n",
    "\n",
    "            except (EOFError, pickle.UnpicklingError) as e:\n",
    "                print(f\"Failed to load data for {song_id} due to {e} Skipping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410dca03-875f-4dbc-a361-2a6def8f41e5",
   "metadata": {},
   "source": [
    "# Data-wrangling Notebook\n",
    "This notebook is comprised of three parts:\n",
    "    1) Audio pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8160b8-9891-49df-9e74-08ffbac0e301",
   "metadata": {
    "id": "2e8160b8-9891-49df-9e74-08ffbac0e301"
   },
   "source": [
    "## Part 1: Audio Pre-processing\n",
    "- The following code iterates through all MP3 files in the `input_dir` and performs the following operations:\n",
    "  - Strips the silence from the audio.\n",
    "  - Parses the MP3 metadata.\n",
    "  - Exports the processed audio to the `output_dir` with a new filename based on a song ID counter.\n",
    "  - Appends song information to the `song_data_list`.\n",
    "  - Converts `song_data_list` to a pandas DataFrame and exports to a CSV file for further use.\n",
    "  - New songs can be added into `input_dir` and processed and appended into the existing CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df676a61-394b-45d2-85f6-681f1cd7d6ca",
   "metadata": {
    "id": "df676a61-394b-45d2-85f6-681f1cd7d6ca"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from mutagen.mp3 import MP3\n",
    "from mutagen.easyid3 import EasyID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bd4bb-8677-4492-ac05-a871202fbcf1",
   "metadata": {
    "id": "2c5bd4bb-8677-4492-ac05-a871202fbcf1"
   },
   "outputs": [],
   "source": [
    "def strip_silence(sound, min_silence_len=1000, silence_thresh=-50):\n",
    "    \"\"\"\n",
    "    Strip silence from an audio segment.\n",
    "\n",
    "    Args:\n",
    "        sound: The audio segment to strip silence from.\n",
    "        min_silence_len: The minimum length of silence (in milliseconds) to be removed.\n",
    "        silence_thresh: The threshold (in dB) below which a segment is considered silent.\n",
    "\n",
    "    Returns:\n",
    "        An audio segment with the silence removed.\n",
    "    \"\"\"\n",
    "\n",
    "    nonsilent_ranges = detect_nonsilent(sound, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "    return reduce(lambda acc, val: acc + sound[val[0]:val[1]], nonsilent_ranges, AudioSegment.empty()) if nonsilent_ranges else sound\n",
    "\n",
    "\n",
    "def parse_mp3_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Parse metadata from an MP3 file.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the track title, artists, and genre.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        audiofile = MP3(file_path, ID3=EasyID3)\n",
    "        track_title = audiofile.get('title', [None])[0]\n",
    "        artists = audiofile.get('artist', [None])[0]\n",
    "        genre = audiofile.get('genre', [None])[0]\n",
    "        return track_title, artists, genre\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading metadata for {file_path}: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f286c3-cb22-4a48-b7d2-1465dc4d06c3",
   "metadata": {
    "id": "f7f286c3-cb22-4a48-b7d2-1465dc4d06c3"
   },
   "source": [
    "#### Run the cell below when processing songs for the ***first time***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b495f0-5624-4b2f-94ae-a385f0bb86d9",
   "metadata": {
    "id": "75b495f0-5624-4b2f-94ae-a385f0bb86d9",
    "outputId": "f65250f3-41dd-4be0-b41f-f3ebe623cb3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████| 554/554 [3:16:50<00:00, 21.32s/file]  \n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "input_dir = r'..\\data\\audio_files\\raw'\n",
    "output_dir = r'..\\data\\audio_files\\processed'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize song data DataFrame\n",
    "df = pd.DataFrame(columns=[\"SongID\", \"TrackName\", \"Artists\", \"Genre\", \"FilePath\"])\n",
    "\n",
    "# Initialize song ID counter and list for song data\n",
    "song_id_counter = 1\n",
    "song_data_list = []\n",
    "\n",
    "# Process audio files\n",
    "for filename in tqdm([f for f in os.listdir(input_dir) if f.endswith('.mp3')], desc=\"Processing audio files\", unit=\"file\"):\n",
    "    file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "    # Generate new filename using song ID counter\n",
    "    new_filename = f\"{song_id_counter}.mp3\"\n",
    "    output_file_path = os.path.join(output_dir, new_filename)\n",
    "\n",
    "    # Load and strip silence from audio file\n",
    "    sound = AudioSegment.from_file(file_path)\n",
    "    stripped_sound = strip_silence(sound)\n",
    "\n",
    "    # Parse metadata\n",
    "    track_title, artists, genre = parse_mp3_metadata(file_path)\n",
    "\n",
    "    # Export stripped audio with updated metadata\n",
    "    stripped_sound.export(output_file_path, format='mp3', tags={\"title\": track_title, \"artist\": artists, \"genre\": genre})\n",
    "\n",
    "    # Create dictionary of song information and add to list\n",
    "    song_data_list.append({\n",
    "        \"SongID\": song_id_counter,\n",
    "        \"TrackName\": track_title,\n",
    "        \"Artists\": artists,\n",
    "        \"Genre\": genre,\n",
    "        \"FilePath\": output_file_path\n",
    "    })\n",
    "\n",
    "    # Increment song ID counter\n",
    "    song_id_counter += 1\n",
    "\n",
    "# Convert list of dictionaries to DataFrame and save to CSV\n",
    "df = pd.DataFrame(song_data_list)\n",
    "df.to_csv(r'..\\data\\dataframes\\song_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff8e874-f27b-4cfc-ade1-c96e43b61d26",
   "metadata": {
    "id": "3ff8e874-f27b-4cfc-ade1-c96e43b61d26"
   },
   "source": [
    "#### Run the cell below if processing additional songs at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedbbfa-e8ce-4468-92d1-c2cc69c23ce1",
   "metadata": {
    "id": "fdedbbfa-e8ce-4468-92d1-c2cc69c23ce1",
    "outputId": "be8bddf5-1e8c-40fd-90bc-baa12871e39e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new audio files: 100%|██████████| 869/869 [2:17:54<00:00,  9.52s/file]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 315 new songs and saved to song_data_20240109110632.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "input_dir = r'..\\data\\audio_files\\raw'\n",
    "output_dir = r'..\\data\\audio_files\\processed'\n",
    "song_data_csv = r'..\\data\\dataframes\\song_data.csv'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load existing song data\n",
    "if os.path.exists(song_data_csv):\n",
    "    existing_df = pd.read_csv(song_data_csv)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"SongID\", \"TrackName\", \"Artists\", \"Genre\", \"FilePath\"])\n",
    "\n",
    "# Initialize song ID counter with the next available ID\n",
    "song_id_counter = existing_df['SongID'].max() + 1 if not existing_df.empty else 1\n",
    "song_data_list = []\n",
    "\n",
    "# Process all new audio files in the input directory\n",
    "for filename in tqdm([f for f in os.listdir(input_dir) if f.endswith('.mp3')], desc=\"Processing new audio files\", unit=\"file\"):\n",
    "    file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "    # Parse metadata to compare with existing data\n",
    "    track_title, artists, genre = parse_mp3_metadata(file_path)\n",
    "\n",
    "    # Check if the song already exists in the DataFrame\n",
    "    if not existing_df[\n",
    "        (existing_df['TrackName'] == track_title) &\n",
    "        (existing_df['Artists'] == artists)\n",
    "    ].empty:\n",
    "        continue  # Skip the file if it already exists\n",
    "\n",
    "    new_filename = f\"{song_id_counter}.mp3\"\n",
    "    output_file_path = os.path.join(output_dir, new_filename)\n",
    "\n",
    "    sound = AudioSegment.from_file(file_path)\n",
    "    stripped_sound = strip_silence(sound)\n",
    "    stripped_sound.export(output_file_path, format='mp3', tags={\"title\": track_title, \"artist\": artists, \"genre\": genre})\n",
    "\n",
    "    # Append new song data to the list\n",
    "    song_data_list.append({\n",
    "        \"SongID\": song_id_counter,\n",
    "        \"TrackName\": track_title,\n",
    "        \"Artists\": artists,\n",
    "        \"Genre\": genre,\n",
    "        \"FilePath\": output_file_path\n",
    "    })\n",
    "\n",
    "    # Increment the song ID counter\n",
    "    song_id_counter += 1\n",
    "\n",
    "# If we have new songs, append them to the existing dataframe and save to a new CSV with a unique identifier\n",
    "if song_data_list:\n",
    "    new_songs_df = pd.DataFrame(song_data_list)\n",
    "    updated_df = pd.concat([existing_df, new_songs_df], ignore_index=True)\n",
    "\n",
    "    # Get the current timestamp to use as a unique identifier\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    new_csv_filename = f'song_data_{timestamp}.csv'\n",
    "    new_song_data_csv = os.path.join(os.path.dirname(song_data_csv), new_csv_filename)\n",
    "\n",
    "    updated_df.to_csv(new_song_data_csv, index=False)\n",
    "    print(f\"Processed {len(song_data_list)} new songs and saved to {new_csv_filename}.\")\n",
    "else:\n",
    "    print(\"No new songs to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6221b7d-af3b-4322-a5b4-c7d4e713fbee",
   "metadata": {},
   "source": [
    "## Part 2: Spotify Data Extraction\n",
    "\n",
    "Using the DataFrame we just created, we'll search for each track on Spotify and extract various Spotify-generated audio features and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a9f3e4-6981-4f5b-af44-4469b49651b5",
   "metadata": {
    "id": "a3a9f3e4-6981-4f5b-af44-4469b49651b5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369d748-6f80-492a-8218-0a23abe60e36",
   "metadata": {},
   "source": [
    "The Spotify client is initialized with credentials stored in a JSON file. These credentials include the `client_id` and `client_secret` necessary for accessing the Spotify Web API. The `SpotifyClientCredentials` manager handles the OAuth 2.0 flow for server-to-server authentication.\n",
    "\n",
    "Using any text editor, create a new text file containing the following information. Save the text file as \"spotify_credentials.json\" and place in '../data/reference' folder. \n",
    "\n",
    "```\n",
    "{\n",
    "  \"client_id\":\"REPLACE WITH CLIENT ID\",\n",
    "  \"client_secret\":\"REPLACE WITH SECRET\",\n",
    "  \"user_id\":\"REPLACE WITH SPOTIFY USERNAME\"\n",
    "}\n",
    "```\n",
    "* Retrieve your client ID and secret here: https://developer.spotify.com/dashboard/\n",
    "* Retrieve your username here: https://www.spotify.com/us/account/overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568fd0ba-04cf-4995-b83c-897921707590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spotify credentials\n",
    "credentials_path = r'../data/reference/spotify_credentials.json'  # Path to your spotify_credentials.json file\n",
    "\n",
    "with open(credentials_path, 'r') as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Initialize Spotify client\n",
    "auth_manager = SpotifyClientCredentials(client_id=creds['client_id'], client_secret=creds['client_secret'])\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2076c6-b3b3-43b3-b47c-202cc435562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case rate-limited by Spotify\n",
    "def make_spotify_request(sp, track_title, artists):\n",
    "    retries = 3  # Maximum number of retries\n",
    "    backoff_factor = 0.5  # Factor to determine the next sleep time\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Format the search query\n",
    "            query = f\"track:{track_title} artist:{artists[0].strip()}\"  # Using the first artist for simplicity\n",
    "            # Search for tracks\n",
    "            return sp.search(q=query, type='track', limit=1)\n",
    "        except spotipy.exceptions.SpotifyException as e:\n",
    "            if e.http_status == 429:\n",
    "                sleep_time = int(e.headers.get('Retry-After', 1))\n",
    "                time.sleep(sleep_time)\n",
    "            elif 500 <= e.http_status < 600:\n",
    "                sleep_time = backoff_factor * (2 ** attempt)\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise\n",
    "    return None  # If all retries failed\n",
    "\n",
    "def extract_spotify_metadata_features(track_title, artists, sp):\n",
    "    \"\"\"\n",
    "    Extracts Spotify metadata and audio features for a given track title and list of artists.\n",
    "\n",
    "    This function performs a search on Spotify for the track using the provided title and artist names.\n",
    "    If a match is found, it retrieves the track's ID, artist IDs, genres, and audio features.\n",
    "    The results are returned in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - track_title (str): The title of the track to search for.\n",
    "    - artists (list): A list of artist names associated with the track.\n",
    "    - sp (Spotify client): An authenticated instance of the Spotify client to perform API calls.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A pandas DataFrame containing the Spotify metadata and audio features for the track.\n",
    "                 If no match is found, an empty DataFrame with the specified columns is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    track_id, artist_ids, unique_genres = None, None, set()\n",
    "\n",
    "    # Search for track ID, artist IDs, and genres\n",
    "    for artist in artists:\n",
    "        query = f\"track:{track_title} artist:{artist.strip()}\"\n",
    "        track_results = sp.search(q=query, type='track', limit=1)\n",
    "        if track_results['tracks']['items']:\n",
    "            track_item = track_results['tracks']['items'][0]\n",
    "            track_id = track_item['id']\n",
    "            artist_ids = [artist['id'] for artist in track_item['artists']]\n",
    "            for artist_id in artist_ids:\n",
    "                artist_genre = sp.artist(artist_id)['genres']\n",
    "                unique_genres.update(artist_genre)\n",
    "            break\n",
    "\n",
    "    # Retrieve audio features if a match was found\n",
    "    if track_id and artist_ids:\n",
    "        audio_features_dict = sp.audio_features(track_id)[0]\n",
    "        audio_features_dict['sp_genre'] = list(unique_genres)\n",
    "        audio_features_df = pd.DataFrame([audio_features_dict])\n",
    "        columns_to_drop = ['type', 'id', 'uri', 'track_href', 'analysis_url', 'duration_ms', 'mode']\n",
    "        audio_features_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    else:\n",
    "        audio_features_df = pd.DataFrame()\n",
    "\n",
    "    return audio_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b6d3bd-f9ca-433c-8987-77247368963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching track features: 100%|██████████| 869/869 [07:19<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFramge or CSV that we created in the step above\n",
    "df = pd.read_csv(r'..\\data\\dataframes\\song_data_20240109110632.csv')\n",
    "\n",
    "# Initialize an empty DataFrame to store all tracks' info\n",
    "spotify_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each row of the DataFrame\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Fetching track features\"):    \n",
    "    track_title = row['TrackName']\n",
    "    artists = [artist.strip() for artist in row['Artists'].split('/')]  # Split artists by '/' and strip whitespace\n",
    "\n",
    "    # Use the function to extract metadata and audio features\n",
    "    track_features_df = extract_spotify_metadata_features(track_title, artists, sp)\n",
    "\n",
    "    # If the DataFrame is not empty, merge the results with the song details\n",
    "    if not track_features_df.empty:\n",
    "        track_features_df['SongID'] = row['SongID']  # Assuming there's a 'SongID' column in the original DataFrame\n",
    "        track_features_df['sp_genre'] = ', '.join(track_features_df.loc[0, 'sp_genre'])  # Join genres into a single string\n",
    "        # Append this track's DataFrame to the main DataFrame\n",
    "        spotify_df = pd.concat([spotify_df, track_features_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67be48d8-8a24-4238-97bb-176cb0f57370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 840 entries, 0 to 839\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   sp_danceability      840 non-null    float64\n",
      " 1   sp_energy            840 non-null    float64\n",
      " 2   sp_key               840 non-null    int64  \n",
      " 3   sp_loudness          840 non-null    float64\n",
      " 4   sp_speechiness       840 non-null    float64\n",
      " 5   sp_acousticness      840 non-null    float64\n",
      " 6   sp_instrumentalness  840 non-null    float64\n",
      " 7   sp_liveness          840 non-null    float64\n",
      " 8   sp_valence           840 non-null    float64\n",
      " 9   sp_tempo             840 non-null    float64\n",
      " 10  sp_time_signature    840 non-null    int64  \n",
      " 11  sp_genre             840 non-null    object \n",
      " 12  SongID               840 non-null    int64  \n",
      "dtypes: float64(9), int64(3), object(1)\n",
      "memory usage: 85.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Adding 'sp_' in front of columns to indicate they are Spotify-derived features\n",
    "prefix_cols = ['danceability', 'energy', 'key', 'loudness', 'speechiness',\n",
    "               'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "               'time_signature']\n",
    "\n",
    "spotify_df.rename(columns=lambda x: 'sp_' + x if x in prefix_cols else x, inplace=True)\n",
    "spotify_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e313309-ea28-42fb-b890-8e9a5a633f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with DataFrame made in Part 1\n",
    "merged_df = pd.merge(df, spotify_df, on='SongID', how='left')\n",
    "merged_df.to_csv(r'../data/dataframes/sp_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959e2a5-a570-4c27-8301-47977b65ac3c",
   "metadata": {},
   "source": [
    "# Part 3: Audio Features & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0868cb9-764f-4b54-8766-65914010005f",
   "metadata": {},
   "source": [
    "## Adding rhythm/chroma features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e6dc1-e95f-4588-a221-99686f6fff31",
   "metadata": {},
   "source": [
    "### Function to create and plot a measure grid for each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb31e3cd-9e1c-4791-b03f-7f96e1602123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_beats(beats, onset_env, tempo, sr, hop_length, duration):\n",
    "    \"\"\"\n",
    "    Adjusts beat times to the nearest detected onsets and creates beat and measure grids for the audio.\n",
    "\n",
    "    This function assumes the beat times are evenly spaced within each measure and are in 4/4 time signature. It also backtracks the beats using the onset envelope to align them to the nearest detected onset.\n",
    "\n",
    "    Parameters:\n",
    "    beats : np.ndarray\n",
    "        An array of beat times in frame units generated from librosa.beat.beat_track.\n",
    "    onset_env : np.ndarray\n",
    "        Onset envelope of the audio signal, used for backtracking beats.\n",
    "    tempo : float\n",
    "        Estimated tempo of the audio in beats per minute.\n",
    "    sr : int\n",
    "        Sampling rate of the audio signal.\n",
    "    hop_length : int\n",
    "        Hop length used in the onset detection and beat tracking.\n",
    "    duration : float\n",
    "        Duration of the audio signal in seconds.\n",
    "\n",
    "    Returns:\n",
    "    beat_grid : np.ndarray\n",
    "        Array of quantized beat times.\n",
    "    measure_grid : np.ndarray\n",
    "        Array of quantized measure start times.\n",
    "    beats_per_measure : int\n",
    "        Number of beats per measure, which is set to 4 for a 4/4 time signature.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the `beats` array is empty or not one-dimensional.\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if beats.ndim != 1:\n",
    "        raise ValueError(\"The 'beats' array must be one-dimensional.\")\n",
    "    if beats.size == 0:\n",
    "        raise ValueError(\"The 'beats' array must not be empty.\")\n",
    "    \n",
    "    # Hardcoded assumption of 4/4 time signature\n",
    "    beats_per_measure = 4\n",
    "\n",
    "    # Track beats to align them to the nearest detected onset\n",
    "    beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Calculate the beat interval (seconds per beat)\n",
    "    beat_interval = 60.0 / tempo\n",
    "\n",
    "    # Backtrack from the first beat to align with time 0 if necessary\n",
    "    first_beat_time = 0\n",
    "\n",
    "    # Create beat grid from the first beat time to the end of the song\n",
    "    beat_grid = np.arange(first_beat_time, duration, beat_interval)\n",
    "\n",
    "    # Ensure beat grid does not go past the duration of the song\n",
    "    beat_grid = beat_grid[beat_grid <= duration]\n",
    "\n",
    "    # Create measure grid\n",
    "    measure_indices = np.arange(0, len(beat_grid), beats_per_measure)\n",
    "    measure_grid = beat_grid[measure_indices]\n",
    "\n",
    "    # Ensure measure grid does not go past the duration of the song\n",
    "    measure_grid = measure_grid[measure_grid <= duration]\n",
    "\n",
    "    return beat_grid, measure_grid\n",
    "\n",
    "\n",
    "def apply_measure_grid(ax, measure_grid):\n",
    "    \"\"\"\n",
    "    This function takes an axis object and applies measure grid lines,\n",
    "    sets x-ticks to measure start times for every fourth measure starting from measure 0,\n",
    "    labels them with measure numbers, and applies sub-ticks for intermediate measure times.\n",
    "    \n",
    "    Parameters:\n",
    "    ax (matplotlib.axes.Axes): The axis object to modify.\n",
    "    measure_grid (list or array): The list or array of measure start times in seconds.\n",
    "    \"\"\"\n",
    "    measure_numbers = np.arange(len(measure_grid))\n",
    "    # Adjust the list to start from measure 0 and get every fourth measure\n",
    "    major_measure_indices = [i for i, measure_num in enumerate(measure_numbers) if (measure_num) % 4 == 0]\n",
    "    major_measures = [measure_grid[i] for i in major_measure_indices]\n",
    "    major_labels = [measure_numbers[i] for i in major_measure_indices]\n",
    "    \n",
    "    # Set major x-axis ticks and labels (for measure 0 and every fourth measure after)\n",
    "    ax.set_xticks(major_measures, minor=False)\n",
    "    ax.set_xticklabels(major_labels, minor=False)\n",
    "\n",
    "    # Set minor x-axis ticks (for intermediate measures)\n",
    "    minor_measures = [measure for i, measure in enumerate(measure_grid) if i not in major_measure_indices]\n",
    "    ax.set_xticks(minor_measures, minor=True)\n",
    "    \n",
    "    # Overlay the major measure grid lines on the plot (for measure 0 and every fourth measure after)\n",
    "    for measure_time in major_measures:\n",
    "        ax.axvline(x=measure_time, color='green', linestyle='--', linewidth=2)  # Adjusted linewidth for major ticks\n",
    "    \n",
    "    # Overlay the minor measure grid lines on the plot (for intermediate measures)\n",
    "    for measure_time in minor_measures:\n",
    "        ax.axvline(x=measure_time, color='grey', linestyle=':', linewidth=1, alpha=0.8)  # Adjusted linewidth for minor ticks\n",
    "    \n",
    "    ax.set_xlabel('Measure Number')\n",
    "\n",
    "\n",
    "# Function to detect key from a chromagram using Krumhansl-Schmuckler key-finding algorithm profiles\n",
    "def detect_key_from_chromagram(chromagram, sr):\n",
    "    pitches = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "    # Calculate the sum of each pitch class across all time frames\n",
    "    chroma_vals = np.sum(chromagram, axis=1)\n",
    "\n",
    "    # Krumhansl-Schmuckler key-finding algorithm profiles\n",
    "    maj_profile = [6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88]\n",
    "    min_profile = [6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17]\n",
    "\n",
    "    # Correlation for major and minor keys\n",
    "    maj_key_corrs = [np.corrcoef(maj_profile, np.roll(chroma_vals, i))[1, 0] for i in range(12)]\n",
    "    min_key_corrs = [np.corrcoef(min_profile, np.roll(chroma_vals, i))[1, 0] for i in range(12)]\n",
    "\n",
    "    # Combine correlations and keys\n",
    "    key_corrs = maj_key_corrs + min_key_corrs\n",
    "    keys = [p + ' major' for p in pitches] + [p + ' minor' for p in pitches]\n",
    "\n",
    "    # Determine the best key\n",
    "    best_idx = np.argmax(key_corrs)\n",
    "    best_key = keys[best_idx]\n",
    "    best_corr = key_corrs[best_idx]\n",
    "\n",
    "    return best_key, best_corr\n",
    "\n",
    "\n",
    "# Function to convert standard key into Camelot key notation\n",
    "def get_camelot(key):\n",
    "    # Mapping from musical key to Camelot code\n",
    "    camelot_major = {\n",
    "        'B': '1B', 'F#': '2B', 'C#': '3B', 'G#': '4B', 'D#': '5B',\n",
    "        'A#': '6B', 'F': '7B', 'C': '8B', 'G': '9B', 'D': '10B', 'A': '11B', 'E': '12B'\n",
    "    }\n",
    "\n",
    "    camelot_minor = {\n",
    "        'G#': '1A', 'D#': '2A', 'A#': '3A', 'F': '4A', 'C': '5A',\n",
    "        'G': '6A', 'D': '7A', 'A': '8A', 'E': '9A', 'B': '10A', 'F#': '11A', 'C#': '12A'\n",
    "    }\n",
    "\n",
    "    # Split the detected key into pitch and mode\n",
    "    pitch, mode = key.split(' ')\n",
    "\n",
    "    # Return the corresponding Camelot code\n",
    "    if mode == 'major':\n",
    "        return camelot_major[pitch]\n",
    "    elif mode == 'minor':\n",
    "        return camelot_minor[pitch]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode in key: should be 'major' or 'minor'.\")\n",
    "\n",
    "\n",
    "def get_studio_bpm(beat_frames: np.ndarray, sr: int = 22050, hop_length: int = 512,\n",
    "                   variance_threshold: float = 0.01, window_length: int = 4) -> Tuple[Optional[float], Optional[float], Optional[float], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Analyze the provided beat frame indices to determine the studio BPM and the start frame of stable intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    - beat_frames (np.ndarray): Array of beat frame indices.\n",
    "    - sr (int): The sample rate of the audio. Default is 22050 Hz.\n",
    "    - hop_length (int): The number of samples per frame. Default is 512.\n",
    "    - variance_threshold (float): The threshold for the variance to consider a window of beats as stable.\n",
    "    - window_length (int): The number of beats to consider within each sliding window when calculating variance.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Optional[float], Optional[float], Optional[float], np.ndarray, np.ndarray]:\n",
    "        - The mean studio BPM (float or None if not determined).\n",
    "        - The median studio BPM (float or None if determined).\n",
    "        - The BPM that occurs most frequently near a whole number (float or None if not determined).\n",
    "        - The frame indices of the first beat of each stable interval.\n",
    "        - An array of beat interval durations that are considered stable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the time in seconds for each beat frame index\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Calculate beat intervals\n",
    "    beat_intervals = np.diff(beat_times)\n",
    "    total_intervals = len(beat_intervals)\n",
    "\n",
    "    # Store stable intervals (low-variance windows)\n",
    "    stable_intervals = []\n",
    "    stable_beat_indices = []\n",
    "\n",
    "    # Calculate variance in a sliding window\n",
    "    for i in range(total_intervals - window_length + 1):\n",
    "        window = beat_intervals[i:i + window_length]\n",
    "        if np.var(window) < variance_threshold:\n",
    "            # Extend the list with intervals from the current stable window\n",
    "            stable_intervals.extend(window)\n",
    "            # Record the frame index of the first beat in the stable window\n",
    "            stable_beat_indices.append(beat_frames[i])\n",
    "\n",
    "    # Initialize the BPM that occurs most frequently near a whole number to None\n",
    "    mode_studio_bpm = None\n",
    "\n",
    "    # If we found any stable intervals, calculate the BPMs\n",
    "    if stable_intervals:\n",
    "        # Calculate BPMs for each stable interval\n",
    "        stable_bpms = 60.0 / np.array(stable_intervals)\n",
    "        mean_studio_bpm = np.mean(stable_bpms)\n",
    "        median_studio_bpm = np.median(stable_bpms)\n",
    "\n",
    "        # Round BPMs to the nearest whole numbers and find the mode\n",
    "        rounded_bpms = np.round(stable_bpms)\n",
    "        mode_bpm, count = stats.mode(rounded_bpms)\n",
    "        if count > 0:\n",
    "            mode_studio_bpm = mode_bpm[0]\n",
    "    else:\n",
    "        # No stable intervals found; return None for mean and median BPM\n",
    "        mean_studio_bpm = None\n",
    "        median_studio_bpm = None\n",
    "\n",
    "    # Convert the stable beat indices to frame indices\n",
    "    stable_frames = beat_frames[stable_beat_indices]\n",
    "\n",
    "    # Return the mean and median studio BPM if calculated, the mode BPM, the frame indices of the stable beats, and the stable interval durations\n",
    "    return mean_studio_bpm, median_studio_bpm, mode_studio_bpm, stable_frames, stable_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4ae41a-150a-4d09-86d9-ddc24c10ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio profiles: 100%|██████████| 554/554 [1:20:07<00:00,  8.68s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# Define your directory and constants \n",
    "merged_df = pd.read_csv(r'..\\data\\dataframes\\sp_merged2.csv')\n",
    "mp3_directory = r\"..\\data\\audio_files\\processed\"\n",
    "export_directory = r\"..\\data\\pkl\"\n",
    "hop_length = 512\n",
    "sr = 22050\n",
    "data_list = []\n",
    "\n",
    "# Process each song\n",
    "for index, row in tqdm(merged_df.iterrows(), desc=\"Processing audio profiles\", total=merged_df.shape[0]):\n",
    "    audio_file_path = row['FilePath']\n",
    "    if os.path.exists(audio_file_path):\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_file_path, sr=sr)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        y_harm, y_perc = librosa.effects.hpss(y)\n",
    "\n",
    "        # Chroma profile\n",
    "        chroma_cq = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n",
    "        key, key_corr = detect_key_from_chromagram(chroma_cq, sr)\n",
    "        camelot = get_camelot(key)\n",
    "\n",
    "        # Tempo/rhythm profile\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "        studio_bpm, stable_intervals = get_studio_bpm(beats)\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "        tempogram_ratio = librosa.feature.tempogram_ratio(tg=tempogram, sr=sr)\n",
    "        \n",
    "        # Quantize beats and create measure grid\n",
    "        beat_grid, measure_grid = quantize_beats(beats, onset_env, studio_bpm, sr, hop_length, duration)\n",
    "        measure_numbers = np.arange(len(measure_grid))\n",
    "        measure_dict = {measure_number: measure_time for measure_number, measure_time in zip(measure_numbers, measure_grid)}\n",
    "        \n",
    "        # Data dictionary to hold features\n",
    "        data_dict = {\n",
    "            'SongID': row['SongID'], \n",
    "            'duration': duration, \n",
    "            'tempo': tempo, \n",
    "            'studio_bpm': studio_bpm,\n",
    "            'key': key,\n",
    "            'key_corr': key_corr,\n",
    "            'camelot_key': camelot\n",
    "        }\n",
    "        \n",
    "        # Append the data dictionary to the data list\n",
    "        data_list.append(data_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a dataframe\n",
    "new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Append this new dataframe to the original dataframe (if that's what you need)\n",
    "merged_df = merged_df.merge(new_data_df, on='SongID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78f76dc-cb80-4f25-b6d7-75d1cf4187d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SongID</th>\n",
       "      <th>TrackName</th>\n",
       "      <th>Artists</th>\n",
       "      <th>Genre</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>sp_danceability</th>\n",
       "      <th>sp_energy</th>\n",
       "      <th>sp_key</th>\n",
       "      <th>sp_loudness</th>\n",
       "      <th>sp_speechiness</th>\n",
       "      <th>...</th>\n",
       "      <th>sp_valence</th>\n",
       "      <th>sp_tempo</th>\n",
       "      <th>sp_time_signature</th>\n",
       "      <th>sp_genre</th>\n",
       "      <th>duration</th>\n",
       "      <th>tempo</th>\n",
       "      <th>studio_bpm</th>\n",
       "      <th>key</th>\n",
       "      <th>key_corr</th>\n",
       "      <th>camelot_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bass Inside</td>\n",
       "      <td>AC Slater</td>\n",
       "      <td>Bass House</td>\n",
       "      <td>..\\data\\audio_files\\processed\\1.mp3</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.838</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.838</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464</td>\n",
       "      <td>126.007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>electro house, brostep, bass house, fidget house</td>\n",
       "      <td>259.995011</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>126.005254</td>\n",
       "      <td>G minor</td>\n",
       "      <td>0.437424</td>\n",
       "      <td>6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fly Kicks - Wax Motif Remix</td>\n",
       "      <td>AC Slater/Chris Lorenzo/Wax Motif</td>\n",
       "      <td>Bass House</td>\n",
       "      <td>..\\data\\audio_files\\processed\\2.mp3</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.692</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-4.985</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607</td>\n",
       "      <td>125.023</td>\n",
       "      <td>4.0</td>\n",
       "      <td>bass house, house, tech house, electro house, ...</td>\n",
       "      <td>276.425034</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>125.006785</td>\n",
       "      <td>F# minor</td>\n",
       "      <td>0.573062</td>\n",
       "      <td>11A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Take Me Away</td>\n",
       "      <td>ACRAZE</td>\n",
       "      <td>Pop Dance</td>\n",
       "      <td>..\\data\\audio_files\\processed\\3.mp3</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.982</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-4.011</td>\n",
       "      <td>0.0782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719</td>\n",
       "      <td>126.036</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pop dance, tech house</td>\n",
       "      <td>179.529025</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>125.997223</td>\n",
       "      <td>E minor</td>\n",
       "      <td>0.333447</td>\n",
       "      <td>9A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Heard It Like This</td>\n",
       "      <td>ACRAZE/Joey Valence &amp; Brae</td>\n",
       "      <td>Pop Dance</td>\n",
       "      <td>..\\data\\audio_files\\processed\\4.mp3</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.906</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730</td>\n",
       "      <td>125.943</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pop dance, tech house</td>\n",
       "      <td>219.281043</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>124.712860</td>\n",
       "      <td>G minor</td>\n",
       "      <td>0.417818</td>\n",
       "      <td>6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Spring Girl - Vintage Culture Remix</td>\n",
       "      <td>Adam Ten/Maori/Vintage Culture</td>\n",
       "      <td>Israeli Techno</td>\n",
       "      <td>..\\data\\audio_files\\processed\\5.mp3</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.865</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-5.296</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643</td>\n",
       "      <td>127.002</td>\n",
       "      <td>4.0</td>\n",
       "      <td>israeli techno, brazilian edm</td>\n",
       "      <td>227.216009</td>\n",
       "      <td>129.199219</td>\n",
       "      <td>127.540242</td>\n",
       "      <td>F minor</td>\n",
       "      <td>0.472424</td>\n",
       "      <td>4A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>550</td>\n",
       "      <td>Clarity</td>\n",
       "      <td>Zedd/Foxes</td>\n",
       "      <td>Complextro</td>\n",
       "      <td>..\\data\\audio_files\\processed\\550.mp3</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.781</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.480</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>128.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>edm, complextro, pop dance, uk pop, electropop...</td>\n",
       "      <td>266.043039</td>\n",
       "      <td>129.199219</td>\n",
       "      <td>128.228314</td>\n",
       "      <td>E major</td>\n",
       "      <td>0.829250</td>\n",
       "      <td>12B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>551</td>\n",
       "      <td>One Three Nine</td>\n",
       "      <td>Zeds Dead/Scrufizzer</td>\n",
       "      <td>Brostep</td>\n",
       "      <td>..\\data\\audio_files\\processed\\551.mp3</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.864</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.086</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768</td>\n",
       "      <td>124.992</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dubstep, edm, grime, progressive electro house...</td>\n",
       "      <td>165.090023</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>124.995820</td>\n",
       "      <td>F# minor</td>\n",
       "      <td>0.605562</td>\n",
       "      <td>11A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>552</td>\n",
       "      <td>Better Recognize</td>\n",
       "      <td>ZHU/Wax Motif</td>\n",
       "      <td>Edm</td>\n",
       "      <td>..\\data\\audio_files\\processed\\552.mp3</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.217</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491</td>\n",
       "      <td>126.018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>electro house, bass house, tech house, edm</td>\n",
       "      <td>209.656009</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>125.257548</td>\n",
       "      <td>F minor</td>\n",
       "      <td>0.678667</td>\n",
       "      <td>4A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>553</td>\n",
       "      <td>Tunnel Vision - Don Diablo Edit</td>\n",
       "      <td>Zonderling/Don Diablo</td>\n",
       "      <td>Dutch House</td>\n",
       "      <td>..\\data\\audio_files\\processed\\553.mp3</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.913</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.101</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365</td>\n",
       "      <td>126.001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dutch house, edm, future house, pop dance, sky...</td>\n",
       "      <td>213.875011</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>125.829299</td>\n",
       "      <td>G minor</td>\n",
       "      <td>0.727484</td>\n",
       "      <td>6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>554</td>\n",
       "      <td>I DON'T CARE</td>\n",
       "      <td>박혜진 Park Hye Jin</td>\n",
       "      <td>Korean Dream Pop</td>\n",
       "      <td>..\\data\\audio_files\\processed\\554.mp3</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.521</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-10.470</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244</td>\n",
       "      <td>124.013</td>\n",
       "      <td>4.0</td>\n",
       "      <td>korean dream pop</td>\n",
       "      <td>330.847029</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>123.533967</td>\n",
       "      <td>F# minor</td>\n",
       "      <td>0.524732</td>\n",
       "      <td>11A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>554 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SongID                            TrackName  \\\n",
       "0         1                          Bass Inside   \n",
       "1         2          Fly Kicks - Wax Motif Remix   \n",
       "2         3                         Take Me Away   \n",
       "3         4                   Heard It Like This   \n",
       "4         5  Spring Girl - Vintage Culture Remix   \n",
       "..      ...                                  ...   \n",
       "549     550                              Clarity   \n",
       "550     551                       One Three Nine   \n",
       "551     552                     Better Recognize   \n",
       "552     553      Tunnel Vision - Don Diablo Edit   \n",
       "553     554                         I DON'T CARE   \n",
       "\n",
       "                               Artists             Genre  \\\n",
       "0                            AC Slater        Bass House   \n",
       "1    AC Slater/Chris Lorenzo/Wax Motif        Bass House   \n",
       "2                               ACRAZE         Pop Dance   \n",
       "3           ACRAZE/Joey Valence & Brae         Pop Dance   \n",
       "4       Adam Ten/Maori/Vintage Culture    Israeli Techno   \n",
       "..                                 ...               ...   \n",
       "549                         Zedd/Foxes        Complextro   \n",
       "550               Zeds Dead/Scrufizzer           Brostep   \n",
       "551                      ZHU/Wax Motif               Edm   \n",
       "552              Zonderling/Don Diablo       Dutch House   \n",
       "553                   박혜진 Park Hye Jin  Korean Dream Pop   \n",
       "\n",
       "                                  FilePath  sp_danceability  sp_energy  \\\n",
       "0      ..\\data\\audio_files\\processed\\1.mp3            0.905      0.838   \n",
       "1      ..\\data\\audio_files\\processed\\2.mp3            0.897      0.692   \n",
       "2      ..\\data\\audio_files\\processed\\3.mp3            0.727      0.982   \n",
       "3      ..\\data\\audio_files\\processed\\4.mp3            0.747      0.901   \n",
       "4      ..\\data\\audio_files\\processed\\5.mp3            0.798      0.865   \n",
       "..                                     ...              ...        ...   \n",
       "549  ..\\data\\audio_files\\processed\\550.mp3            0.509      0.781   \n",
       "550  ..\\data\\audio_files\\processed\\551.mp3            0.810      0.864   \n",
       "551  ..\\data\\audio_files\\processed\\552.mp3            0.839      0.786   \n",
       "552  ..\\data\\audio_files\\processed\\553.mp3            0.762      0.913   \n",
       "553  ..\\data\\audio_files\\processed\\554.mp3            0.886      0.521   \n",
       "\n",
       "     sp_key  sp_loudness  sp_speechiness  ...  sp_valence  sp_tempo  \\\n",
       "0       6.0       -6.838          0.0499  ...       0.464   126.007   \n",
       "1      11.0       -4.985          0.0492  ...       0.607   125.023   \n",
       "2      11.0       -4.011          0.0782  ...       0.719   126.036   \n",
       "3       1.0       -5.906          0.0491  ...       0.730   125.943   \n",
       "4       7.0       -5.296          0.0568  ...       0.643   127.002   \n",
       "..      ...          ...             ...  ...         ...       ...   \n",
       "549     8.0       -3.480          0.0720  ...       0.176   128.000   \n",
       "550    11.0       -5.086          0.0374  ...       0.768   124.992   \n",
       "551     1.0       -6.217          0.0560  ...       0.491   126.018   \n",
       "552     5.0       -4.101          0.1110  ...       0.365   126.001   \n",
       "553     6.0      -10.470          0.1100  ...       0.244   124.013   \n",
       "\n",
       "     sp_time_signature                                           sp_genre  \\\n",
       "0                  4.0   electro house, brostep, bass house, fidget house   \n",
       "1                  4.0  bass house, house, tech house, electro house, ...   \n",
       "2                  4.0                              pop dance, tech house   \n",
       "3                  4.0                              pop dance, tech house   \n",
       "4                  4.0                      israeli techno, brazilian edm   \n",
       "..                 ...                                                ...   \n",
       "549                4.0  edm, complextro, pop dance, uk pop, electropop...   \n",
       "550                4.0  dubstep, edm, grime, progressive electro house...   \n",
       "551                4.0         electro house, bass house, tech house, edm   \n",
       "552                4.0  dutch house, edm, future house, pop dance, sky...   \n",
       "553                4.0                                   korean dream pop   \n",
       "\n",
       "       duration       tempo  studio_bpm       key  key_corr  camelot_key  \n",
       "0    259.995011  123.046875  126.005254   G minor  0.437424           6A  \n",
       "1    276.425034  123.046875  125.006785  F# minor  0.573062          11A  \n",
       "2    179.529025  123.046875  125.997223   E minor  0.333447           9A  \n",
       "3    219.281043  123.046875  124.712860   G minor  0.417818           6A  \n",
       "4    227.216009  129.199219  127.540242   F minor  0.472424           4A  \n",
       "..          ...         ...         ...       ...       ...          ...  \n",
       "549  266.043039  129.199219  128.228314   E major  0.829250          12B  \n",
       "550  165.090023  123.046875  124.995820  F# minor  0.605562          11A  \n",
       "551  209.656009  123.046875  125.257548   F minor  0.678667           4A  \n",
       "552  213.875011  123.046875  125.829299   G minor  0.727484           6A  \n",
       "553  330.847029  123.046875  123.533967  F# minor  0.524732          11A  \n",
       "\n",
       "[554 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a7c5f-4169-4f3f-9c60-c63fea3d4076",
   "metadata": {},
   "source": [
    "## Saving the audio features as pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fad31c-8198-495d-b08f-00e3cfe9ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea98fa6-9544-4f25-944b-4f9831a0bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your directory and constants \n",
    "merged_df = pd.read_csv(r'..\\data\\dataframes\\sp_merged2.csv')\n",
    "\n",
    "mp3_directory = r\"..\\data\\audio_files\\processed\"\n",
    "export_directory = r\"..\\data\\pkl\"\n",
    "hop_length = 512\n",
    "sr = 22050\n",
    "\n",
    "\n",
    "# Process each song\n",
    "for index, row in tqdm(merged_df.iterrows(), desc=\"Processing audio profiles\", total=merged_df.shape[0]):\n",
    "    audio_file_path = row['FilePath']\n",
    "    if os.path.exists(audio_file_path):\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_file_path, sr=sr)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        y_harm, y_perc = librosa.effects.hpss(y)\n",
    "\n",
    "        # Chroma profile\n",
    "        chroma_cq = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n",
    "        key, key_corr = detect_key_from_chromagram(chroma_cq, sr)\n",
    "        camelot = get_camelot(key)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr, chroma=chroma_cq)\n",
    "\n",
    "        # Spectrogram\n",
    "        D = np.abs(librosa.stft(y))**2\n",
    "        S_mel = librosa.feature.melspectrogram(S=D, sr=sr)\n",
    "        S_mel_db = librosa.power_to_db(S_mel, ref=np.max)\n",
    "        # Centroid\n",
    "        centroid_mel = librosa.feature.spectral_centroid(S=S_mel, sr=sr, hop_length=hop_length)\n",
    "        # MFCC\n",
    "        mfccs = librosa.feature.mfcc(S=S_mel_db)\n",
    "        \n",
    "        # Tempo/rhythm profile\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "        studio_bpm, stable_intervals = get_studio_bpm(beats)\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "        tempogram_ratio = librosa.feature.tempogram_ratio(tg=tempogram, sr=sr)\n",
    "        \n",
    "        # Quantize beats and create measure grid\n",
    "        beat_grid, measure_grid = quantize_beats(beats, onset_env, studio_bpm, sr, hop_length, duration)\n",
    "        measure_numbers = np.arange(len(measure_grid))\n",
    "        measure_dict = {measure_number: measure_time for measure_number, measure_time in zip(measure_numbers, measure_grid)}\n",
    "\n",
    "        # Data dictionary to hold features\n",
    "        data = {\n",
    "            'SongID': row['SongID'], \n",
    "            'duration': duration, \n",
    "            'tempo': tempo, \n",
    "            'studio_bpm': studio_bpm,\n",
    "            'key': key,\n",
    "            'key_corr': key_corr,\n",
    "            'camelot_key': camelot,\n",
    "            'stable_intervals': stable_intervals.tolist(),\n",
    "            'y': y.tolist(),\n",
    "            'chroma_cq': chroma_cq.tolist(),\n",
    "            'tonnetz': tonnetz.tolist(),\n",
    "            'S_mel_db': S_mel_db.tolist(),\n",
    "            'centroid_mel': centroid_mel.tolist(),\n",
    "            'mfccs': mfccs.tolist(),\n",
    "            'tempogram': tempogram.tolist(),\n",
    "            'tempogram_ratio': tempogram_ratio.tolist(),\n",
    "            'MeasureDict': measure_dict\n",
    "        }\n",
    "\n",
    "        # Save to pickle file\n",
    "        pickle_file_path = os.path.join(export_directory, f\"{row['SongID']}.pkl\")  # Ensure proper path joining\n",
    "        pd.to_pickle(data, pickle_file_path)\n",
    "    else:\n",
    "        print(f\"File not found: {audio_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc4217-5c12-465d-8785-2d83c9fbbf12",
   "metadata": {},
   "source": [
    "# Making visual plots for all songs using pkl audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0a7e0d-eebc-4912-b898-1a5020a530ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_measure_grid(ax, measure_grid, measure_numbers):\n",
    "    \"\"\"\n",
    "    This function takes an axis object and applies measure grid lines,\n",
    "    sets x-ticks to measure start times for every fourth measure starting from measure 0,\n",
    "    labels them with measure numbers, and applies sub-ticks for intermediate measure times.\n",
    "    \n",
    "    Parameters:\n",
    "    ax (matplotlib.axes.Axes): The axis object to modify.\n",
    "    measure_grid (np.array): The array of measure start times in seconds.\n",
    "    measure_numbers (np.array): The array of measure numbers corresponding to the start times.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the major (every fourth) measures\n",
    "    major_indices = measure_numbers % 4 == 0\n",
    "\n",
    "    # Set major x-axis ticks and labels (for measure 0 and every fourth measure after)\n",
    "    ax.set_xticks(measure_grid[major_indices])\n",
    "    ax.set_xticklabels(measure_numbers[major_indices])\n",
    "\n",
    "    # Set minor x-axis ticks (for intermediate measures)\n",
    "    ax.set_xticks(measure_grid[~major_indices], minor=True)\n",
    "\n",
    "    # Overlay the major measure grid lines on the plot (for measure 0 and every fourth measure after)\n",
    "    ax.vlines(measure_grid[major_indices], ax.get_ylim()[0], ax.get_ylim()[1], color='green', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Overlay the minor measure grid lines on the plot (for intermediate measures)\n",
    "    ax.vlines(measure_grid[~major_indices], ax.get_ylim()[0], ax.get_ylim()[1], color='grey', linestyle=':', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Measure Number')\n",
    "\n",
    "\n",
    "def load_pickle_data(pkl_path):\n",
    "    with open(pkl_path, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7dba17-d90c-4661-bbe8-c489ed1ce250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio profiles: 100%|██████████| 554/554 [03:47<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "merged_df = pd.read_csv(r'../data/dataframes/sp_merged2.csv')\n",
    "merged_df = merged_df['SongID']\n",
    "export_directory = r\"../figures/audio_plots\"\n",
    "pkl_directory = r\"../data/pkl\"\n",
    "sr = 22050  \n",
    "hop_length = 512\n",
    "\n",
    "def apply_measure_grid(ax, measure_grid, measure_numbers):\n",
    "    # Filter the major (every fourth) measures\n",
    "    major_indices = measure_numbers % 4 == 0\n",
    "\n",
    "    # Set major x-axis ticks and labels (for measure 0 and every fourth measure after)\n",
    "    ax.set_xticks(measure_grid[major_indices])\n",
    "    ax.set_xticklabels(measure_numbers[major_indices])\n",
    "\n",
    "    # Set minor x-axis ticks (for intermediate measures)\n",
    "    ax.set_xticks(measure_grid[~major_indices], minor=True)\n",
    "\n",
    "    # Overlay the major measure grid lines on the plot (for measure 0 and every fourth measure after)\n",
    "    ax.vlines(measure_grid[major_indices], ax.get_ylim()[0], ax.get_ylim()[1], color='green', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Overlay the minor measure grid lines on the plot (for intermediate measures)\n",
    "    ax.vlines(measure_grid[~major_indices], ax.get_ylim()[0], ax.get_ylim()[1], color='grey', linestyle=':', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Measure Number')\n",
    "\n",
    "    \n",
    "def load_pickle_data(pkl_path):\n",
    "    with open(pkl_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "        \n",
    "for song_id in tqdm(merged_df, desc=\"Processing audio profiles\"):\n",
    "    export_fig_path = os.path.join(export_directory, f\"{song_id}.png\")\n",
    "    pkl_path = os.path.join(pkl_directory, f\"{song_id}.pkl\")\n",
    "\n",
    "    if not os.path.exists(export_fig_path):\n",
    "        if os.path.exists(pkl_path):\n",
    "            try:\n",
    "                data = load_pickle_data(pkl_path)\n",
    "\n",
    "                # Extracting the individual components from the data dictionary\n",
    "                y_harm = np.asarray(data['y_harm'])\n",
    "                y_perc = np.asarray(data['y_perc'])\n",
    "                S_mel_db = np.asarray(data['S_mel_db'])\n",
    "                tempogram = np.asarray(data['tempogram'])\n",
    "                tempogram_ratio = np.asarray(data['tempogram_ratio'])\n",
    "                chroma_cq = np.asarray(data['chroma_cq'])\n",
    "                tonnetz = np.asarray(data['tonnetz'])\n",
    "                duration = data['duration']\n",
    "                measure_grid = np.array(list(data['MeasureDict'].values()))\n",
    "                measure_numbers = np.array(list(data['MeasureDict'].keys()))\n",
    "        \n",
    "                # Create subplots\n",
    "                fig, axs = plt.subplots(6, 1, figsize=(20, 30), dpi=125)\n",
    "\n",
    "                # Harmonic/Percussive Waveform plot\n",
    "                axs[0].plot(np.linspace(0, duration, len(y_harm)), y_harm, alpha=0.5, label='Harmonic', color='b')\n",
    "                axs[0].plot(np.linspace(0, duration, len(y_perc)), y_perc, alpha=0.5, label='Percussive', color='r')\n",
    "                apply_measure_grid(axs[0], measure_grid, measure_numbers)\n",
    "                axs[0].set_title('Harmonic and Percussive Waveform')\n",
    "                axs[0].set_xlim([0, duration])\n",
    "        \n",
    "                # Mel Spectrogram plot\n",
    "                librosa.display.specshow(S_mel_db, sr=sr, x_axis='time', y_axis='mel', ax=axs[1], fmax=8000)\n",
    "                apply_measure_grid(axs[1], measure_grid, measure_numbers)\n",
    "                axs[1].set_title('Mel Spectrogram')\n",
    "                # Set the y-axis limits\n",
    "                axs[1].set_ylim(0, 8000)  # Assuming the fmax is 8000 Hz as specified in the specshow call\n",
    "        \n",
    "                # Tempogram plot\n",
    "                librosa.display.specshow(tempogram, sr=sr, hop_length=512, x_axis='time', y_axis='tempo', cmap='magma', ax=axs[2])\n",
    "                apply_measure_grid(axs[2], measure_grid, measure_numbers)\n",
    "                axs[2].set_title('Tempogram')\n",
    "        \n",
    "                # Tempogram ratio\n",
    "                # Define note labels for tempogram ratio\n",
    "                note_labels = [\n",
    "                    'Sixteenth note',\n",
    "                    'Dotted sixteenth',\n",
    "                    'Eighth triplet',\n",
    "                    'Eighth note',\n",
    "                    'Dotted eighth',\n",
    "                    'Quarter triplet',\n",
    "                    'Quarter note',\n",
    "                    'Dotted quarter',\n",
    "                    'Half triplet',\n",
    "                    'Half note',\n",
    "                    'Dotted half note',\n",
    "                    'Whole triplet',\n",
    "                    'Whole note'\n",
    "                ]\n",
    "                \n",
    "                librosa.display.specshow(tempogram_ratio, x_axis='time', ax=axs[3], sr=sr)\n",
    "                axs[3].set_xlim([0, duration])\n",
    "                apply_measure_grid(axs[3], measure_grid, measure_numbers)\n",
    "                axs[3].set_yticks(range(len(note_labels)))\n",
    "                axs[3].set_yticklabels(note_labels)\n",
    "                axs[3].set_title('Tempogram Ratio')\n",
    "        \n",
    "                # Chroma CQT plot\n",
    "                librosa.display.specshow(chroma_cq, y_axis='chroma', x_axis='time', ax=axs[4])\n",
    "                apply_measure_grid(axs[4], measure_grid, measure_numbers)\n",
    "                axs[4].set_title('Chroma CQT')\n",
    "        \n",
    "                # Tonnetz plot\n",
    "                librosa.display.specshow(tonnetz, sr=sr, hop_length=hop_length, y_axis='tonnetz', x_axis='time', ax=axs[5])\n",
    "                apply_measure_grid(axs[5], measure_grid, measure_numbers)\n",
    "                axs[5].set_title('Tonnetz')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(export_fig_path)\n",
    "                plt.close(fig)\n",
    "                del y_harm, y_perc, S_mel_db, tempogram, tempogram_ratio, chroma_cq, tonnetz, duration, measure_grid, measure_numbers\n",
    "                gc.collect()\n",
    "\n",
    "            except (EOFError, pickle.UnpicklingError) as e:\n",
    "                print(f\"Failed to load data for {song_id} due to {e} Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e490315-51aa-42e3-9e41-ffdff2d24671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import pickle\n",
    "\n",
    "# Define paths\n",
    "export_directory = r\"../figures/audio_plots\"\n",
    "pkl_directory = r\"../data/pkl\"\n",
    "sr = 22050\n",
    "hop_length = 512\n",
    "\n",
    "# Specific song_id to test\n",
    "song_id = '1'  # Replace '1' with the actual ID if it's different\n",
    "export_fig_path = os.path.join(export_directory, f\"{song_id}.png\")\n",
    "pkl_path = os.path.join(pkl_directory, f\"{song_id}.pkl\")\n",
    "\n",
    "# Check if the pickle file exists\n",
    "if os.path.exists(pkl_path):\n",
    "    with open(pkl_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    # Unpack data\n",
    "    y_harm = np.asarray(data['y_harm'])\n",
    "    y_perc =  np.asarray(data['y_perc'])\n",
    "    S_mel_db =  np.asarray(data['S_mel_db'])\n",
    "    tempogram =  np.asarray(data['tempogram'])\n",
    "    tempogram_ratio =  np.asarray(data['tempogram_ratio'])\n",
    "    chroma_cq =  np.asarray(data['chroma_cq'])\n",
    "    tonnetz =  np.asarray(data['tonnetz'])\n",
    "    duration =  data['duration']\n",
    "    measure_grid = np.array(list(data['MeasureDict'].values()))\n",
    "    measure_numbers = np.array(list(data['MeasureDict'].keys()))\n",
    "    \n",
    "    # Define the figure and axes\n",
    "    fig, axs = plt.subplots(6, 1, figsize=(20, 30), dpi=300)\n",
    "\n",
    "    # Harmonic/Percussive Waveform plot\n",
    "    axs[0].plot(np.linspace(0, duration, len(y_harm)), y_harm, alpha=0.5, label='Harmonic', color='b')\n",
    "    axs[0].plot(np.linspace(0, duration, len(y_perc)), y_perc, alpha=0.5, label='Percussive', color='r')\n",
    "    axs[0].set_xlim([0, duration])\n",
    "    apply_measure_grid(axs[0], measure_grid, measure_numbers)\n",
    "    axs[0].set_title('Harmonic and Percussive Waveform')\n",
    "\n",
    "    # Mel Spectrogram plot\n",
    "    librosa.display.specshow(S_mel_db, sr=sr, x_axis='time', y_axis='mel', ax=axs[1], fmax=8000)\n",
    "    apply_measure_grid(axs[1], measure_grid, measure_numbers)\n",
    "    axs[1].set_title('Mel Spectrogram')\n",
    "    # Set the y-axis limits\n",
    "    axs[1].set_ylim(0, 8000)  # Assuming the fmax is 8000 Hz as specified in the specshow call\n",
    "\n",
    "    # Tempogram plot\n",
    "    librosa.display.specshow(tempogram, sr=sr, hop_length=512, x_axis='time', y_axis='tempo', cmap='magma', ax=axs[2])\n",
    "    apply_measure_grid(axs[2], measure_grid, measure_numbers)\n",
    "    axs[2].set_title('Tempogram')\n",
    "\n",
    "    # Tempogram ratio\n",
    "    # Define note labels for tempogram ratio\n",
    "    note_labels = [\n",
    "        'Sixteenth note',\n",
    "        'Dotted sixteenth',\n",
    "        'Eighth triplet',\n",
    "        'Eighth note',\n",
    "        'Dotted eighth',\n",
    "        'Quarter triplet',\n",
    "        'Quarter note',\n",
    "        'Dotted quarter',\n",
    "        'Half triplet',\n",
    "        'Half note',\n",
    "        'Dotted half note',\n",
    "        'Whole triplet',\n",
    "        'Whole note'\n",
    "    ]\n",
    "    librosa.display.specshow(tempogram_ratio, x_axis='time', ax=axs[3], sr=sr)\n",
    "    axs[3].set_xlim([0, duration])\n",
    "    apply_measure_grid(axs[3], measure_grid, measure_numbers)\n",
    "    axs[3].set_yticks(range(len(note_labels)))\n",
    "    axs[3].set_yticklabels(note_labels)\n",
    "    axs[3].set_title('Tempogram Ratio')\n",
    "\n",
    "    # Chroma CQT plot\n",
    "    librosa.display.specshow(chroma_cq, y_axis='chroma', x_axis='time', ax=axs[4])\n",
    "    apply_measure_grid(axs[4], measure_grid, measure_numbers)\n",
    "    axs[4].set_title('Chroma CQT')\n",
    "\n",
    "    # Tonnetz plot\n",
    "    librosa.display.specshow(tonnetz, sr=sr, hop_length=hop_length, y_axis='tonnetz', x_axis='time', ax=axs[5])\n",
    "    apply_measure_grid(axs[5], measure_grid, measure_numbers)\n",
    "    axs[5].set_title('Tonnetz')\n",
    "\n",
    "    # Layout and saving\n",
    "    plt.tight_layout()\n",
    "    #plt.show()  # Use plt.show() for testing instead of saving the figure\n",
    "    plt.savefig(export_fig_path)  # Uncomment this to save the figure\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(f\"The file {pkl_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c4b6b97-89ff-4542-8742-dbbef01f84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio profiles:  19%|█▉        | 107/554 [36:45<2:33:34, 20.61s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 56.7 MiB for an array with shape (1025, 14499) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_file_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22050\u001b[39m)\n\u001b[0;32m     21\u001b[0m duration \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mget_duration(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m---> 22\u001b[0m y_harm, y_perc \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mhpss(y)\n\u001b[0;32m     23\u001b[0m onset_env \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39monset\u001b[38;5;241m.\u001b[39monset_strength(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     24\u001b[0m tempo, beats \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mbeat\u001b[38;5;241m.\u001b[39mbeat_track(onset_envelope\u001b[38;5;241m=\u001b[39monset_env, sr\u001b[38;5;241m=\u001b[39msr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AudioInfoRetrieval\\Lib\\site-packages\\librosa\\effects.py:99\u001b[0m, in \u001b[0;36mhpss\u001b[1;34m(y, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m stft \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mstft(y)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Decompose into harmonic and percussives\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m stft_harm, stft_perc \u001b[38;5;241m=\u001b[39m decompose\u001b[38;5;241m.\u001b[39mhpss(stft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Invert the STFTs.  Adjust length to match the input.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m y_harm \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mistft(stft_harm, dtype\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mdtype, length\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AudioInfoRetrieval\\Lib\\site-packages\\librosa\\decompose.py:357\u001b[0m, in \u001b[0;36mhpss\u001b[1;34m(S, kernel_size, power, mask, margin)\u001b[0m\n\u001b[0;32m    354\u001b[0m phase: Union[\u001b[38;5;28mfloat\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39miscomplexobj(S):\n\u001b[1;32m--> 357\u001b[0m     S, phase \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mmagphase(S)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     phase \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AudioInfoRetrieval\\Lib\\site-packages\\librosa\\core\\spectrum.py:1344\u001b[0m, in \u001b[0;36mmagphase\u001b[1;34m(D, power)\u001b[0m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;66;03m# Compute real and imaginary separately, because complex division can\u001b[39;00m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# produce NaNs when denormalized numbers are involved (< ~2e-39 for\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;66;03m# complex64, ~5e-309 for complex128)\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m phase \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(D, dtype\u001b[38;5;241m=\u001b[39mutil\u001b[38;5;241m.\u001b[39mdtype_r2c(D\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m-> 1344\u001b[0m phase\u001b[38;5;241m.\u001b[39mreal \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39mreal \u001b[38;5;241m/\u001b[39m mag_nonzero \u001b[38;5;241m+\u001b[39m zeros_to_ones\n\u001b[0;32m   1345\u001b[0m phase\u001b[38;5;241m.\u001b[39mimag \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39mimag \u001b[38;5;241m/\u001b[39m mag_nonzero\n\u001b[0;32m   1347\u001b[0m mag \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m power\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 56.7 MiB for an array with shape (1025, 14499) and data type float32"
     ]
    }
   ],
   "source": [
    "# Define your directory and constants \n",
    "merged_df = pd.read_csv(r'../data/dataframes/sp_merged2.csv')\n",
    "\n",
    "mp3_directory = r\"..\\data\\audio_files\\processed\"\n",
    "export_directory = r\"..\\figures\\audio_plots\"\n",
    "pkl_directory = r\"..\\data\\pkl\"\n",
    "hop_length = 512\n",
    "sr = 22050\n",
    "\n",
    "for index, row in tqdm(merged_df.iterrows(), desc=\"Processing audio profiles\", total=merged_df.shape[0]):\n",
    "    \n",
    "    song_id = row['SongID']\n",
    "    export_fig_path = os.path.join(export_directory, f\"{song_id}.png\")\n",
    "    pkl_path = os.path.join(pkl_directory, f\"{song_id}.pkl\")\n",
    "    audio_file_path = row['FilePath']\n",
    "    \n",
    "    if os.path.exists(audio_file_path) & os.path.exists(pkl_path):\n",
    "        #df = pd.read_pickle(pkl_path)\n",
    "        \n",
    "        y, sr = librosa.load(audio_file_path, sr=22050)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        y_harm, y_perc = librosa.effects.hpss(y)\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "        #tempo = math.floor(tempo)\n",
    "    \n",
    "        # Quantize beats and create measure grid\n",
    "        beat_grid, measure_grid = quantize_beats(beats, onset_env, tempo, sr, hop_length, duration)\n",
    "        measure_numbers = np.arange(len(measure_grid))\n",
    "        measure_dict = {measure_number: measure_time for measure_number, measure_time in zip(measure_numbers, measure_grid)}\n",
    "    \n",
    "        # Start a new figure\n",
    "        fig = plt.figure(figsize=(20, 24), dpi=300)  # Adjust the size as needed\n",
    "    \n",
    "        # Create a GridSpec for the entire figure\n",
    "        gs = gridspec.GridSpec(3, 2, figure=fig)\n",
    "    \n",
    "        # Harmonic/Percussive Waveform plot \n",
    "        ax_waveform = plt.subplot(gs[0, 0])  \n",
    "        librosa.display.waveshow(y_harm, sr=sr, alpha=0.5, ax=ax_waveform, label='Harmonic', color = 'b')\n",
    "        librosa.display.waveshow(y_perc, sr=sr, alpha=0.5, ax=ax_waveform, label='Percussive', color='r')\n",
    "        apply_measure_grid(ax_waveform, measure_grid, measure_numbers)\n",
    "        ax_waveform.set_title('Harmonic and Percussive Waveform')\n",
    "        ax_waveform.set_ylabel('Amplitude')\n",
    "        ax_waveform.set_xlim([0, duration])   \n",
    "        ax_waveform.legend()\n",
    "\n",
    "        # MelSpectrogram plot\n",
    "        ax_melspec = plt.subplot(gs[0, 1])\n",
    "        D = np.abs(librosa.stft(y))**2\n",
    "        S = librosa.feature.melspectrogram(S=D, sr=sr)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "        librosa.display.specshow(S_dB, x_axis='time',\n",
    "                                 y_axis='mel', sr=sr,\n",
    "                                 fmax=8000, ax=ax_melspec)\n",
    "        apply_measure_grid(ax_melspec, measure_grid, measure_numbers)\n",
    "        ax_melspec.set_title('MelSpectrogram')\n",
    "    \n",
    "        # Tempogram plot\n",
    "        ax_tempogram = plt.subplot(gs[1, 0])\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "        librosa.display.specshow(tempogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='tempo', ax=ax_tempogram)\n",
    "        ax_tempogram.set_yticks([tempo])\n",
    "        ax_tempogram.set_yticklabels([str(tempo) + ' BPM'])\n",
    "        apply_measure_grid(ax_tempogram, measure_grid, measure_numbers)\n",
    "        ax_tempogram.set(title='Tempogram')\n",
    "    \n",
    "        # Tempogram ratio\n",
    "        ax_tgr = plt.subplot(gs[1, 1])\n",
    "        tgr = librosa.feature.tempogram_ratio(tg=tempogram, sr=sr)\n",
    "        # Define note labels\n",
    "        note_labels = [\n",
    "            'Sixteenth note',\n",
    "            'Dotted sixteenth',\n",
    "            'Eighth triplet',\n",
    "            'Eighth note',\n",
    "            'Dotted eighth',\n",
    "            'Quarter triplet',\n",
    "            'Quarter note',\n",
    "            'Dotted quarter',\n",
    "            'Half triplet',\n",
    "            'Half note',\n",
    "            'Dotted half note',\n",
    "            'Whole triplet',\n",
    "            'Whole note'\n",
    "        ]\n",
    "        librosa.display.specshow(tgr, x_axis='time', ax=ax_tgr, sr=sr)\n",
    "        ax_tgr.set_xlim([0, duration])\n",
    "        apply_measure_grid(ax_tgr, measure_grid, measure_numbers)\n",
    "        ax_tgr.set(title=\"Tempogram ratio\")\n",
    "        ax_tgr.set_yticks(range(len(note_labels)))\n",
    "        ax_tgr.set_yticklabels(note_labels)\n",
    "    \n",
    "        # Chromagram plot\n",
    "        ax_chromagram = plt.subplot(gs[2, 0])\n",
    "        chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "        librosa.display.specshow(chroma_cq, sr=sr, hop_length=hop_length, y_axis='chroma', x_axis='time', ax=ax_chromagram)\n",
    "        apply_measure_grid(ax_chromagram, measure_grid, measure_numbers)\n",
    "        ax_chromagram.set_ylabel('Pitch Class')\n",
    "        ax_chromagram.set_title('Chromagram')\n",
    "        \n",
    "        # Tonnetz Features plot\n",
    "        ax_tonnetz = plt.subplot(gs[2, 1])\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr, chroma=chroma_cq)\n",
    "        librosa.display.specshow(tonnetz, sr=sr, hop_length=hop_length, y_axis='tonnetz', x_axis='time', ax=ax_tonnetz)\n",
    "        apply_measure_grid(ax_tonnetz, measure_grid, measure_numbers)\n",
    "        ax_tonnetz.set_title('Tonnetz features')\n",
    "        \n",
    "       \n",
    "        # Save the figure to a PNG file with the same base name as the MP3 file\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(export_fig_path)\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "        del y, sr  # Delete large variables that are no longer needed\n",
    "        gc.collect()  # Call garbage collector manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75098bc4-336a-4efb-91a2-7c6ea870c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(r\"..\\data\\pkl\\1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2529d37b-bf46-4025-a4b7-535484f615fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6b7c9d-5fb2-4d4b-ac42-3c772e930245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\audio_files\\\\processed\\\\554.mp3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['FilePath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3666c64-ca97-473a-9485-bbed5e6ed611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

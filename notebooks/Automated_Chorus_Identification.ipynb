{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b024bf7-9c0e-4df8-8774-3fb52ad361ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import gzip\n",
    "\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4a60d-7c0d-462b-80de-2c3f29414a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_key_invariant(chromagram):\n",
    "    \"\"\"Returns a key-invariant chromagram.\"\"\"\n",
    "    maj_profile = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])\n",
    "    min_profile = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])\n",
    "    avg_chroma = np.mean(chromagram, axis=1)\n",
    "    maj_corrs = np.correlate(avg_chroma, maj_profile, mode='same')\n",
    "    min_corrs = np.correlate(avg_chroma, min_profile, mode='same')\n",
    "    key_shift = np.argmax(np.concatenate((maj_corrs, min_corrs))) % 12\n",
    "    return np.roll(chromagram, -key_shift, axis=0)\n",
    "\n",
    "def segment_by_beats(feature_array, beat_frames):\n",
    "    \"\"\"Segments a 2D array of audio features by beat frames.\"\"\"\n",
    "    beat_frames = np.append(beat_frames, feature_array.shape[1])\n",
    "    return [feature_array[:, beat_frames[i]:beat_frames[i + 1]] for i in range(len(beat_frames) - 1)]\n",
    "\n",
    "def map_labels_to_beats(df_labels, beat_times):\n",
    "    \"\"\"Map chorus labels to beat-synced data.\"\"\"\n",
    "    labels = np.zeros(len(beat_times) - 1)\n",
    "    for _, row in df_labels.iterrows():\n",
    "        indices = np.where((beat_times[:-1] >= row['start_time']) & (beat_times[:-1] < row['end_time']))[0]\n",
    "        labels[indices] = 1 if row['label'] == 'chorus' else 0\n",
    "    return labels\n",
    "\n",
    "def save_compressed_pickle(file_path, data):\n",
    "    \"\"\"Save data to a compressed pickle file.\"\"\"\n",
    "    with gzip.open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62ff02-f3b5-4e81-8bb5-5a87e02f3906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure required directories exist\n",
    "os.makedirs(\"../data/pkl/segments\", exist_ok=True)\n",
    "os.makedirs(\"../data/pkl/labels\", exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "df = pd.read_csv('../data/dataframes/clean_labeled.csv')\n",
    "TARGET_SR = 12000 # Target sample rate chosen to be 1/4 of the original 48kHz.\n",
    "HOP_LENGTH = 128  # Hop length for short-time Fourier transform. Hop length of 128 at 12kHz gives a similar frame rate to a hop length of 512 at 48kHz.\n",
    "\n",
    "# Process each song in the dataset\n",
    "for song_id in tqdm(df['SongID'].unique(), desc=\"Processing...\"):\n",
    "    # Load the audio file\n",
    "    audio_path = f'../data/audio_files/processed/{song_id}.mp3'\n",
    "    y, _ = librosa.load(audio_path, sr=TARGET_SR)\n",
    "    \n",
    "    # Harmonic-percussive source separation\n",
    "    y_harm, y_perc = librosa.effects.hpss(y)\n",
    "    \n",
    "    # Compute onset envelope from the percussive component\n",
    "    onset_env = librosa.onset.onset_strength(y=y_perc, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "\n",
    "    # Beat tracking\n",
    "    _, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "    beat_times = librosa.frames_to_time(beats, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "    \n",
    "    # Map labels to beats\n",
    "    df_labels = df[df['SongID'] == song_id][['start_time', 'end_time', 'label']]\n",
    "    labels = map_labels_to_beats(df_labels, beat_times)\n",
    "    \n",
    "    # Compute RMS energy from spectrogram to give a more accurate representation of energy over time because its frames can be windowed\n",
    "    S = np.abs(librosa.stft(y, hop_length=HOP_LENGTH))\n",
    "    rms = librosa.feature.rms(S=S)\n",
    "    \n",
    "    # Compute Mel Spectrogram and decompose into 4 components (4 chosen from EDA)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=TARGET_SR, n_mels=128, hop_length=HOP_LENGTH)\n",
    "    mel_acts = librosa.decompose.decompose(mel, n_components=4, sort=True)[1]\n",
    "    \n",
    "    # Compute chromagram, make it key invariant, and decompose \n",
    "    chromagram = librosa.feature.chroma_cqt(y=y_harm, sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "    chroma_ki = make_key_invariant(chromagram)\n",
    "    chroma_acts = librosa.decompose.decompose(chroma_ki, n_components=3, sort=True)[1]\n",
    "    \n",
    "    # Compute tempogram, ensure non-negative b/c tempograms are finicky, and decompose \n",
    "    tempogram = np.clip(librosa.feature.tempogram(onset_envelope=onset_env, sr=TARGET_SR, hop_length=HOP_LENGTH), 0, None)\n",
    "    tempogram_acts = librosa.decompose.decompose(tempogram, n_components=3, sort=True)[1]\n",
    "    \n",
    "    # Compute MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=TARGET_SR, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "    \n",
    "    # Standardize features, stack, and segment by beats\n",
    "    features = [rms, mel_acts, chroma_ki, chroma_acts, tempogram_acts, mfccs]\n",
    "    total_inv_dim = sum(1.0 / dim for dim in dims.values()) # Calculate the total sum of inverse dimensions to normalize weights\n",
    "    weights = {feature: (1.0 / dims[feature]) / total_inv_dim for feature in dims} # Normalize weights so each feature weighs the same despite dimensionality\n",
    "    # Apply StandardScaler and weights to each feature\n",
    "    standardized_weighted_features = [StandardScaler().fit_transform(feature.T).T * weights[feature_name]\n",
    "                                      for feature, feature_name in zip(features, dims)]\n",
    "    concat_features = np.vstack(standardized_weighted_features)\n",
    "    segments = segment_by_beats(concat_features, beats)\n",
    "\n",
    "    # Save results with compression\n",
    "    save_compressed_pickle(f\"../data/pkl/segments/{song_id}_beats.pkl.gz\", segments)\n",
    "    save_compressed_pickle(f\"../data/pkl/labels/{song_id}_labels.pkl.gz\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35801f49-b806-48f5-a930-82a28e0046e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Start here after processing data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12048a72-9d59-4888-930d-dc48bc31cee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Third-party imports for numerical operations and machine learning\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep learning frameworks\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6423455-eaf5-4d31-9672-9405d278ab70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = '../data/pkl'\n",
    "# Load the data\n",
    "with gzip.open(os.path.join(save_path, 'beat_data.pkl.gz'), 'rb') as f:\n",
    "    X_dict, y_dict = pickle.load(f)\n",
    "\n",
    "song_data = list(X_dict.values())\n",
    "song_labels = list(y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf90362-21c7-4dd7-83d5-8d5eb0540692",
   "metadata": {},
   "source": [
    "## Data Structure Overview\n",
    "\n",
    "- `song_data`: A list where each element corresponds to an individual song's feature data.\n",
    "- `song_data[0]`: The feature data for the first song in the dataset.\n",
    "- `song_data[i]`: The feature data for the i-th song in the dataset.\n",
    "- `song_data[i][j]`: The j-th beat segment's feature data within the i-th song.\n",
    "- Each `song_data[i][j]` is structured as a 2D array:\n",
    "  - The first dimension has a fixed size of 24, representing the number of features.\n",
    "  - The second dimension has a variable size y, representing the number of frames in the beat segment.\n",
    "- The feature count is consistent across the dataset, with each beat segment containing 24 features.\n",
    "- `song_labels`: A list containing the corresponding labels for each song's beat segments.\n",
    "- `song_labels[i]`: An array of labels for each beat segment within the i-th song.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147aec64-dac6-4e60-a817-0e40a03743b3",
   "metadata": {},
   "source": [
    "### 1) 70/15/15 train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d58e1a-4cc0-4afb-a02a-c036c195780f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    song_data, song_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a3b7b-86fb-4c09-8bc2-d45052e37b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7435-e4be-4299-8f68-af5ffbf47d99",
   "metadata": {},
   "source": [
    "### 2) Generate batches with batch_size = 32\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6054ed-c67b-4353-8c42-342727d1adad",
   "metadata": {},
   "source": [
    "### 1: Find max beat and max frames in a beat\n",
    "- Find maximum number of beats in any song in the batch (`max_beats`).\n",
    "- Find maximum number of frames in any beat segment across all songs in a batch (`max_frames`).\n",
    "\n",
    "### 2: Pad Data using -1\n",
    "- Pad songs to have max beats, pad beats to have max frames, ensuring that newly added beats have max frames.\n",
    "- Pad using -1\n",
    "\n",
    "### 3: Pad Labels using -1\n",
    "- Pad the array of labels for each song to match `max_beats`. Use `-1` as the padding value to denote labels for dummy beats.\n",
    "\n",
    "### Implementation Notes\n",
    "- Padding with `-1` is a common approach when the padded values need to be easily distinguished from valid data. Ensure that your model or subsequent processing steps can handle this special value appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b275ef-32e7-45ed-bee1-0e4766c717b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def song_data_generator_with_pos_encoding(song_data, song_labels, pos_encoding_matrix):\n",
    "    for features, label in zip(song_data, song_labels):\n",
    "        seq_length = features.shape[0]\n",
    "        features += pos_encoding_matrix[:seq_length, :]\n",
    "        yield features, label\n",
    "\n",
    "def positional_encoding(max_len, d_model):\n",
    "    pos_enc = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(d_model):\n",
    "            if i % 2 == 0:\n",
    "                pos_enc[pos, i] = np.sin(pos / np.power(10000, (2 * i) / d_model))\n",
    "            else:\n",
    "                pos_enc[pos, i] = np.cos(pos / np.power(10000, (2 * (i - 1)) / d_model))\n",
    "    return tf.constant(pos_enc, dtype=tf.float32)\n",
    "\n",
    "def make_tf_dataset_with_pos_encoding(song_data, song_labels, pos_encoding_matrix, d_model):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(None, None, d_model), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: song_data_generator_with_pos_encoding(song_data, song_labels, pos_encoding_matrix),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    padded_shapes = ([None, None, d_model], [None])\n",
    "    padding_values = (tf.constant(-1, dtype=tf.float32), tf.constant(-1, dtype=tf.float32))\n",
    "    dataset = dataset.padded_batch(32, padded_shapes=padded_shapes, padding_values=padding_values)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "max_length = max(max(len(song) for song in song_data), max(len(song) for song in song_labels))  \n",
    "d_model = 24  \n",
    "pos_encoding_matrix = positional_encoding(max_length, d_model)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset_tf = make_tf_dataset_with_pos_encoding(X_train, y_train, pos_encoding_matrix, d_model)\n",
    "val_dataset_tf = make_tf_dataset_with_pos_encoding(X_val, y_val, pos_encoding_matrix, d_model)\n",
    "test_dataset_tf = make_tf_dataset_with_pos_encoding(X_test, y_test, pos_encoding_matrix, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff35395-fdbd-4d09-9f7e-15b3113fb817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"Custom binary cross-entropy loss to handle -1 labels, which are used for padding and should be ignored during loss calculation.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "    loss = bce * mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "class MaskedF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='masked_f1_score', **kwargs):\n",
    "        super(MaskedF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Create a mask for all values not equal to -1\n",
    "        mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "\n",
    "        # If sample_weight is provided, combine it with the mask\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            # Ensure sample_weight has the same shape as y_true and y_pred\n",
    "            sample_weight = tf.broadcast_to(sample_weight, tf.shape(y_true))\n",
    "            # Combine the existing mask with the sample_weight\n",
    "            mask *= sample_weight\n",
    "\n",
    "        # Use the resulting mask as the sample_weight in update_state calls\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight=mask)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight=mask)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "def create_model(max_frames, n_features, max_beats):\n",
    "    \"\"\"\n",
    "    Model definition including custom loss and F1 score metric.\n",
    "    \"\"\"\n",
    "    frame_input = layers.Input(shape=(max_frames, n_features, 1), name='FrameInput')\n",
    "\n",
    "    # Convolutional block with 2D convolution and pooling\n",
    "    conv = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(frame_input)\n",
    "    pool = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(conv)\n",
    "    dropout = layers.Dropout(0.3)(pool)\n",
    "\n",
    "    frame_features = layers.Flatten()(dropout)\n",
    "    frame_feature_model = models.Model(inputs=frame_input, outputs=frame_features, name='FrameFeatureModel')\n",
    "\n",
    "    # Input shape for sequences of beat segments\n",
    "    measure_input = layers.Input(shape=(max_beats, max_frames, n_features, 1), name='BeatInput')\n",
    "    time_distributed = layers.TimeDistributed(frame_feature_model)(measure_input)\n",
    "\n",
    "    masking_layer = layers.Masking(mask_value=-1.0)(time_distributed)\n",
    "    lstm_out = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(masking_layer)\n",
    "    output = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'), name='Output')(lstm_out)\n",
    "\n",
    "    model = models.Model(inputs=measure_input, outputs=output, name='ChorusIdentificationModel')\n",
    "\n",
    "    # Note: Using custom_binary_crossentropy as the loss function\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=custom_binary_crossentropy,\n",
    "                  metrics=[metrics.BinaryAccuracy(name='accuracy'), \n",
    "                           MaskedF1Score(),\n",
    "                           metrics.Precision(name='precision'),\n",
    "                           metrics.Recall(name='recall'),\n",
    "                           metrics.AUC(name='auc')])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model(max_beats, n_features, max_frames):\n",
    "    \"\"\"\n",
    "    Model definition including custom loss and F1 score metric.\n",
    "    \"\"\"\n",
    "    beat_input = layers.Input(shape=(max_beats, max_frames, n_features, 1), name='BeatInput')\n",
    "\n",
    "    # 2D Convolutional block\n",
    "    conv = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(beat_input)\n",
    "\n",
    "    # Reshape the output to 4 dimensions\n",
    "    reshape = layers.Reshape((max_beats, max_frames, 64))(conv)\n",
    "\n",
    "    pool = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(reshape)\n",
    "    dropout = layers.Dropout(0.3)(pool)\n",
    "\n",
    "    beat_features = layers.Flatten()(dropout)\n",
    "    beat_feature_model = models.Model(inputs=beat_input, outputs=beat_features, name='BeatFeatureModel')\n",
    "\n",
    "    # Input shape for sequences of song segments\n",
    "    song_input = layers.Input(shape=(None, max_beats, max_frames, n_features, 1), name='SongInput')\n",
    "    time_distributed = layers.TimeDistributed(beat_feature_model)(song_input)\n",
    "\n",
    "    masking_layer = layers.Masking(mask_value=-1.0)(time_distributed)\n",
    "    lstm_out = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(masking_layer)\n",
    "    output = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'), name='Output')(lstm_out)\n",
    "\n",
    "    model = models.Model(inputs=song_input, outputs=output, name='ChorusIdentificationModel')\n",
    "\n",
    "    # Note: Using custom_binary_crossentropy as the loss function\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=custom_binary_crossentropy,\n",
    "                  metrics=[metrics.BinaryAccuracy(name='accuracy'), \n",
    "                           MaskedF1Score(),\n",
    "                           metrics.Precision(name='precision'),\n",
    "                           metrics.Recall(name='recall'),\n",
    "                           metrics.AUC(name='auc')])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Find the maximum dimensions of frames and beats in your datasets for training data\n",
    "max_frames_train = max(max(len(segment) for segment in song) for song in X_train)\n",
    "max_beats_train = max(len(song) for song in X_train)\n",
    "\n",
    "# Find the maximum dimensions of frames and beats in your datasets for validation data\n",
    "max_frames_val = max(max(len(segment) for segment in song) for song in X_val)\n",
    "max_beats_val = max(len(song) for song in X_val)\n",
    "\n",
    "# Find the maximum dimensions of frames and beats in your datasets for testing data\n",
    "max_frames_test = max(max(len(segment) for segment in song) for song in X_test)\n",
    "max_beats_test = max(len(song) for song in X_test)\n",
    "\n",
    "# Create the model\n",
    "model = create_model(max_beats=max_beats_train, n_features=24, max_frames=max_frames_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69c827-6d42-4796-a7c9-acb49f786d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fdbeb-98e2-4f26-9462-e83e953e9af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_frames_train, max_beats_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb286820-2ded-482c-824e-64e53c89f74c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the maximum dimensions of frames and beats in your datasets for training data\n",
    "max_frames_train = max(max(len(segment) for segment in song) for song in X_train)\n",
    "max_beats_train = max(len(song) for song in X_train)\n",
    "\n",
    "# Find the maximum dimensions of frames and beats in your datasets for validation data\n",
    "max_frames_val = max(max(len(segment) for segment in song) for song in X_val)\n",
    "max_beats_val = max(len(song) for song in X_val)\n",
    "\n",
    "# Find the maximum dimensions of frames and beats in your datasets for testing data\n",
    "max_frames_test = max(max(len(segment) for segment in song) for song in X_test)\n",
    "max_beats_test = max(len(song) for song in X_test)\n",
    "\n",
    "# Create the model\n",
    "model = create_model(max_frames=max_frames_train, n_features=24, max_beats=max_beats_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3670da-5dfe-4a9d-b5e5-de12f320f358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset):\n",
    "    # Define the directories for checkpoints and models\n",
    "    checkpoint_dir = os.path.join('checkpoints', 'ChorusIdentificationModel')\n",
    "    model_dir = os.path.join('models', 'ChorusIdentificationModel')\n",
    "\n",
    "    # Ensure the checkpoint and model directories exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Define the checkpoint path for the best model\n",
    "    best_model_filepath = os.path.join(model_dir, 'best_model.h5')\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=best_model_filepath,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_masked_f1_score',  # Monitor custom metric\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',  \n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model with the simplified callbacks list\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,  # Adjust number of epochs as needed\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy, test_f1_score, test_precision, test_recall, test_auc = model.evaluate(test_dataset)\n",
    "\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"Test F1 Score:\", test_f1_score)\n",
    "    print(\"Test Precision:\", test_precision)\n",
    "    print(\"Test Recall:\", test_recall)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    \n",
    "def plot_training_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f0000-e481-49b7-b02b-7794d8487b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train_model(model, train_dataset=train_dataset_tf, val_dataset=val_dataset_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311c07f-1e69-4bc7-ae42-a1d8d13515bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train_model(model, train_dataset=train_dataset_tf, val_dataset=val_dataset_tf)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "evaluate_model(model, test_dataset=test_dataset_tf)\n",
    "\n",
    "# Visualize training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1900d78-d3f8-4a42-a9b0-c7f67151e8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset):\n",
    "    # Define the directories for checkpoints and models\n",
    "    checkpoint_dir = os.path.join('checkpoints', 'ChorusIdentificationModel')\n",
    "    model_dir = os.path.join('models', 'ChorusIdentificationModel')\n",
    "\n",
    "    # Ensure the checkpoint and model directories exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Define the checkpoint path for the best model\n",
    "    best_model_filepath = os.path.join(model_dir, 'best_model.h5')\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=best_model_filepath,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_masked_f1_score',  # Monitor custom metric\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',  \n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model with the simplified callbacks list\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,  # Adjust number of epochs as needed\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "def plot_training_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# Train the model and get the history\n",
    "history = train_model(model, train_dataset_tf, val_dataset_tf)\n",
    "\n",
    "# Visualize training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy, test_f1_score, test_precision, test_recall, test_auc = model.evaluate(test_dataset_tf)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test F1 Score:\", test_f1_score)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test AUC:\", test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c9134-0fa8-4671-a98a-49c975faaf19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset):\n",
    "    # Define the directories for checkpoints and models\n",
    "    checkpoint_dir = os.path.join('checkpoints', 'ChorusIdentificationModel')\n",
    "    model_dir = os.path.join('models', 'ChorusIdentificationModel')\n",
    "\n",
    "    # Ensure the checkpoint and model directories exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Define the checkpoint path for the best model\n",
    "    best_model_filepath = os.path.join(model_dir, 'best_model.h5')\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=best_model_filepath,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_masked_f1_score',  # Monitor custom metric\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',  \n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model with the simplified callbacks list\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,  # Adjust number of epochs as needed\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19e67d-c483-4dc2-ae6f-57a30f555d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673df1aa-864f-47c8-a8ec-4c751f2e5134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_model(max_frames, max_freq_bins, n_features, custom_binary_crossentropy, custom_accuracy, max_beats):\n",
    "    \"\"\"\n",
    "    Define and compile a CNN model with considerations for environmental sound classification.\n",
    "    \"\"\"\n",
    "    frame_input = layers.Input(shape=(max_frames, max_freq_bins, n_features))\n",
    "    \n",
    "    # Assuming Mx3 filter means spanning the entire frequency bins with a width of 3.\n",
    "    conv1 = layers.Conv2D(filters=180, kernel_size=(max_freq_bins, 3), activation='relu', padding='same')(frame_input)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(4, 4), padding='same')(conv1)\n",
    "    dropout1 = layers.Dropout(0.5)(pool1)  # Applying 50% dropout after pooling\n",
    "    \n",
    "    conv2 = layers.Conv2D(filters=180, kernel_size=(1, 3), activation='relu', padding='same')(dropout1) # Changed to 1x3 to not reduce the frequency dimension further\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(4, 4), padding='same')(conv2)\n",
    "    dropout2 = layers.Dropout(0.5)(pool2)  # Applying 50% dropout after pooling\n",
    "    \n",
    "    conv3 = layers.Conv2D(filters=180, kernel_size=(1, 3), activation='relu', padding='same')(dropout2) # Changed to 1x3 for consistency\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(4, 4), padding='same')(conv3)\n",
    "    dropout3 = layers.Dropout(0.5)(pool3)  # Applying 50% dropout after pooling\n",
    "    \n",
    "    frame_features = layers.Flatten()(dropout3)\n",
    "    frame_feature_model = models.Model(inputs=frame_input, outputs=frame_features)\n",
    "\n",
    "    measure_input = layers.Input(shape=(max_beats, max_frames, max_freq_bins, n_features))\n",
    "    time_distributed = layers.TimeDistributed(frame_feature_model)(measure_input)\n",
    "    masking_layer = layers.Masking(mask_value=-1)(time_distributed)\n",
    "    lstm_out = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(masking_layer)\n",
    "    output = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(lstm_out)\n",
    "    model = models.Model(inputs=measure_input, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596ea8c-e986-4d12-9f10-006064e55385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation with stratified splitting, batching, and positional encoding\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"Generates sinusoidal positional encodings.\"\"\"\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model // 2)[np.newaxis, :]\n",
    "    angles = pos / np.power(10000, (2 * i) / d_model)\n",
    "    sin_enc = np.sin(angles)\n",
    "    cos_enc = np.cos(angles)\n",
    "    encoding = np.concatenate([sin_enc, cos_enc[:, :d_model // 2]], axis=-1)\n",
    "    return encoding\n",
    "\n",
    "def apply_positional_encoding(features, pos_encoding):\n",
    "    \"\"\"Applies positional encoding to the input features.\"\"\"\n",
    "    return features + pos_encoding[:features.shape[1], :]\n",
    "\n",
    "\n",
    "def run_cross_validation(n_splits=5, batch_size=32, feat_dim=128):\n",
    "    \"\"\"Runs cross-validation with stratified splitting, batching, and positional encoding.\"\"\"\n",
    "    song_ids = df['SongID'].unique()\n",
    "    all_labels = np.concatenate([padded_beat_labels[id] for id in song_ids])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(song_ids, all_labels), start=1):\n",
    "        print(f\"Fold {fold}/{n_splits}\")\n",
    "\n",
    "        train_ids, val_ids = song_ids[train_idx], song_ids[val_idx]\n",
    "        train_batches = [(get_batch_segments(ids, padded_beat_segments, padded_beat_labels, feat_dim), \n",
    "                          get_batch_labels(ids, padded_beat_labels)) for ids in create_batches(train_ids, batch_size)]\n",
    "        val_batches = [(get_batch_segments(ids, padded_beat_segments, padded_beat_labels, feat_dim), \n",
    "                        get_batch_labels(ids, padded_beat_labels)) for ids in create_batches(val_ids, batch_size)]\n",
    "\n",
    "        # Train and evaluate model\n",
    "        model = train_model(train_batches)\n",
    "        val_score = evaluate_model(model, val_batches)\n",
    "        fold_scores.append(val_score)\n",
    "\n",
    "    avg_score = np.mean(fold_scores)\n",
    "    print(f\"Average Cross-Validation Score: {avg_score}\")\n",
    "\n",
    "def train_model(train_batches):\n",
    "    # Define the directories for checkpoints and models\n",
    "    checkpoint_dir = os.path.join('..', 'checkpoints', 'CRNN')\n",
    "    model_dir = os.path.join('..', 'models', 'CRNN')\n",
    "\n",
    "    # Ensure the checkpoint and model directories exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Define the checkpoint path for the best model\n",
    "    best_model_filepath = os.path.join(model_dir, 'best_model.h5')\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=best_model_filepath,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_custom_accuracy',  # Use 'val_custom_accuracy' for validation custom accuracy\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',  \n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model with the simplified callbacks list\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "def evaluate_model(model, val_batches):\n",
    "    # Implement model evaluation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21052a43-7191-4e43-aba1-7f65f54ec21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db4149-9754-4c22-8716-9e1d44c6f349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f39cba-b94e-4b70-af11-5f6818ed106b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015c5b33-ff2d-4053-b43b-82e02f9c450a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, LSTM, TimeDistributed, LayerNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36d1e88-f437-4be0-9a91-a6e3e667c0bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_scaled_melspec(y, sr, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"Extracts a Mel-spectrogram from an audio signal.\"\"\"\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    min_val = np.min(S_dB)\n",
    "    max_val = np.max(S_dB)\n",
    "    scaled_melspec = (S_dB - min_val) / (max_val - min_val)\n",
    "    return scaled_melspec\n",
    "\n",
    "\n",
    "def decompose_spectrogram_with_nmf(scaled_spectrogram, n_components):\n",
    "    \"\"\"Decomposes a scaled Mel-spectrogram using NMF from sklearn, without sorting the components.\"\"\"\n",
    "    # Initialize the NMF model with the specified max_iter\n",
    "    model = NMF(n_components=n_components, max_iter=1500, init='nndsvd', random_state=0)\n",
    "    \n",
    "    # Fit the model to the transposed spectrogram (samples x features)\n",
    "    # Note: sklearn expects features as columns, hence the transpose\n",
    "    V = scaled_spectrogram.T\n",
    "    W = model.fit_transform(V)  # W corresponds to the code (activation matrix)\n",
    "    H = model.components_       # H corresponds to the components (dictionary matrix)\n",
    "    \n",
    "    # Return the components, code, and the reconstruction without sorting the components\n",
    "    return H, W\n",
    "\n",
    "def find_anchor_frame(beats, tempo, sr):\n",
    "    # Convert tempo to interval in seconds between beats\n",
    "    seconds_per_beat = 60.0 / tempo\n",
    "    \n",
    "    # Convert interval to expected frames between beats\n",
    "    expected_interval = int(librosa.time_to_frames(seconds_per_beat, sr=sr))\n",
    "    \n",
    "    # Calculate the difference between consecutive beats\n",
    "    beat_intervals = np.diff(beats)\n",
    "    \n",
    "    # Initialize variables to keep track of the best matching sequence\n",
    "    best_match_start = None\n",
    "    best_match_quality = 0\n",
    "    \n",
    "    # Sliding window size based on a small multiplier of the expected interval to capture tempo variations\n",
    "    window_size = 3  # Small window to check consistency of intervals\n",
    "    \n",
    "    # Iterate through beat intervals with a sliding window\n",
    "    for i in range(len(beat_intervals) - window_size + 1):\n",
    "        window = beat_intervals[i:i+window_size]\n",
    "        \n",
    "        # Calculate the average interval in the current window and its match quality\n",
    "        avg_interval = np.mean(window)\n",
    "        match_quality = 1 - abs(avg_interval - expected_interval) / expected_interval\n",
    "        \n",
    "        # Update the best match if this window represents a higher quality match\n",
    "        if match_quality > best_match_quality:\n",
    "            best_match_start = beats[i]\n",
    "            best_match_quality = match_quality\n",
    "            \n",
    "            # Early exit condition if the match quality is high enough\n",
    "            if match_quality > 0.95:\n",
    "                return best_match_start\n",
    "    \n",
    "    return best_match_start\n",
    "\n",
    "\n",
    "def create_beat_grid(beats, anchor_frame, sr, beat_interval_in_frames, time_signature, duration_in_frames):\n",
    "    \"\"\"Creates a grid of measures based on the tempo, time signature, and beats of the song.\"\"\"  \n",
    "    if not anchor_frame:\n",
    "        anchor_frame = beats[0]\n",
    "\n",
    "    # Add beats before the first onset (working backwards)\n",
    "    beat_grid = [anchor_frame]\n",
    "    current_frame = anchor_frame\n",
    "    while current_frame >= 0:\n",
    "        current_frame -= beat_interval_in_frames\n",
    "        beat_grid.insert(0, current_frame)\n",
    "\n",
    "    # Remove the first beat if it's negative\n",
    "    if beat_grid[0] < 0:\n",
    "        beat_grid.pop(0)\n",
    "        \n",
    "    # Group beats into measures\n",
    "    measure_grid = []\n",
    "    current_frame = beat_grid[0]\n",
    "    while current_frame <= duration_in_frames:\n",
    "        measure_grid.append(current_frame)\n",
    "        current_frame += beat_interval_in_frames * time_signature\n",
    "\n",
    "    # Insert 0 if not already in the list\n",
    "    if measure_grid[0] != 0:\n",
    "        measure_grid.insert(0, 0)\n",
    "    \n",
    "    # Append the duration if not already in the list\n",
    "    if measure_grid[-1] != duration_in_frames:\n",
    "        measure_grid.append(duration_in_frames)\n",
    "    \n",
    "    return np.array(beat_grid), np.array(measure_grid)\n",
    "\n",
    "\n",
    "def generate_and_align_labels(df, n_frames, measure_grid_frames):\n",
    "    \"\"\"Generates a binary sequence of labels (1 for 'chorus', 0 for 'other') for each frame in a song,\n",
    "    and aligns the label sequence with the measure grid by labeling a measure as 'chorus' if at least\n",
    "    1/4 of the frames within the measure are labeled as 'chorus'.\n",
    "    \"\"\"\n",
    "    # Generate binary label sequence\n",
    "    binary_label_sequence = np.zeros(n_frames, dtype=int)\n",
    "    for index, row in df.iterrows():\n",
    "        start_frame = row['start_frame']\n",
    "        end_frame = row['end_frame']\n",
    "        if row['label'] == 'chorus':\n",
    "            binary_label_sequence[start_frame:end_frame] = 1\n",
    "\n",
    "    # Initialize the aligned_labels array\n",
    "    aligned_labels = np.zeros(len(measure_grid_frames) - 1, dtype=int)\n",
    "\n",
    "    # Iterate over each measure\n",
    "    for i in range(len(measure_grid_frames) - 1):\n",
    "        start_frame = measure_grid_frames[i]\n",
    "        end_frame = measure_grid_frames[i + 1]\n",
    "\n",
    "        # Extract the labels for the current measure\n",
    "        measure_labels = binary_label_sequence[start_frame:end_frame]\n",
    "\n",
    "        # Calculate the proportion of '1' labels in the current measure\n",
    "        proportion_of_ones = np.sum(measure_labels) / len(measure_labels)\n",
    "\n",
    "        # If at least 1/4 of the measure is labeled as 'chorus', set the measure label to '1'\n",
    "        if proportion_of_ones >= 0.25:\n",
    "            aligned_labels[i] = 1\n",
    "        else:\n",
    "            aligned_labels[i] = 0\n",
    "\n",
    "    return aligned_labels\n",
    "\n",
    "\n",
    "def segment_data_measures(data, measure_grid_frames):\n",
    "    segments = []\n",
    "    for i in range(len(measure_grid_frames) - 1):\n",
    "        start_frame = measure_grid_frames[i]\n",
    "        end_frame = measure_grid_frames[i + 1]\n",
    "        segment = data[start_frame:end_frame]\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Apply sin to even indices in the array\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Apply cos to odd indices in the array\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return pos_encoding\n",
    "\n",
    "\n",
    "def apply_hybrid_positional_encoding(segments):\n",
    "    num_measures = len(segments)\n",
    "    n_features = segments[0].shape[1]  # Assuming all segments have the same feature dimension\n",
    "    \n",
    "    # Generate measure-level positional encodings (assuming it's correct and matches the number of measures)\n",
    "    measure_level_encodings = positional_encoding(num_measures, n_features)\n",
    "    \n",
    "    encoded_segments = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        num_frames_per_measure = segment.shape[0]\n",
    "        \n",
    "        # Generate frame-level positional encodings for the current segment length\n",
    "        frame_level_encodings = positional_encoding(num_frames_per_measure, n_features)\n",
    "        \n",
    "        # Apply measure-level encoding to each frame in the measure\n",
    "        # Note: measure_encoding is broadcastable as it's applied equally across all frames\n",
    "        measure_encoding = measure_level_encodings[0, i, :]\n",
    "        \n",
    "        # Apply frame-level encoding to the segment\n",
    "        # This operation is now valid as frame_level_encodings matches the segment's shape\n",
    "        segment_with_frame_encoding = segment + frame_level_encodings\n",
    "        \n",
    "        # Combine the two encodings by adding them\n",
    "        combined_encoding = segment_with_frame_encoding + measure_encoding\n",
    "        \n",
    "        encoded_segments.append(combined_encoding)\n",
    "    \n",
    "    return encoded_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6233e9d5-f19b-42ba-932a-144240427c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the DataFrame with labeled data\n",
    "df = pd.read_csv('../data/dataframes/clean_labeled.csv')\n",
    "\n",
    "segment_dir = \"../data/pkl/activations\"\n",
    "labels_dir = \"../data/pkl/activation_labels\"\n",
    "os.makedirs(segment_dir, exist_ok=True)\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the DataFrame and prepare data for each song\n",
    "for _, group in tqdm(df.groupby('SongID'), desc='Processing'):\n",
    "    song_id = group['SongID'].values[0]\n",
    "    audio_path = group['FilePath'].values[0]\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    spectrogram = extract_scaled_melspec(y, sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "    _, activations = decompose_spectrogram_with_nmf(spectrogram, n_components=32)\n",
    "\n",
    "    # Extract tempogram, beat frames\n",
    "    C = np.abs(librosa.cqt(y=y, sr=sr))\n",
    "    onset_env = librosa.onset.onset_strength(sr=sr, S=librosa.amplitude_to_db(C, ref=np.max))\n",
    "    tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "    # Create a measure grid\n",
    "    # Tempo\n",
    "    bpm = group['sp_tempo'].values[0] if not pd.isna(group['sp_tempo'].values[0]) else tempo\n",
    "    if bpm == 0:\n",
    "        bpm = tempo\n",
    "    if bpm > 140:\n",
    "        bpm /= 2\n",
    "    if bpm <= 70:\n",
    "        bpm *= 2\n",
    "    # Time signature\n",
    "    time_signature = group['sp_time_signature'].values[0] if not pd.isna(group['sp_time_signature'].values[0]) else 4\n",
    "    time_signature = int(time_signature) if time_signature != 0 else 4\n",
    "    \n",
    "    duration_in_frames = len(activations)\n",
    "    beat_interval_in_frames = int(librosa.time_to_frames(60/bpm, sr=sr))\n",
    "    \n",
    "    anchor_frame = find_anchor_frame(beats, bpm, sr)\n",
    "    beat_grid, measure_grid = create_beat_grid(beats, anchor_frame, sr, beat_interval_in_frames, time_signature, duration_in_frames)\n",
    "\n",
    "    # Align and aggregate labels at the measure level\n",
    "    aligned_labels = generate_and_align_labels(group, duration_in_frames, measure_grid)\n",
    "\n",
    "    # Segment features into measures\n",
    "    segmented_features = segment_data_measures(activations, measure_grid)\n",
    "\n",
    "    # Position encode\n",
    "    encoded_features = apply_hybrid_positional_encoding(segmented_features)\n",
    "    \n",
    "    # Save segmented_H as a pickle file\n",
    "    with open(os.path.join(segment_dir, f\"{song_id}_encoded.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(encoded_features, f)\n",
    "\n",
    "    # Save aligned_labels as a pickle file\n",
    "    with open(os.path.join(labels_dir, f\"{song_id}_labels.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(aligned_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcfd7fa-580e-4eae-b789-5e908df8598c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pickles_from_directory(directory):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pkl'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "                all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "# Load all segments and labels\n",
    "all_segments = load_pickles_from_directory(segment_dir)\n",
    "all_labels = load_pickles_from_directory(labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a156af50-b3d5-40ff-b7d8-fe0a2b93f609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_max_frames_and_measures(all_segments):\n",
    "    \"\"\"Find the maximum number of frames per measure and the maximum number of measures across all songs, accommodating the extra list layer.\"\"\"\n",
    "    max_frames_per_measure = max(max(len(measure_list[0]) for measure_list in song) for song in all_segments)\n",
    "    max_measures = max(len(song) for song in all_segments)\n",
    "    return max_frames_per_measure, max_measures\n",
    "    \n",
    "def pad_measures(all_segments, max_frames_per_measure):\n",
    "    \"\"\"Pad all measures within each song to have the same number of frames using 0 padding.\"\"\"\n",
    "    padded_segments_within_songs = []\n",
    "    for song in all_segments:\n",
    "        padded_song = []\n",
    "        for measure_list in song:\n",
    "            measure = measure_list[0]  # Access the actual measure data\n",
    "            padding_needed = max_frames_per_measure - measure.shape[0]\n",
    "            padded_measure = np.pad(measure, ((0, padding_needed), (0, 0)), mode='constant', constant_values=(0))\n",
    "            padded_song.append(padded_measure)\n",
    "        padded_segments_within_songs.append(padded_song)\n",
    "    return padded_segments_within_songs\n",
    "\n",
    "def pad_songs(padded_measures, max_measures, max_frames_per_measure, n_features=32):\n",
    "    \"\"\"Pad all songs to have the same number of measures using 0 padding.\"\"\"\n",
    "    padded_segments_across_songs = []\n",
    "    for song in padded_measures:\n",
    "        measures_to_add = max_measures - len(song)\n",
    "        if measures_to_add > 0:\n",
    "            padding_measures = [np.zeros((max_frames_per_measure, n_features)) for _ in range(measures_to_add)]\n",
    "            padded_song = song + padding_measures\n",
    "        else:\n",
    "            padded_song = song\n",
    "        padded_segments_across_songs.append(padded_song)\n",
    "    return padded_segments_across_songs\n",
    "\n",
    "def pad_labels(all_labels, max_measures):\n",
    "    \"\"\"Pad all labels to have the same number of measures using -1 padding.\"\"\"\n",
    "    padded_labels = []\n",
    "    for labels in all_labels:\n",
    "        padding_needed = max_measures - len(labels)\n",
    "        # Ensure labels are numpy arrays for consistent operations\n",
    "        labels = np.asarray(labels)\n",
    "        # Pad labels with -1\n",
    "        padded_label = np.pad(labels, (0, padding_needed), mode='constant', constant_values=(-1))\n",
    "        padded_labels.append(padded_label)\n",
    "    return padded_labels\n",
    "\n",
    "\n",
    "# Find the maximum frames per measure and maximum measures across all songs\n",
    "max_frames_per_measure, max_measures = find_max_frames_and_measures(all_segments)\n",
    "\n",
    "# First, pad measures within each song to have the same number of frames\n",
    "padded_measures = pad_measures(all_segments, max_frames_per_measure)\n",
    "\n",
    "# Then, pad all songs to have the same number of measures\n",
    "padded_songs = pad_songs(padded_measures, max_measures, max_frames_per_measure)\n",
    "\n",
    "# Pad labels to match the structure of padded_segments\n",
    "padded_labels = pad_labels(all_labels, max_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c6d4a2a-d3c4-4838-a263-05a2c99f8454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of songs\n",
    "num_songs = len(padded_songs)\n",
    "\n",
    "# Create indices for the songs\n",
    "indices = np.arange(num_songs)\n",
    "\n",
    "# Split indices into training and temporary (validation + test) sets\n",
    "train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the temporary set into validation and test sets\n",
    "val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# Generate the actual train, validation, and test sets using the indices\n",
    "X_train = [padded_songs[i] for i in train_indices]\n",
    "y_train = [padded_labels[i] for i in train_indices]\n",
    "\n",
    "X_val = [padded_songs[i] for i in val_indices]\n",
    "y_val = [padded_labels[i] for i in val_indices]\n",
    "\n",
    "X_test = [padded_songs[i] for i in test_indices]\n",
    "y_test = [padded_labels[i] for i in test_indices]\n",
    "\n",
    "# Define the batch size as the number of songs per batch\n",
    "batch_size = 32\n",
    "\n",
    "def data_generator(X, y):\n",
    "    for features, labels in zip(X, y):\n",
    "        # Assuming features is a list of lists of lists (songs, measures, frames)\n",
    "        # Convert features to a NumPy array with shape: (num_measures, num_frames, 32)\n",
    "        features_array = np.array(features)  # This assumes features is a 3D list\n",
    "        \n",
    "        # Assuming labels is a 1D NumPy array of labels for each measure\n",
    "        # Reshape labels to have an additional dimension: (num_measures, 1)\n",
    "        labels_reshaped = np.expand_dims(labels, axis=-1)\n",
    "        \n",
    "        yield features_array, labels_reshaped\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels...\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(X_train, y_train),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, None, 32), dtype=tf.float32),  # Adjust the shape as necessary\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "    )\n",
    ").batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(X_val, y_val),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, None, 32), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "    )\n",
    ").batch(batch_size)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(X_train, y_train),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, None, 32), dtype=tf.float32),  # Adjust the shape as necessary\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "    )\n",
    ").batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(X_test, y_test),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, None, 32), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "    )\n",
    ").batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "704417b0-34ad-4e5f-b381-2a686a90c4c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 204, 320, 32)]    0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 204, 10240)       307840    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " masking_1 (Masking)         (None, 204, 10240)        0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 204, 512)         21497856  \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 204, 1)           513       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,806,209\n",
      "Trainable params: 21,806,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"Custom binary cross-entropy loss to handle -1 labels, which are used for padding and should be ignored during loss calculation.\"\"\"\n",
    "    # Converts y_true to float32 to match y_pred\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    # Calculate the binary crossentropy\n",
    "    bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Apply the mask to the loss\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "    loss = bce * mask\n",
    "    \n",
    "    # Return the mean loss, but only for masked (non-ignored) values\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)  # Identifying non-padded labels\n",
    "    correct_predictions = tf.equal(tf.cast(tf.round(y_pred), tf.float32), y_true)  # True or False for each prediction\n",
    "    masked_correct_predictions = tf.cast(correct_predictions, tf.float32) * mask  # Apply mask\n",
    "    accuracy = tf.reduce_sum(masked_correct_predictions) / tf.reduce_sum(mask)  # Calculate accuracy on non-padded data\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def create_crnn_model(max_frames_per_measure, max_measures, feature_per_frame):\n",
    "    # Define the frame-level model\n",
    "    frame_input = tf.keras.layers.Input(shape=(max_frames_per_measure, feature_per_frame))\n",
    "    conv1 =  tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(frame_input)\n",
    "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(pool1)\n",
    "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(pool2)\n",
    "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(conv3)\n",
    "    frame_features = tf.keras.layers.Flatten()(pool3)\n",
    "    frame_feature_model = Model(inputs=frame_input, outputs=frame_features)\n",
    "\n",
    "    # Define the measure-level model\n",
    "    measure_input = tf.keras.layers.Input(shape=(max_measures, max_frames_per_measure, feature_per_frame))\n",
    "    time_distributed = tf.keras.layers.TimeDistributed(frame_feature_model)(measure_input)\n",
    "    masking_layer = tf.keras.layers.Masking(mask_value=0.0)(time_distributed)\n",
    "    lstm_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(masking_layer)\n",
    "\n",
    "    # Directly connect LSTM output to TimeDistributed Dense layer\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='sigmoid'))(lstm_out)\n",
    "    model = Model(inputs=measure_input, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=custom_binary_crossentropy, metrics=[custom_accuracy])\n",
    "\n",
    "    return model\n",
    "\n",
    "feature_per_frame = 32\n",
    "model = create_crnn_model(max_frames_per_measure, max_measures, feature_per_frame)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5c94a76-72da-4c73-b0f0-4695d6488dca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "      8/Unknown - 67s 7s/step - loss: 1.6194 - custom_accuracy: 0.5181\n",
      "Epoch 1: val_custom_accuracy improved from -inf to 0.41894, saving model to ..\\models\\CRNN\\spectrogram_best_model.h5\n",
      "8/8 [==============================] - 74s 8s/step - loss: 1.6194 - custom_accuracy: 0.5181 - val_loss: 0.7867 - val_custom_accuracy: 0.4189 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7651 - custom_accuracy: 0.4226\n",
      "Epoch 2: val_custom_accuracy did not improve from 0.41894\n",
      "8/8 [==============================] - 62s 8s/step - loss: 0.7651 - custom_accuracy: 0.4226 - val_loss: 0.7374 - val_custom_accuracy: 0.4170 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7201 - custom_accuracy: 0.4222\n",
      "Epoch 3: val_custom_accuracy did not improve from 0.41894\n",
      "8/8 [==============================] - 63s 8s/step - loss: 0.7201 - custom_accuracy: 0.4222 - val_loss: 0.7034 - val_custom_accuracy: 0.4188 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6906 - custom_accuracy: 0.5161\n",
      "Epoch 4: val_custom_accuracy improved from 0.41894 to 0.59171, saving model to ..\\models\\CRNN\\spectrogram_best_model.h5\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6906 - custom_accuracy: 0.5161 - val_loss: 0.6800 - val_custom_accuracy: 0.5917 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6774 - custom_accuracy: 0.5859\n",
      "Epoch 5: val_custom_accuracy did not improve from 0.59171\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6774 - custom_accuracy: 0.5859 - val_loss: 0.6718 - val_custom_accuracy: 0.5917 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6728 - custom_accuracy: 0.5859\n",
      "Epoch 6: val_custom_accuracy did not improve from 0.59171\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6728 - custom_accuracy: 0.5859 - val_loss: 0.6666 - val_custom_accuracy: 0.5917 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6692 - custom_accuracy: 0.5859\n",
      "Epoch 7: val_custom_accuracy did not improve from 0.59171\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6692 - custom_accuracy: 0.5859 - val_loss: 0.6574 - val_custom_accuracy: 0.5917 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6537 - custom_accuracy: 0.5859\n",
      "Epoch 8: val_custom_accuracy did not improve from 0.59171\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6537 - custom_accuracy: 0.5859 - val_loss: 0.6310 - val_custom_accuracy: 0.5917 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6269 - custom_accuracy: 0.5859\n",
      "Epoch 9: val_custom_accuracy did not improve from 0.59171\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6269 - custom_accuracy: 0.5859 - val_loss: 0.6078 - val_custom_accuracy: 0.5917 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6166 - custom_accuracy: 0.5859\n",
      "Epoch 10: val_custom_accuracy did not improve from 0.59171\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6166 - custom_accuracy: 0.5859 - val_loss: 0.6047 - val_custom_accuracy: 0.5917 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Define the directories for checkpoints and models\n",
    "checkpoint_dir = os.path.join('..', 'checkpoints', 'CRNN')\n",
    "model_dir = os.path.join('..', 'models', 'CRNN')\n",
    "\n",
    "# Ensure the checkpoint and model directories exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Define the checkpoint path for the best model\n",
    "best_model_filepath = os.path.join(model_dir, 'spectrogram_best_model.h5')\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=best_model_filepath,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_custom_accuracy',  # Use 'val_custom_accuracy' for validation custom accuracy\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',  \n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',  \n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        min_delta=0.0001,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model with the simplified callbacks list\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94b33cf2-ddaf-44b8-8122-60a53450cf4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input vector should be 1-D.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m change_points \u001b[38;5;241m=\u001b[39m \u001b[43mdtw_change_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChange points detected at:\u001b[39m\u001b[38;5;124m\"\u001b[39m, change_points)\n",
      "Cell \u001b[1;32mIn[46], line 9\u001b[0m, in \u001b[0;36mdtw_change_detection\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features)):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(features)):\n\u001b[1;32m----> 9\u001b[0m         distance, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcatenated_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcatenated_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meuclidean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         distances[i, j] \u001b[38;5;241m=\u001b[39m distance\n\u001b[0;32m     11\u001b[0m         distances[j, i] \u001b[38;5;241m=\u001b[39m distance\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\fastdtw\\fastdtw.py:53\u001b[0m, in \u001b[0;36mfastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' return the approximate distance between 2 time series with O(N)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    time and memory complexity\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    (2.0, [(0, 0), (1, 0), (2, 1), (3, 2), (4, 2)])\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     52\u001b[0m x, y, dist \u001b[38;5;241m=\u001b[39m __prep_inputs(x, y, dist)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__fastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\fastdtw\\fastdtw.py:73\u001b[0m, in \u001b[0;36m__fastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     70\u001b[0m x_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(x)\n\u001b[0;32m     71\u001b[0m y_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(y)\n\u001b[0;32m     72\u001b[0m distance, path \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 73\u001b[0m     \u001b[43m__fastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m window \u001b[38;5;241m=\u001b[39m __expand_window(path, \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y), radius)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m __dtw(x, y, window, dist\u001b[38;5;241m=\u001b[39mdist)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\fastdtw\\fastdtw.py:73\u001b[0m, in \u001b[0;36m__fastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     70\u001b[0m x_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(x)\n\u001b[0;32m     71\u001b[0m y_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(y)\n\u001b[0;32m     72\u001b[0m distance, path \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 73\u001b[0m     \u001b[43m__fastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m window \u001b[38;5;241m=\u001b[39m __expand_window(path, \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y), radius)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m __dtw(x, y, window, dist\u001b[38;5;241m=\u001b[39mdist)\n",
      "    \u001b[1;31m[... skipping similar frames: __fastdtw at line 73 (5 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\fastdtw\\fastdtw.py:73\u001b[0m, in \u001b[0;36m__fastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     70\u001b[0m x_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(x)\n\u001b[0;32m     71\u001b[0m y_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(y)\n\u001b[0;32m     72\u001b[0m distance, path \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 73\u001b[0m     \u001b[43m__fastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m window \u001b[38;5;241m=\u001b[39m __expand_window(path, \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y), radius)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m __dtw(x, y, window, dist\u001b[38;5;241m=\u001b[39mdist)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\fastdtw\\fastdtw.py:68\u001b[0m, in \u001b[0;36m__fastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     65\u001b[0m min_time_size \u001b[38;5;241m=\u001b[39m radius \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m<\u001b[39m min_time_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m<\u001b[39m min_time_size:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m x_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(x)\n\u001b[0;32m     71\u001b[0m y_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\fastdtw\\fastdtw.py:130\u001b[0m, in \u001b[0;36mdtw\u001b[1;34m(x, y, dist)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' return the distance between 2 time series without approximation\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    (2.0, [(0, 0), (1, 0), (2, 1), (3, 2), (4, 2)])\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    129\u001b[0m x, y, dist \u001b[38;5;241m=\u001b[39m __prep_inputs(x, y, dist)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__dtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\fastdtw\\fastdtw.py:141\u001b[0m, in \u001b[0;36m__dtw\u001b[1;34m(x, y, window, dist)\u001b[0m\n\u001b[0;32m    139\u001b[0m D[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m window:\n\u001b[1;32m--> 141\u001b[0m     dt \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     D[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((D[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, j][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mdt, i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, j), (D[i, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mdt, i, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    143\u001b[0m                   (D[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mdt, i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    144\u001b[0m path \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\scipy\\spatial\\distance.py:520\u001b[0m, in \u001b[0;36meuclidean\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meuclidean\u001b[39m(u, v, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    485\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    Computes the Euclidean distance between two 1-D arrays.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mminkowski\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\scipy\\spatial\\distance.py:463\u001b[0m, in \u001b[0;36mminkowski\u001b[1;34m(u, v, p, w)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski\u001b[39m(u, v, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m    Compute the Minkowski distance between two 1-D arrays.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m \n\u001b[0;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     v \u001b[38;5;241m=\u001b[39m _validate_vector(v)\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mixin\\lib\\site-packages\\scipy\\spatial\\distance.py:302\u001b[0m, in \u001b[0;36m_validate_vector\u001b[1;34m(u, dtype)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m u\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m u\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput vector should be 1-D.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Input vector should be 1-D."
     ]
    }
   ],
   "source": [
    "change_points = dtw_change_detection(features)\n",
    "print(\"Change points detected at:\", change_points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7481ada1-2956-472d-8edd-abc6ea849cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     3,   183,   363,   543,   723,   903,  1083,  1263,\n",
       "        1443,  1623,  1803,  1983,  2163,  2343,  2523,  2703,  2883,\n",
       "        3063,  3243,  3423,  3603,  3783,  3963,  4143,  4323,  4503,\n",
       "        4683,  4863,  5043,  5223,  5403,  5583,  5763,  5943,  6123,\n",
       "        6303,  6483,  6663,  6843,  7023,  7203,  7383,  7563,  7743,\n",
       "        7923,  8103,  8283,  8463,  8643,  8823,  9003,  9183,  9363,\n",
       "        9543,  9723,  9903, 10083, 10263, 10443, 10623, 10803, 10983,\n",
       "       11163, 11343, 11523, 11703, 11883, 12063, 12243, 12423, 12603,\n",
       "       12783, 12963, 13143, 13323, 13503, 13683, 13863, 14043, 14223,\n",
       "       14403, 14583, 14763, 14943, 15123, 15303, 15483, 15663, 15843,\n",
       "       16023, 16203, 16383, 16563, 16743, 16923, 17103, 17283, 17463,\n",
       "       17643, 17823, 18003, 18183, 18363, 18543, 18723, 18903, 19083,\n",
       "       19263, 19443, 19623, 19803, 19983, 20163, 20343, 20523, 20703,\n",
       "       20883, 21063, 21243, 21423, 21603, 21783, 21963, 22143, 22323,\n",
       "       22503, 22683, 22863, 23043, 23223, 23403, 23583, 23763, 23943,\n",
       "       24123, 24303, 24483, 24663, 24843, 25023, 25203, 25383, 25563,\n",
       "       25743, 25914])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = '../data/audio_files/processed/2.mp3'\n",
    "df = pd.read_csv('../data/dataframes/clean_labeled.csv')\n",
    "data = df.loc[df['SongID']==2]\n",
    "sp_tempo = data['sp_tempo'].values[0]\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "def extract_mel_spectrogram(y, sr, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"Extracts a Mel-spectrogram from an audio signal.\"\"\"\n",
    "    return librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "spectrogram = extract_mel_spectrogram(y, sr)\n",
    "duration = librosa.frames_to_time(len(spectrogram.T), sr=sr, hop_length=512)\n",
    "C = np.abs(librosa.cqt(y=y, sr=sr))\n",
    "onset_env = librosa.onset.onset_strength(sr=sr, S=librosa.amplitude_to_db(C, ref=np.max))\n",
    "onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, backtrack=True, units='frames')\n",
    "tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "if sp_tempo == 0:\n",
    "    sp_tempo = tempo\n",
    "if sp_tempo > 150:\n",
    "    sp_tempo /= 2\n",
    "tempo = round_tempo(sp_tempo)\n",
    "time_signature = data['sp_time_signature'].values[0] if not pd.isna(data['sp_time_signature'].values[0]) else 4\n",
    "time_signature = int(time_signature) if time_signature != 0 else 4\n",
    "measure_grid = create_measure_grid(onset_env, sr, tempo, time_signature, duration)\n",
    "measure_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93d6359-82f4-425b-90d8-96dcef281e15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.,   180.,   360.,   540.,   720.,   900.,  1080.,  1260.,\n",
       "        1440.,  1620.,  1800.,  1980.,  2160.,  2340.,  2520.,  2700.,\n",
       "        2880.,  3060.,  3240.,  3420.,  3600.,  3780.,  3960.,  4140.,\n",
       "        4320.,  4500.,  4680.,  4860.,  5040.,  5220.,  5400.,  5580.,\n",
       "        5760.,  5940.,  6120.,  6300.,  6480.,  6660.,  6840.,  7020.,\n",
       "        7200.,  7380.,  7560.,  7740.,  7920.,  8100.,  8280.,  8460.,\n",
       "        8640.,  8820.,  9000.,  9180.,  9360.,  9540.,  9720.,  9900.,\n",
       "       10080., 10260., 10440., 10620., 10800., 10980., 11160., 11340.,\n",
       "       11520., 11700., 11880., 12060., 12240., 12420., 12600., 12780.,\n",
       "       12960., 13140., 13320., 13500., 13680., 13860., 14040., 14220.,\n",
       "       14400., 14580., 14760., 14940., 15120., 15300., 15480., 15660.,\n",
       "       15840., 16020., 16200., 16380., 16560., 16740., 16920., 17100.,\n",
       "       17280., 17460., 17640., 17820., 18000., 18180., 18360., 18540.,\n",
       "       18720., 18900., 19080., 19260., 19440., 19620., 19800., 19980.,\n",
       "       20160., 20340., 20520., 20700., 20880., 21060., 21240., 21420.,\n",
       "       21600., 21780., 21960., 22140., 22320., 22500., 22680., 22860.,\n",
       "       23040., 23220., 23400., 23580., 23760., 23940., 24120., 24300.,\n",
       "       24480., 24660., 24840., 25020., 25200., 25380., 25560., 25740.,\n",
       "       25914.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = '../data/audio_files/processed/2.mp3'\n",
    "df = pd.read_csv('../data/dataframes/clean_labeled.csv')\n",
    "data = df.loc[df['SongID']==2]\n",
    "sp_tempo = data['sp_tempo'].values[0]\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "duration = librosa.get_duration(y=y, sr=sr, hop_length=512)\n",
    "C = np.abs(librosa.cqt(y=y, sr=sr))\n",
    "onset_env = librosa.onset.onset_strength(sr=sr, S=librosa.amplitude_to_db(C, ref=np.max))\n",
    "tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "if sp_tempo == 0:\n",
    "    sp_tempo = tempo\n",
    "if sp_tempo > 150:\n",
    "    sp_tempo /= 2\n",
    "tempo = round_tempo(sp_tempo)\n",
    "time_signature = data['sp_time_signature'].values[0] if not pd.isna(data['sp_time_signature'].values[0]) else 4\n",
    "time_signature = int(time_signature) if time_signature != 0 else 4\n",
    "anchor_frame = find_anchor_frame(onset_env, sr, tempo, time_signature, duration)\n",
    "measure_grid = create_measure_grid(sr, tempo, time_signature, duration, anchor_frame)\n",
    "measure_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6086f9-85bb-41d9-92a3-875b02555e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
